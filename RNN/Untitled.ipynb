{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53313d71-6ae8-4ea8-89ae-05b915f9dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: [0.12336938]\n",
      "Epoch 2/25, Loss: [0.1229217]\n",
      "Epoch 3/25, Loss: [0.12248081]\n",
      "Epoch 4/25, Loss: [0.12204565]\n",
      "Epoch 5/25, Loss: [0.12161534]\n",
      "Epoch 6/25, Loss: [0.1211891]\n",
      "Epoch 7/25, Loss: [0.12076631]\n",
      "Epoch 8/25, Loss: [0.12034646]\n",
      "Epoch 9/25, Loss: [0.11992915]\n",
      "Epoch 10/25, Loss: [0.11951406]\n",
      "Epoch 11/25, Loss: [0.11910097]\n",
      "Epoch 12/25, Loss: [0.11868973]\n",
      "Epoch 13/25, Loss: [0.11828027]\n",
      "Epoch 14/25, Loss: [0.11787256]\n",
      "Epoch 15/25, Loss: [0.11746664]\n",
      "Epoch 16/25, Loss: [0.11706258]\n",
      "Epoch 17/25, Loss: [0.1166605]\n",
      "Epoch 18/25, Loss: [0.11626054]\n",
      "Epoch 19/25, Loss: [0.11586288]\n",
      "Epoch 20/25, Loss: [0.1154677]\n",
      "Epoch 21/25, Loss: [0.11507521]\n",
      "Epoch 22/25, Loss: [0.11468563]\n",
      "Epoch 23/25, Loss: [0.11429916]\n",
      "Epoch 24/25, Loss: [0.11391604]\n",
      "Epoch 25/25, Loss: [0.11353648]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_xor_rnn(X, Y, hidden_size=2, epochs=25, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train an RNN on the XOR problem.\n",
    "    \n",
    "    :param X: Input data - sequence of bits.\n",
    "    :param Y: Target output data.\n",
    "    :param hidden_size: Number of neurons in the hidden layer.\n",
    "    :param epochs: Number of epochs to train for.\n",
    "    :param learning_rate: Learning rate for the optimizer.\n",
    "    :return: Trained weight and bias parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights\n",
    "    Wxh = np.random.randn(hidden_size, 1)  # Weight matrix for input to hidden\n",
    "    Whh = np.random.randn(hidden_size, hidden_size)  # Weight matrix for hidden to hidden\n",
    "    Why = np.random.randn(1, hidden_size)  # Weight matrix for hidden to output\n",
    "    bh = np.zeros((hidden_size, 1))  # Bias for hidden layer\n",
    "    by = np.zeros((1, 1))  # Bias for output layer\n",
    "\n",
    "    # Sigmoid activation function and its derivative\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_prime(x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass\n",
    "            h = np.zeros((hidden_size, 1))  # Reset the hidden state at the start of each sequence\n",
    "            y_pred = None\n",
    "\n",
    "            for t in range(2):  # Loop through the sequence\n",
    "                x = X[i][t].reshape(-1, 1)  # Get the input for this timestep\n",
    "\n",
    "                # Update the hidden state\n",
    "                h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "\n",
    "                # Calculate the output\n",
    "                y_pred = sigmoid(np.dot(Why, h) + by)\n",
    "\n",
    "            # Calculate the loss using mean squared error\n",
    "            loss = (y_pred - Y[i][0]) ** 2 / 2\n",
    "            total_loss += loss[0]\n",
    "            \n",
    "            # Backward pass\n",
    "            # Gradients for each parameter\n",
    "            dWhy = np.zeros_like(Why)\n",
    "            dWxh = np.zeros_like(Wxh)\n",
    "            dWhh = np.zeros_like(Whh)\n",
    "            dbh = np.zeros_like(bh)\n",
    "            dby = np.zeros_like(by)\n",
    "            dhnext = np.zeros_like(h)\n",
    "            \n",
    "            # Derivative of loss with respect to y_pred\n",
    "            dy = (y_pred - Y[i][0]) * sigmoid_prime(y_pred)\n",
    "            \n",
    "            # Backpropagate through the network\n",
    "            dWhy += np.dot(dy, h.T)\n",
    "            dby += dy\n",
    "            dh = np.dot(Why.T, dy)  # Backprop into h\n",
    "            \n",
    "            # Backprop through time (BPTT)\n",
    "            for t in reversed(range(2)):\n",
    "                dhraw = (1 - h * h) * dh  # Backprop through tanh nonlinearity\n",
    "                dbh += dhraw\n",
    "                dWxh += np.dot(dhraw, X[i][t].reshape(1, -1))\n",
    "                dWhh += np.dot(dhraw, h.T)\n",
    "                dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "            # Clip gradients to mitigate exploding gradients\n",
    "            for dparam in [dWhy, dWxh, dWhh, dbh, dby]:\n",
    "                np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            Wxh -= learning_rate * dWxh\n",
    "            Whh -= learning_rate * dWhh\n",
    "            Why -= learning_rate * dWhy\n",
    "            bh -= learning_rate * dbh\n",
    "            by -= learning_rate * dby\n",
    "\n",
    "        # Print loss every epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X)}\")\n",
    "\n",
    "# Example usage:\n",
    "# XOR input and output sequences\n",
    "X = np.array([[[0], [0]], [[0], [1]], [[1], [0]], [[1], [1]]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "train_xor_rnn(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
