{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69780de6-7a11-4bd7-af56-d0a5cedf1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b2d03-ef14-4856-aa04-cd4b497bb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_docs = [\n",
    "    re.sub(r\"[^a-zA-Z\\s]\",\"\",open(f\"./txt_sentoken/neg/{file}\").read())\n",
    "    for file in os.listdir('./txt_sentoken/neg/')\n",
    "]\n",
    "pos_docs = [\n",
    "    re.sub(r\"[^a-zA-Z\\s]\",\"\",open(f\"./txt_sentoken/pos/{file}\").read())\n",
    "    for file in os.listdir('./txt_sentoken/pos/')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68474f8c-94d1-44e3-815f-7dc076ff92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [re.sub(r\"[^a-zA-Z\\s]\",\"\",word) for word in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83d7514-a7b3-4fff-a6b0-f18325a94641",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = [\n",
    "    list(filter(\n",
    "        lambda x: (x not in stop_words and len(x) > 1),\n",
    "        re.findall(r'\\S+', doc)\n",
    "    ))\n",
    "    for doc in neg_docs\n",
    "]\n",
    "pos_words = [\n",
    "    list(filter(\n",
    "        lambda x: (x not in stop_words and len(x) > 1),\n",
    "        re.findall(r'\\S+', doc)\n",
    "    ))\n",
    "    for doc in pos_docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ee3657-c094-4e88-9b05-a477b70e1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(neg_words)\n",
    "random.shuffle(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859fbc0d-ddaf-4dcd-89dc-b0306e41c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = 800\n",
    "train_x = pos_words[:split_point] + neg_words[:split_point]\n",
    "test_x = pos_words[split_point:] + neg_words[split_point:]\n",
    "train_y = np.array([1 for _ in range(split_point)] + [0 for _ in range(split_point)])\n",
    "test_y = np.array([1 for _ in range(split_point)] + [0 for _ in range(split_point)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb6c21f-e77f-4f5e-a399-614687e5aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 18:33:02.390190: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-07 18:33:02.442432: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-07 18:33:02.444006: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-07 18:33:03.298550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Conv1D, MaxPool1D, Dropout\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68e75254-203e-421c-a563-b9a9aaf1d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "encoded = tokenizer.texts_to_sequences(train_x)\n",
    "tokenizer.fit_on_texts(test_x)\n",
    "encoded_test = tokenizer.texts_to_sequences(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "395cd366-3d94-435b-99c3-8734ce54b1d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1600,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sen) \u001b[38;5;241m>\u001b[39m dataset_maxlen:\n\u001b[1;32m      4\u001b[0m         dataset_maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sen)\n\u001b[0;32m----> 5\u001b[0m padded \u001b[38;5;241m=\u001b[39m pad_sequences(encoded, maxlen\u001b[38;5;241m=\u001b[39mdataset_maxlen, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m padded_test \u001b[38;5;241m=\u001b[39m pad_sequences(encoded_test, maxlen\u001b[38;5;241m=\u001b[39mdataset_maxlen, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/data_utils.py:1097\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x):\n\u001b[0;32m-> 1097\u001b[0m         sample_shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   1098\u001b[0m         flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1600,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "dataset_maxlen = 0\n",
    "for sen in pos_words + neg_words:\n",
    "    if len(sen) > dataset_maxlen:\n",
    "        dataset_maxlen = len(sen)\n",
    "padded = pad_sequences(encoded, maxlen=dataset_maxlen, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=dataset_maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829108c-48a5-4fc1-8e50-96e76a5c860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "806b39de-f0df-4a92-9f3c-ec9c64a5defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_len, 100, input_length=dataset_maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "405cf7a8-f4b4-4813-bec7-050120a176df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e713473-b839-4557-919c-01b092c24c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1374, 100)         4660500   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1371, 32)          12832     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1371, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 685, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 21920)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                219210    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4892553 (18.66 MB)\n",
      "Trainable params: 4892553 (18.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5cda5d9-5215-4fb7-9a0a-6b43a255a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert the argument `type_value`: Int64Dtype() to a TensorFlow DType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(padded,train_y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(padded_test, test_y))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:846\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_value, _dtypes\u001b[38;5;241m.\u001b[39mDType):\n\u001b[1;32m    844\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _INTERN_TABLE[type_value\u001b[38;5;241m.\u001b[39mas_datatype_enum]\n\u001b[0;32m--> 846\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert the argument `type_value`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_value\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto a TensorFlow DType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: Int64Dtype() to a TensorFlow DType."
     ]
    }
   ],
   "source": [
    "model.fit(padded,train_y, epochs=10, batch_size=20, validation_data=(padded_test, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa1ff5-ae29-4e9e-8b49-c413ad79f0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakernel",
   "language": "python",
   "name": "condakernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
