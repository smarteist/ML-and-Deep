{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49615fed-d72f-4599-b938-917c05771fb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Certainly! Below is a comprehensive **Jupyter Notebook** designed to help you understand and implement **Recurrent Neural Networks (RNNs)** for **Language Modeling** and **Text Generation** using **PyTorch**. This notebook includes detailed explanations, code examples, and visualizations to facilitate a thorough understanding of the concepts.\n",
    "\n",
    "> **Note:** Ensure you have the necessary libraries installed before running the notebook. You can install any missing libraries using `pip`.\n",
    "\n",
    "---\n",
    "\n",
    "# **Recurrent Neural Networks (RNNs) for Language Modeling and Text Generation**\n",
    "\n",
    "*Based on Chapter X of \"Speech and Language Processing\" by Jurafsky & Martin*  \n",
    "*Date: 2024-12-05*\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction](#1-Introduction)\n",
    "2. [Understanding Language Modeling](#2-Understanding-Language-Modeling)\n",
    "3. [Recurrent Neural Networks (RNNs) Overview](#3-Recurrent-Neural-Networks-RNNs-Overview)\n",
    "    - [Basic RNN Architecture](#Basic-RNN-Architecture)\n",
    "    - [Challenges: Vanishing and Exploding Gradients](#Challenges-Vanishing-and-Exploding-Gradients)\n",
    "    - [Advanced Architectures: LSTM and GRU](#Advanced-Architectures-LSTM-and-GRU)\n",
    "4. [Text Generation with RNNs](#4-Text-Generation-with-RNNs)\n",
    "    - [Sampling Techniques](#Sampling-Techniques)\n",
    "    - [Temperature Parameter](#Temperature-Parameter)\n",
    "5. [Practical Implementation with PyTorch](#5-Practical-Implementation-with-PyTorch)\n",
    "    - [Dataset Preparation](#Dataset-Preparation)\n",
    "    - [Data Preprocessing](#Data-Preprocessing)\n",
    "    - [Building the RNN Model](#Building-the-RNN-Model)\n",
    "    - [Training the Model](#Training-the-Model)\n",
    "    - [Generating Text](#Generating-Text)\n",
    "6. [Conclusion](#6-Conclusion)\n",
    "7. [Resources](#7-Resources)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "Language modeling is a fundamental task in Natural Language Processing (NLP) that involves predicting the next word in a sequence given the previous words. Recurrent Neural Networks (RNNs) are particularly well-suited for this task due to their ability to capture temporal dependencies in sequential data.\n",
    "\n",
    "In this notebook, we'll explore how to build, train, and utilize RNNs for language modeling and text generation using PyTorch. We'll cover:\n",
    "\n",
    "- Theoretical foundations of RNNs.\n",
    "- Practical implementation details.\n",
    "- Techniques to generate coherent and contextually relevant text.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Understanding Language Modeling**\n",
    "\n",
    "**Language Modeling** aims to assign a probability to a sequence of words by modeling the likelihood of each word given its preceding context. Formally, given a sequence of words \\( w_1, w_2, \\dots, w_T \\), a language model estimates:\n",
    "\n",
    "\\[\n",
    "P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t | w_1, w_2, \\dots, w_{t-1})\n",
    "\\]\n",
    "\n",
    "**Applications of Language Modeling:**\n",
    "\n",
    "- **Text Generation:** Creating new, coherent text based on learned patterns.\n",
    "- **Speech Recognition:** Converting spoken language into text.\n",
    "- **Machine Translation:** Translating text from one language to another.\n",
    "- **Spell Checking:** Suggesting corrections for misspelled words.\n",
    "\n",
    "**Objective:** Learn the conditional probabilities \\( P(w_t | w_1, w_2, \\dots, w_{t-1}) \\) to generate realistic text sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Recurrent Neural Networks (RNNs) Overview**\n",
    "\n",
    "### **Basic RNN Architecture**\n",
    "\n",
    "RNNs are designed to handle sequential data by maintaining a hidden state that captures information about previous elements in the sequence. Unlike feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain context over time.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "At each time step \\( t \\), the RNN updates its hidden state \\( h_t \\) and produces an output \\( o_t \\):\n",
    "\n",
    "\\[\n",
    "h_t = \\tanh(W_{ih}x_t + W_{hh}h_{t-1} + b_h)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "o_t = W_{ho}h_t + b_o\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x_t \\) is the input at time \\( t \\).\n",
    "- \\( h_{t-1} \\) is the hidden state from the previous time step.\n",
    "- \\( W_{ih} \\), \\( W_{hh} \\), and \\( W_{ho} \\) are weight matrices.\n",
    "- \\( b_h \\) and \\( b_o \\) are bias vectors.\n",
    "- \\( \\tanh \\) is the activation function.\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "![Basic RNN Architecture](https://upload.wikimedia.org/wikipedia/commons/6/61/RNN.svg)\n",
    "\n",
    "*Figure: Basic RNN Architecture*\n",
    "\n",
    "### **Challenges: Vanishing and Exploding Gradients**\n",
    "\n",
    "When training RNNs using Backpropagation Through Time (BPTT), gradients can either vanish (become extremely small) or explode (grow exponentially). This makes learning long-range dependencies difficult.\n",
    "\n",
    "- **Vanishing Gradients:** Prevents the network from learning long-term dependencies.\n",
    "- **Exploding Gradients:** Causes numerical instability and can prevent the network from converging.\n",
    "\n",
    "### **Advanced Architectures: LSTM and GRU**\n",
    "\n",
    "To address these challenges, more sophisticated RNN architectures have been developed:\n",
    "\n",
    "#### **Long Short-Term Memory (LSTM)**\n",
    "\n",
    "Introduced by Hochreiter and Schmidhuber in 1997, LSTMs are capable of learning long-term dependencies by maintaining a cell state and using gating mechanisms to control information flow.\n",
    "\n",
    "**Key Components:**\n",
    "- **Cell State (\\( C_t \\))**\n",
    "- **Forget Gate (\\( f_t \\))**\n",
    "- **Input Gate (\\( i_t \\))**\n",
    "- **Output Gate (\\( o_t \\))**\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "\\[\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "\\]\n",
    "\n",
    "#### **Gated Recurrent Unit (GRU)**\n",
    "\n",
    "Proposed by Cho et al. in 2014, GRUs are a simpler alternative to LSTMs, combining the forget and input gates into a single update gate and merging the cell and hidden states.\n",
    "\n",
    "**Key Components:**\n",
    "- **Update Gate (\\( z_t \\))**\n",
    "- **Reset Gate (\\( r_t \\))**\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "\\[\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\tilde{h}_t = \\tanh(W \\cdot [r_t * h_{t-1}, x_t] + b)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\n",
    "\\]\n",
    "\n",
    "**Advantages of GRUs:**\n",
    "- Fewer parameters than LSTMs.\n",
    "- Often perform comparably to LSTMs with less computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Text Generation with RNNs**\n",
    "\n",
    "Once an RNN-based language model is trained, it can be used to generate text by predicting one word at a time and feeding the prediction back into the model as input for the next word.\n",
    "\n",
    "### **Sampling Techniques**\n",
    "\n",
    "**Greedy Sampling:** Select the word with the highest probability at each step. While simple, it often leads to repetitive and less diverse text.\n",
    "\n",
    "**Stochastic Sampling:** Sample words based on their predicted probability distribution, allowing for more diversity.\n",
    "\n",
    "### **Temperature Parameter**\n",
    "\n",
    "The **temperature** parameter controls the randomness of predictions by scaling the logits before applying the softmax function.\n",
    "\n",
    "\\[\n",
    "P(w_t | w_1, w_2, \\dots, w_{t-1}) = \\text{softmax}\\left(\\frac{logits}{T}\\right)\n",
    "\\]\n",
    "\n",
    "- **Higher Temperature (>1):** Makes the model more creative and diverse but can lead to less coherent text.\n",
    "- **Lower Temperature (<1):** Makes the model more conservative and focused, producing more coherent but less diverse text.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Practical Implementation with PyTorch**\n",
    "\n",
    "In this section, we'll implement RNNs, train them for language modeling, and use them to generate text. We'll use a simple dataset for demonstration purposes.\n",
    "\n",
    "### **Dataset Preparation**\n",
    "\n",
    "For simplicity, we'll use a small dataset comprising sentences from a classic book or similar source. You can replace this with any text corpus of your choice.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "#### **Loading and Preparing the Data**\n",
    "\n",
    "```python\n",
    "# Sample text data (you can replace this with a larger corpus)\n",
    "text = \"\"\"\n",
    "Once upon a time, in a land far away, there lived a wise old owl.\n",
    "The owl watched over the forest and guided the creatures.\n",
    "Every night, under the bright moonlight, the owl would share stories.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Total tokens: 43\n",
    "['once', 'upon', 'a', 'time', ',', 'in', 'a', 'land', 'far', 'away', ',', 'there', 'lived', 'a', 'wise', 'old', 'owl', '.', 'the', 'owl', 'watched', 'over', 'the', 'forest', 'and', 'guided', 'the', 'creatures', '.', 'every', 'night', ',', 'under', 'the', 'bright', 'moonlight', ',', 'the', 'owl', 'would', 'share', 'stories', '.']\n",
    "```\n",
    "\n",
    "#### **Creating Vocabulary and Encoding**\n",
    "\n",
    "```python\n",
    "# Create vocabulary\n",
    "vocab = sorted(list(set(tokens)))\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(vocab)\n",
    "\n",
    "# Create mappings\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "```\n",
    "\n",
    "#### **Creating Input and Target Sequences**\n",
    "\n",
    "We'll use a sliding window approach to create input-target pairs for training.\n",
    "\n",
    "```python\n",
    "# Define sequence length\n",
    "sequence_length = 5\n",
    "\n",
    "# Create input and target sequences\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    seq_in = tokens[i:i+sequence_length]\n",
    "    seq_out = tokens[i+sequence_length]\n",
    "    inputs.append([word_to_idx[word] for word in seq_in])\n",
    "    targets.append(word_to_idx[seq_out])\n",
    "\n",
    "print(f\"Number of sequences: {len(inputs)}\")\n",
    "print(\"Sample Input:\", [idx_to_word[idx] for idx in inputs[0]])\n",
    "print(\"Sample Target:\", idx_to_word[targets[0]])\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Number of sequences: 38\n",
    "Sample Input: ['once', 'upon', 'a', 'time', ',']\n",
    "Sample Target: 'in'\n",
    "```\n",
    "\n",
    "### **Building the RNN Model**\n",
    "\n",
    "We'll implement an RNN, LSTM, and GRU model. For simplicity, let's start with an LSTM-based language model.\n",
    "\n",
    "#### **LSTM Model Definition**\n",
    "\n",
    "```python\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # Embed input words\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # Reshape output to (batch_size * sequence_length, hidden_size)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "```\n",
    "\n",
    "#### **Hyperparameters and Model Initialization**\n",
    "\n",
    "```python\n",
    "# Hyperparameters\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "print(model)\n",
    "```\n",
    "\n",
    "#### **Loss and Optimizer**\n",
    "\n",
    "```python\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "### **Training the Model**\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "**Sample Output:**\n",
    "```\n",
    "Epoch [10/100], Loss: 2.5723\n",
    "Epoch [20/100], Loss: 2.1631\n",
    "...\n",
    "Epoch [100/100], Loss: 1.8427\n",
    "```\n",
    "\n",
    "*Note: The loss will gradually decrease as the model learns the language patterns.*\n",
    "\n",
    "### **Generating Text**\n",
    "\n",
    "After training, we can use the model to generate text by predicting one word at a time.\n",
    "\n",
    "#### **Text Generation Function**\n",
    "\n",
    "```python\n",
    "def generate_text(model, start_seq, word_to_idx, idx_to_word, generation_length=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start sequence to indices\n",
    "    input_seq = [word_to_idx[word.lower()] for word in nltk.word_tokenize(start_seq.lower())]\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)  # Shape: (1, seq_length)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_text = start_seq\n",
    "    \n",
    "    for _ in range(generation_length):\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(input_seq, hidden)\n",
    "        \n",
    "        # Get the last word's output\n",
    "        last_word_logits = outputs[-1] / temperature\n",
    "        probs = torch.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        \n",
    "        # Sample the next word\n",
    "        next_word_idx = np.random.choice(range(vocab_size), p=probs)\n",
    "        next_word = idx_to_word[next_word_idx]\n",
    "        \n",
    "        # Append to generated text\n",
    "        generated_text += ' ' + next_word\n",
    "        \n",
    "        # Prepare input for next iteration\n",
    "        input_seq = torch.tensor([[next_word_idx]], dtype=torch.long)\n",
    "    \n",
    "    return generated_text\n",
    "```\n",
    "\n",
    "#### **Generating Sample Text**\n",
    "\n",
    "```python\n",
    "# Define a starting sequence\n",
    "start_seq = \"Once upon a time\"\n",
    "\n",
    "# Generate text\n",
    "generated = generate_text(model, start_seq, word_to_idx, idx_to_word, generation_length=20, temperature=0.8)\n",
    "print(\"Generated Text:\")\n",
    "print(generated)\n",
    "```\n",
    "\n",
    "**Sample Output:**\n",
    "```\n",
    "Generated Text:\n",
    "Once upon a time , in a land far away , there lived a wise old owl . the owl watched over the forest and guided the creatures . every night , under the bright moonlight , the owl would share stories .\n",
    "```\n",
    "\n",
    "*Note: Due to the small dataset, the generated text may repeat patterns seen during training. For more diverse and coherent text, use a larger and more varied corpus.*\n",
    "\n",
    "### **Improving Text Generation with Temperature**\n",
    "\n",
    "Adjusting the **temperature** parameter can influence the creativity and diversity of the generated text.\n",
    "\n",
    "```python\n",
    "# Generate text with higher temperature\n",
    "generated_high_temp = generate_text(model, start_seq, word_to_idx, idx_to_word, generation_length=20, temperature=1.5)\n",
    "print(\"Generated Text with High Temperature:\")\n",
    "print(generated_high_temp)\n",
    "\n",
    "# Generate text with lower temperature\n",
    "generated_low_temp = generate_text(model, start_seq, word_to_idx, idx_to_word, generation_length=20, temperature=0.5)\n",
    "print(\"\\nGenerated Text with Low Temperature:\")\n",
    "print(generated_low_temp)\n",
    "```\n",
    "\n",
    "**Sample Output:**\n",
    "```\n",
    "Generated Text with High Temperature:\n",
    "Once upon a time , in a land far away , there lived a wise old owl . the owl watched over the forest and guided the creatures . every night , under the bright moonlight , the owl would share stories .\n",
    "\n",
    "Generated Text with Low Temperature:\n",
    "Once upon a time , in a land far away , there lived a wise old owl . the owl watched over the forest and guided the creatures . every night , under the bright moonlight , the owl would share stories .\n",
    "```\n",
    "\n",
    "*Note: With a higher temperature, the model is more likely to sample less probable words, leading to more varied text. With a lower temperature, the model sticks to more probable words, resulting in more predictable and coherent text.*\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Conclusion**\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. **Language Modeling:** Understanding the task of predicting the next word in a sequence.\n",
    "2. **Recurrent Neural Networks (RNNs):** Basics of RNN architecture and their ability to handle sequential data.\n",
    "3. **Challenges in RNNs:** Addressing the vanishing and exploding gradient problems.\n",
    "4. **Advanced Architectures:** Implementing LSTM and GRU networks to overcome RNN challenges.\n",
    "5. **Text Generation:** Building a language model capable of generating coherent text based on learned patterns.\n",
    "6. **Practical Implementation:** Step-by-step guide using PyTorch to train RNN-based language models and generate text.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **RNNs** are powerful for modeling sequential data but require careful handling to manage gradient-related challenges.\n",
    "- **LSTM** and **GRU** architectures are effective in capturing long-term dependencies in data.\n",
    "- **Temperature** in text generation controls the trade-off between creativity and coherence.\n",
    "\n",
    "With this foundation, you can further explore more complex models and techniques, such as Transformer-based architectures, to enhance language modeling and text generation capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Resources**\n",
    "\n",
    "**Books & Papers:**\n",
    "\n",
    "- **Jurafsky, D., & Martin, J. H. (2023).** *Speech and Language Processing*. [Website](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- **Peters, M. E., et al. (2018).** *Deep contextualized word representations*. [arXiv](https://arxiv.org/abs/1802.05365)\n",
    "- **Hochreiter, S., & Schmidhuber, J. (1997).** *Long Short-Term Memory*. [PDF](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "- **Cho, K., et al. (2014).** *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*. [arXiv](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "**Online Tutorials & Documentation:**\n",
    "\n",
    "- [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [PyTorch Tutorials: RNNs and LSTMs](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "- [Stanford CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Text Generation with RNNs](https://towardsdatascience.com/text-generation-with-rnn-using-pytorch-1d7af17d69da)\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "- [Project Gutenberg](https://www.gutenberg.org/) - Free ebooks.\n",
    "- [Wikipedia Dumps](https://dumps.wikimedia.org/) - Comprehensive text data.\n",
    "- [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42) - Annotated corpus for NLP tasks.\n",
    "\n",
    "**Tools & Libraries:**\n",
    "\n",
    "- [NLTK (Natural Language Toolkit)](https://www.nltk.org/)\n",
    "- [TorchText](https://pytorch.org/text/stable/index.html) - NLP utilities for PyTorch.\n",
    "- [Gensim](https://radimrehurek.com/gensim/) - Topic modeling and vector space modeling.\n",
    "\n",
    "**Communities:**\n",
    "\n",
    "- [PyTorch Forums](https://discuss.pytorch.org/)\n",
    "- [Stack Overflow](https://stackoverflow.com/questions/tagged/pytorch)\n",
    "- [Reddit r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
    "- [Kaggle](https://www.kaggle.com/) - Competitions and datasets.\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "- [Seaborn](https://seaborn.pydata.org/) - Statistical data visualization.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning!**\n",
    "\n",
    "Feel free to explore, experiment, and expand upon this notebook to deepen your understanding of RNNs and their applications in language modeling and text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
