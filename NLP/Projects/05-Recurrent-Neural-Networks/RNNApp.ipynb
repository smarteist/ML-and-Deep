{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfed5867ee7fba7",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Language Modeling for Text Generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- **Introduce** the concept of Recurrent Neural Networks (RNNs) and why they are useful for sequence modeling.\n",
    "- **Explore** one of the key applications of RNNs: language modeling and text generation.\n",
    "- **Implement** a simple RNN-based language model using PyTorch.\n",
    "- **Generate** new text sequences from the trained model.\n",
    "\n",
    "**Resources for Further Reading:**\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- Familiarity with Python and basic machine learning concepts.\n",
    "- A basic understanding of feedforward neural networks.\n",
    "\n",
    "**Note:** RNNs have largely been supplanted in many NLP tasks by architectures like Transformers (e.g., BERT, GPT), but RNNs are still crucial to understand for foundational knowledge and certain specialized sequence tasks.\n",
    "\n",
    "## What Are Recurrent Neural Networks?\n",
    "\n",
    "Traditional neural networks assume that inputs are independent of each other. However, this isn't always the case, especially with sequential data like text, time-series, or any data that has a notion of order.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. They maintain a **hidden state** that acts as a kind of memory of what has been processed so far. At each timestep, the RNN takes in:\n",
    "\n",
    "- The current input vector\n",
    "- The previous hidden state\n",
    "\n",
    "It then outputs a new hidden state. This recurrence allows RNNs to \"remember\" previous inputs in a sequence.\n",
    "\n",
    "Formally, for a sequence of inputs \\((x_1, x_2, ..., x_T)\\), an RNN computes:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n",
    "$$\n",
    "\n",
    "and often we also produce an output \\( y_t \\):\n",
    "\n",
    "$$\n",
    "y_t = W_{hy}h_t\n",
    "$$\n",
    "\n",
    "where \\(h_t\\) is the hidden state at time t, and \\(f\\) is often a nonlinearity such as \\(\\tanh\\).\n",
    "\n",
    "**Problems with Vanilla RNNs:**  \n",
    "RNNs struggle with long-term dependencies due to issues like vanishing and exploding gradients. This led to the development of more sophisticated variants like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units).\n",
    "\n",
    "## Language Modeling and Text Generation\n",
    "\n",
    "A language model assigns probabilities to sequences of words. For example, it estimates the probability of a sentence:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, w_2, ..., w_{t-1})\n",
    "$$\n",
    "\n",
    "An RNN-based language model uses the hidden state to encode the history of words seen so far. The model is trained to predict the next word given the previous words.\n",
    "\n",
    "Once we train such a model on a corpus of text, we can use it for:\n",
    "\n",
    "- **Text generation:** Start with a seed text and sample from the model to generate new sentences.\n",
    "- **Other NLP tasks:** Language modeling is a fundamental building block for many downstream tasks.\n",
    "\n",
    "In this tutorial, we'll focus on a simple text generation task. We'll:\n",
    "\n",
    "1. **Load** a text corpus.\n",
    "2. **Preprocess** it into a suitable form (convert words or characters into integers).\n",
    "3. **Train** an RNN-based model to predict the next token.\n",
    "4. **Generate** new text from the trained model.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll use PyTorch for building and training the RNN model. Make sure you have PyTorch installed.\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "We'll also use standard Python libraries for data loading and text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0d58f92c44c4890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:01.446257Z",
     "iopub.status.busy": "2024-12-09T18:07:01.445897Z",
     "iopub.status.idle": "2024-12-09T18:07:01.452901Z",
     "shell.execute_reply": "2024-12-09T18:07:01.452039Z",
     "shell.execute_reply.started": "2024-12-09T18:07:01.446226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Enable GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b508a2a107c79",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "For simplicity, let's use a public domain text. We'll download a small text like a part of Shakespeare's works (public domain).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b5ad45b5c49de42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:01.458651Z",
     "iopub.status.busy": "2024-12-09T18:07:01.458019Z",
     "iopub.status.idle": "2024-12-09T18:07:01.663198Z",
     "shell.execute_reply": "2024-12-09T18:07:01.662291Z",
     "shell.execute_reply.started": "2024-12-09T18:07:01.458622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "Sample text:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's download a small text snippet (if you have no internet, you can just define text manually)\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(\"Length of text:\", len(text))\n",
    "print(\"Sample text:\\n\", text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c70adb27dde62a",
   "metadata": {},
   "source": [
    "\n",
    "We have a large text (Shakespeare). For demonstration, this will do. If the text is too large for memory or training time, we can truncate it for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de6afc87ee813984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:01.665603Z",
     "iopub.status.busy": "2024-12-09T18:07:01.664845Z",
     "iopub.status.idle": "2024-12-09T18:07:01.671640Z",
     "shell.execute_reply": "2024-12-09T18:07:01.670772Z",
     "shell.execute_reply.started": "2024-12-09T18:07:01.665559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 59\n"
     ]
    }
   ],
   "source": [
    "# Let's shorten the text to make training faster for demonstration\n",
    "# In a real scenario, you'd keep the full text.\n",
    "text = text[:50000]\n",
    "\n",
    "# Let's consider character-level modeling for simplicity.\n",
    "# We'll map each unique character to an integer.\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Unique characters:\", chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a3b26ab3e6b8f",
   "metadata": {},
   "source": [
    "We'll split the text into sequences. For example, we can chunk the text into sequences of a fixed length, and the model will learn to predict the next character from the previous characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e99e4d2943834d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:01.672783Z",
     "iopub.status.busy": "2024-12-09T18:07:01.672562Z",
     "iopub.status.idle": "2024-12-09T18:07:02.271150Z",
     "shell.execute_reply": "2024-12-09T18:07:02.270418Z",
     "shell.execute_reply.started": "2024-12-09T18:07:01.672761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (49900, 100) (49900,)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100  # length of the input sequence\n",
    "step_size = 1     # how far to step through the text each time\n",
    "\n",
    "def create_dataset(text, seq_length, step_size):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text)-seq_length, step_size):\n",
    "        seq_in = text[i:i+seq_length]\n",
    "        seq_out = text[i+seq_length]\n",
    "        inputs.append([char_to_idx[ch] for ch in seq_in])\n",
    "        targets.append(char_to_idx[seq_out])\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "X, Y = create_dataset(text, seq_length, step_size)\n",
    "print(\"Dataset size:\", X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49f83257f7163d",
   "metadata": {},
   "source": [
    "We'll now create a PyTorch `Dataset` and `DataLoader` to handle batching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edbb7545b57a0884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T14:27:11.777333Z",
     "start_time": "2024-12-09T14:27:11.494769Z"
    },
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:02.274285Z",
     "iopub.status.busy": "2024-12-09T18:07:02.273638Z",
     "iopub.status.idle": "2024-12-09T18:07:02.281006Z",
     "shell.execute_reply": "2024-12-09T18:07:02.279994Z",
     "shell.execute_reply.started": "2024-12-09T18:07:02.274255Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "dataset = TextDataset(X, Y)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e0748f0a935d8",
   "metadata": {},
   "source": [
    "\n",
    "## Defining the RNN Model\n",
    "\n",
    "We will use a simple `nn.LSTM` or `nn.RNN` layer for our model. The model will:\n",
    "\n",
    "- Take a sequence of character indices as input.\n",
    "- Embed them into a vector space.\n",
    "- Feed the embeddings into an RNN (LSTM or GRU).\n",
    "- Project the output to vocabulary size to predict the next character.\n",
    "\n",
    "Let's use `nn.LSTM` for better handling of long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2561a7be55413a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:02.282627Z",
     "iopub.status.busy": "2024-12-09T18:07:02.282322Z",
     "iopub.status.idle": "2024-12-09T18:07:02.292367Z",
     "shell.execute_reply": "2024-12-09T18:07:02.291539Z",
     "shell.execute_reply.started": "2024-12-09T18:07:02.282602Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # LSTM hidden state: (num_layers, batch_size, hidden_dim)\n",
    "        # LSTM cell state: (num_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46c9e8dd9179454b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:02.293547Z",
     "iopub.status.busy": "2024-12-09T18:07:02.293244Z",
     "iopub.status.idle": "2024-12-09T18:07:02.317660Z",
     "shell.execute_reply": "2024-12-09T18:07:02.316816Z",
     "shell.execute_reply.started": "2024-12-09T18:07:02.293496Z"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = CharRNN(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3c04a904aa560",
   "metadata": {},
   "source": [
    "\n",
    "## Training the Model\n",
    "\n",
    "We'll train the model for a few epochs. Note that training language models can be time-consuming and may require more epochs or larger models for good results. Here, we'll just run a few epochs to illustrate the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bae83b863d7f813e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:07:02.318909Z",
     "iopub.status.busy": "2024-12-09T18:07:02.318658Z",
     "iopub.status.idle": "2024-12-09T18:12:30.566187Z",
     "shell.execute_reply": "2024-12-09T18:12:30.565384Z",
     "shell.execute_reply.started": "2024-12-09T18:07:02.318886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/21], Step [100/779], Loss: 2.9117\n",
      "Epoch [1/21], Step [200/779], Loss: 2.6223\n",
      "Epoch [1/21], Step [300/779], Loss: 2.4781\n",
      "Epoch [1/21], Step [400/779], Loss: 2.3798\n",
      "Epoch [1/21], Step [500/779], Loss: 2.3109\n",
      "Epoch [1/21], Step [600/779], Loss: 2.2480\n",
      "Epoch [1/21], Step [700/779], Loss: 2.1952\n",
      "Epoch [2/21], Step [100/779], Loss: 1.7705\n",
      "Epoch [2/21], Step [200/779], Loss: 1.7581\n",
      "Epoch [2/21], Step [300/779], Loss: 1.7480\n",
      "Epoch [2/21], Step [400/779], Loss: 1.7420\n",
      "Epoch [2/21], Step [500/779], Loss: 1.7346\n",
      "Epoch [2/21], Step [600/779], Loss: 1.7235\n",
      "Epoch [2/21], Step [700/779], Loss: 1.7191\n",
      "Epoch [3/21], Step [100/779], Loss: 1.5600\n",
      "Epoch [3/21], Step [200/779], Loss: 1.5582\n",
      "Epoch [3/21], Step [300/779], Loss: 1.5568\n",
      "Epoch [3/21], Step [400/779], Loss: 1.5612\n",
      "Epoch [3/21], Step [500/779], Loss: 1.5655\n",
      "Epoch [3/21], Step [600/779], Loss: 1.5727\n",
      "Epoch [3/21], Step [700/779], Loss: 1.5717\n",
      "Epoch [4/21], Step [100/779], Loss: 1.4111\n",
      "Epoch [4/21], Step [200/779], Loss: 1.4347\n",
      "Epoch [4/21], Step [300/779], Loss: 1.4387\n",
      "Epoch [4/21], Step [400/779], Loss: 1.4494\n",
      "Epoch [4/21], Step [500/779], Loss: 1.4537\n",
      "Epoch [4/21], Step [600/779], Loss: 1.4599\n",
      "Epoch [4/21], Step [700/779], Loss: 1.4645\n",
      "Epoch [5/21], Step [100/779], Loss: 1.3163\n",
      "Epoch [5/21], Step [200/779], Loss: 1.3201\n",
      "Epoch [5/21], Step [300/779], Loss: 1.3416\n",
      "Epoch [5/21], Step [400/779], Loss: 1.3451\n",
      "Epoch [5/21], Step [500/779], Loss: 1.3551\n",
      "Epoch [5/21], Step [600/779], Loss: 1.3650\n",
      "Epoch [5/21], Step [700/779], Loss: 1.3711\n",
      "Epoch [6/21], Step [100/779], Loss: 1.2289\n",
      "Epoch [6/21], Step [200/779], Loss: 1.2418\n",
      "Epoch [6/21], Step [300/779], Loss: 1.2458\n",
      "Epoch [6/21], Step [400/779], Loss: 1.2633\n",
      "Epoch [6/21], Step [500/779], Loss: 1.2709\n",
      "Epoch [6/21], Step [600/779], Loss: 1.2801\n",
      "Epoch [6/21], Step [700/779], Loss: 1.2832\n",
      "Epoch [7/21], Step [100/779], Loss: 1.1438\n",
      "Epoch [7/21], Step [200/779], Loss: 1.1489\n",
      "Epoch [7/21], Step [300/779], Loss: 1.1607\n",
      "Epoch [7/21], Step [400/779], Loss: 1.1777\n",
      "Epoch [7/21], Step [500/779], Loss: 1.1927\n",
      "Epoch [7/21], Step [600/779], Loss: 1.2004\n",
      "Epoch [7/21], Step [700/779], Loss: 1.2091\n",
      "Epoch [8/21], Step [100/779], Loss: 1.0896\n",
      "Epoch [8/21], Step [200/779], Loss: 1.0937\n",
      "Epoch [8/21], Step [300/779], Loss: 1.0984\n",
      "Epoch [8/21], Step [400/779], Loss: 1.1095\n",
      "Epoch [8/21], Step [500/779], Loss: 1.1092\n",
      "Epoch [8/21], Step [600/779], Loss: 1.1219\n",
      "Epoch [8/21], Step [700/779], Loss: 1.1316\n",
      "Epoch [9/21], Step [100/779], Loss: 0.9940\n",
      "Epoch [9/21], Step [200/779], Loss: 0.9944\n",
      "Epoch [9/21], Step [300/779], Loss: 1.0115\n",
      "Epoch [9/21], Step [400/779], Loss: 1.0219\n",
      "Epoch [9/21], Step [500/779], Loss: 1.0324\n",
      "Epoch [9/21], Step [600/779], Loss: 1.0381\n",
      "Epoch [9/21], Step [700/779], Loss: 1.0500\n",
      "Epoch [10/21], Step [100/779], Loss: 0.9118\n",
      "Epoch [10/21], Step [200/779], Loss: 0.9216\n",
      "Epoch [10/21], Step [300/779], Loss: 0.9316\n",
      "Epoch [10/21], Step [400/779], Loss: 0.9405\n",
      "Epoch [10/21], Step [500/779], Loss: 0.9539\n",
      "Epoch [10/21], Step [600/779], Loss: 0.9594\n",
      "Epoch [10/21], Step [700/779], Loss: 0.9684\n",
      "Epoch [11/21], Step [100/779], Loss: 0.8008\n",
      "Epoch [11/21], Step [200/779], Loss: 0.8160\n",
      "Epoch [11/21], Step [300/779], Loss: 0.8352\n",
      "Epoch [11/21], Step [400/779], Loss: 0.8538\n",
      "Epoch [11/21], Step [500/779], Loss: 0.8662\n",
      "Epoch [11/21], Step [600/779], Loss: 0.8783\n",
      "Epoch [11/21], Step [700/779], Loss: 0.8925\n",
      "Epoch [12/21], Step [100/779], Loss: 0.7520\n",
      "Epoch [12/21], Step [200/779], Loss: 0.7589\n",
      "Epoch [12/21], Step [300/779], Loss: 0.7723\n",
      "Epoch [12/21], Step [400/779], Loss: 0.7929\n",
      "Epoch [12/21], Step [500/779], Loss: 0.8032\n",
      "Epoch [12/21], Step [600/779], Loss: 0.8160\n",
      "Epoch [12/21], Step [700/779], Loss: 0.8234\n",
      "Epoch [13/21], Step [100/779], Loss: 0.6813\n",
      "Epoch [13/21], Step [200/779], Loss: 0.6871\n",
      "Epoch [13/21], Step [300/779], Loss: 0.6984\n",
      "Epoch [13/21], Step [400/779], Loss: 0.7117\n",
      "Epoch [13/21], Step [500/779], Loss: 0.7282\n",
      "Epoch [13/21], Step [600/779], Loss: 0.7421\n",
      "Epoch [13/21], Step [700/779], Loss: 0.7555\n",
      "Epoch [14/21], Step [100/779], Loss: 0.6276\n",
      "Epoch [14/21], Step [200/779], Loss: 0.6452\n",
      "Epoch [14/21], Step [300/779], Loss: 0.6592\n",
      "Epoch [14/21], Step [400/779], Loss: 0.6670\n",
      "Epoch [14/21], Step [500/779], Loss: 0.6748\n",
      "Epoch [14/21], Step [600/779], Loss: 0.6923\n",
      "Epoch [14/21], Step [700/779], Loss: 0.7026\n",
      "Epoch [15/21], Step [100/779], Loss: 0.5664\n",
      "Epoch [15/21], Step [200/779], Loss: 0.5736\n",
      "Epoch [15/21], Step [300/779], Loss: 0.5826\n",
      "Epoch [15/21], Step [400/779], Loss: 0.5994\n",
      "Epoch [15/21], Step [500/779], Loss: 0.6141\n",
      "Epoch [15/21], Step [600/779], Loss: 0.6254\n",
      "Epoch [15/21], Step [700/779], Loss: 0.6380\n",
      "Epoch [16/21], Step [100/779], Loss: 0.5065\n",
      "Epoch [16/21], Step [200/779], Loss: 0.5175\n",
      "Epoch [16/21], Step [300/779], Loss: 0.5274\n",
      "Epoch [16/21], Step [400/779], Loss: 0.5434\n",
      "Epoch [16/21], Step [500/779], Loss: 0.5523\n",
      "Epoch [16/21], Step [600/779], Loss: 0.5660\n",
      "Epoch [16/21], Step [700/779], Loss: 0.5794\n",
      "Epoch [17/21], Step [100/779], Loss: 0.4744\n",
      "Epoch [17/21], Step [200/779], Loss: 0.4850\n",
      "Epoch [17/21], Step [300/779], Loss: 0.4996\n",
      "Epoch [17/21], Step [400/779], Loss: 0.5093\n",
      "Epoch [17/21], Step [500/779], Loss: 0.5191\n",
      "Epoch [17/21], Step [600/779], Loss: 0.5346\n",
      "Epoch [17/21], Step [700/779], Loss: 0.5470\n",
      "Epoch [18/21], Step [100/779], Loss: 0.4631\n",
      "Epoch [18/21], Step [200/779], Loss: 0.4574\n",
      "Epoch [18/21], Step [300/779], Loss: 0.4730\n",
      "Epoch [18/21], Step [400/779], Loss: 0.4800\n",
      "Epoch [18/21], Step [500/779], Loss: 0.4888\n",
      "Epoch [18/21], Step [600/779], Loss: 0.4978\n",
      "Epoch [18/21], Step [700/779], Loss: 0.5087\n",
      "Epoch [19/21], Step [100/779], Loss: 0.4325\n",
      "Epoch [19/21], Step [200/779], Loss: 0.4215\n",
      "Epoch [19/21], Step [300/779], Loss: 0.4313\n",
      "Epoch [19/21], Step [400/779], Loss: 0.4390\n",
      "Epoch [19/21], Step [500/779], Loss: 0.4506\n",
      "Epoch [19/21], Step [600/779], Loss: 0.4651\n",
      "Epoch [19/21], Step [700/779], Loss: 0.4831\n",
      "Epoch [20/21], Step [100/779], Loss: 0.3820\n",
      "Epoch [20/21], Step [200/779], Loss: 0.3956\n",
      "Epoch [20/21], Step [300/779], Loss: 0.4062\n",
      "Epoch [20/21], Step [400/779], Loss: 0.4159\n",
      "Epoch [20/21], Step [500/779], Loss: 0.4246\n",
      "Epoch [20/21], Step [600/779], Loss: 0.4335\n",
      "Epoch [20/21], Step [700/779], Loss: 0.4456\n",
      "Epoch [21/21], Step [100/779], Loss: 0.3578\n",
      "Epoch [21/21], Step [200/779], Loss: 0.3567\n",
      "Epoch [21/21], Step [300/779], Loss: 0.3653\n",
      "Epoch [21/21], Step [400/779], Loss: 0.3767\n",
      "Epoch [21/21], Step [500/779], Loss: 0.3898\n",
      "Epoch [21/21], Step [600/779], Loss: 0.4000\n",
      "Epoch [21/21], Step [700/779], Loss: 0.4134\n"
     ]
    }
   ],
   "source": [
    "epochs = 21  # Feel free to increase if you want better results\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    total_loss = 0\n",
    "    for i, (inp, target) in enumerate(dataloader):\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Detach hidden state to prevent backprop through entire history\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "        # Reshape output to (batch*seq_length, vocab_size) and targets to (batch*seq_length)\n",
    "        # Actually here seq_length is fixed, and out shape is [batch_size, seq_length, vocab_size]\n",
    "        # We only need the last character to predict? Actually we need to predict all next chars.\n",
    "        # But in this dataset, we only predicted one next char per sequence. Let's align dimensions:\n",
    "        \n",
    "        # Wait, we structured dataset so that for each sequence input of length=seq_length\n",
    "        # we have one target character (the next char). So we should only consider the last timestep:\n",
    "        # The output includes predictions for every timestep in the sequence. We only want the prediction of the last character in the sequence.\n",
    "        # The last output is out[:, -1, :]\n",
    "        \n",
    "        out = out[:, -1, :]  # get the prediction of the last time step\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {total_loss/(i+1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a89349a1575e0f",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "Now that the model is trained, we can use it to generate text. The process is:\n",
    "\n",
    "1. **Start** with a seed string (prompt).\n",
    "2. **Feed** it into the model and sample from the output distribution to select the next character.\n",
    "3. **Append** the sampled character to the seed string and use the last `seq_length` characters as input for the next step.\n",
    "4. **Repeat** for as many characters as you want to generate.\n",
    "\n",
    "**Note:** With such a short training time and a small model, the generated text will likely not be very coherent. But it should reflect some patterns from Shakespeare's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "524f6d8d7bed9885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:12:30.567981Z",
     "iopub.status.busy": "2024-12-09T18:12:30.567380Z",
     "iopub.status.idle": "2024-12-09T18:12:30.575069Z",
     "shell.execute_reply": "2024-12-09T18:12:30.574197Z",
     "shell.execute_reply.started": "2024-12-09T18:12:30.567938Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_str='ROMEO:', length=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    chars = list(start_str)\n",
    "    # We will use the model one character at a time now:\n",
    "    # To do that, we must feed the model a sequence of the last seq_length chars seen so far.\n",
    "    # If we have fewer than seq_length chars at start, we can pad or just start with fewer characters.\n",
    "    \n",
    "    # Encode the seed\n",
    "    input_seq = torch.tensor([char_to_idx[ch] for ch in chars], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # If our input_seq is longer than seq_length, we only take the last seq_length characters\n",
    "        if input_seq.size(1) < seq_length:\n",
    "            inp = input_seq\n",
    "        else:\n",
    "            inp = input_seq[:, -seq_length:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Focus on the last character's output\n",
    "        out = out[:, -1, :] / temperature\n",
    "        probs = torch.softmax(out, dim=1).squeeze()\n",
    "        # Sample from the distribution\n",
    "        idx = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        chars.append(idx_to_char[idx])\n",
    "        # Append the new character index to input_seq\n",
    "        input_seq = torch.cat([input_seq, torch.tensor([[idx]], device=device)], dim=1)\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9983cd2-24da-4359-82b8-81645abc8e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:12:30.576768Z",
     "iopub.status.busy": "2024-12-09T18:12:30.576236Z",
     "iopub.status.idle": "2024-12-09T18:12:32.056066Z",
     "shell.execute_reply": "2024-12-09T18:12:32.055173Z",
     "shell.execute_reply.started": "2024-12-09T18:12:30.576730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "We have pluck Rome.\n",
      "\n",
      "BRUTUS:\n",
      "Come, come, that were sent to see me honours,\n",
      "That they passing tree, with a man were an-hungry in his former\n",
      "To the icher sound more: whereof, if they were afoot.\n",
      "\n",
      "Second Citizen:\n",
      "Care for us thou get him and tent thouge heaven, I make a grave.\n",
      "\n",
      "COMINIUS:\n",
      "If I should tell you.\n",
      "\n",
      "BRUTUS:\n",
      "In am our Cominius.\n",
      "\n",
      "BRUTUS:\n",
      "So it must speaks! restands!\n",
      "\n",
      "SICINIUS:\n",
      "Besisat lamb the cormorn.\n",
      "\n",
      "BRUTUS:\n",
      "I am constant of these shrease, and they true,\n",
      "engn entre their like upon him.\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model, start_str=\"ROMEO:\", length=500, temperature=0.8)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59435250-a42e-49f2-bb6f-1dba86c834ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:13:35.284625Z",
     "iopub.status.busy": "2024-12-09T18:13:35.284280Z",
     "iopub.status.idle": "2024-12-09T18:13:35.528338Z",
     "shell.execute_reply": "2024-12-09T18:13:35.527499Z",
     "shell.execute_reply.started": "2024-12-09T18:13:35.284596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caius Marcius:\n",
      "Leave your commissians to visit the only sons,\n",
      "We prove that they would have their love or no.\n",
      "\n",
      "ME\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model, start_str=\"Caius Marcius\", length=100, temperature=0.4)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c130b-1270-4587-b597-1d9e0e9045a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677f9ebf1a9a901",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "You will likely see somewhat \"Shakespeare-like\" text (with letters and some semblance of English structure) but since we only trained briefly and on a small model, don't expect coherent text.\n",
    "\n",
    "With longer training (more epochs), larger hidden sizes, and better optimization, the generated text becomes more coherent. The technique demonstrated here is a foundational approach. More advanced techniques (like using LSTM, GRU, or Transformer models trained for longer) can produce very fluent text.\n",
    "\n",
    "## Further Steps\n",
    "\n",
    "- **Increase** the number of epochs.\n",
    "- **Increase** model complexity (hidden_dim, num_layers).\n",
    "- **Try** a word-level model instead of a character-level model.\n",
    "- **Experiment** with different temperatures during generation to control randomness.\n",
    "\n",
    "**Remember:** RNN-based text generation was a breakthrough approach back in the day, but it has been largely surpassed by Transformer-based models (like GPT). Nevertheless, understanding RNNs and LSTMs is a key foundational concept in sequence modeling.\n",
    "\n",
    "## References\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  \n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
