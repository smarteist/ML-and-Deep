{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9bfed5867ee7fba7","cell_type":"markdown","source":"# Recurrent Neural Networks (RNNs) and Language Modeling for Text Generation\n\n## Introduction\n\nIn this notebook, we will:\n\n- **Introduce** the concept of Recurrent Neural Networks (RNNs) and why they are useful for sequence modeling.\n- **Explore** one of the key applications of RNNs: language modeling and text generation.\n- **Implement** a simple RNN-based language model using PyTorch.\n- **Generate** new text sequences from the trained model.\n\n**Resources for Further Reading:**\n\n- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah\n\n**Prerequisites:**\n\n- Familiarity with Python and basic machine learning concepts.\n- A basic understanding of feedforward neural networks.\n\n**Note:** RNNs have largely been supplanted in many NLP tasks by architectures like Transformers (e.g., BERT, GPT), but RNNs are still crucial to understand for foundational knowledge and certain specialized sequence tasks.\n\n## What Are Recurrent Neural Networks?\n\nTraditional neural networks assume that inputs are independent of each other. However, this isn't always the case, especially with sequential data like text, time-series, or any data that has a notion of order.\n\nRecurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. They maintain a **hidden state** that acts as a kind of memory of what has been processed so far. At each timestep, the RNN takes in:\n\n- The current input vector\n- The previous hidden state\n\nIt then outputs a new hidden state. This recurrence allows RNNs to \"remember\" previous inputs in a sequence.\n\nFormally, for a sequence of inputs \\((x_1, x_2, ..., x_T)\\), an RNN computes:\n\n$$\nh_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n$$\n\nand often we also produce an output \\( y_t \\):\n\n$$\ny_t = W_{hy}h_t\n$$\n\nwhere \\(h_t\\) is the hidden state at time t, and \\(f\\) is often a nonlinearity such as \\(\\tanh\\).\n\n**Problems with Vanilla RNNs:**  \nRNNs struggle with long-term dependencies due to issues like vanishing and exploding gradients. This led to the development of more sophisticated variants like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units).\n\n## Language Modeling and Text Generation\n\nA language model assigns probabilities to sequences of words. For example, it estimates the probability of a sentence:\n\n$$\nP(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, w_2, ..., w_{t-1})\n$$\n\nAn RNN-based language model uses the hidden state to encode the history of words seen so far. The model is trained to predict the next word given the previous words.\n\nOnce we train such a model on a corpus of text, we can use it for:\n\n- **Text generation:** Start with a seed text and sample from the model to generate new sentences.\n- **Other NLP tasks:** Language modeling is a fundamental building block for many downstream tasks.\n\nIn this tutorial, we'll focus on a simple text generation task. We'll:\n\n1. **Load** a text corpus.\n2. **Preprocess** it into a suitable form (convert words or characters into integers).\n3. **Train** an RNN-based model to predict the next token.\n4. **Generate** new text from the trained model.\n\n## Setup\n\nWe'll use PyTorch for building and training the RNN model. Make sure you have PyTorch installed.\n\n**Installation:**\n\n\n```bash\npip install torch torchvision torchaudio\n```\nWe'll also use standard Python libraries for data loading and text processing.\n","metadata":{}},{"id":"e0d58f92c44c4890","cell_type":"code","source":"import numpy as np\nimport requests\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Enable GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:01.445897Z","iopub.execute_input":"2024-12-09T18:07:01.446257Z","iopub.status.idle":"2024-12-09T18:07:01.452901Z","shell.execute_reply.started":"2024-12-09T18:07:01.446226Z","shell.execute_reply":"2024-12-09T18:07:01.452039Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":40},{"id":"462b508a2a107c79","cell_type":"markdown","source":"\n## Data Preparation\n\nFor simplicity, let's use a public domain text. We'll download a small text like a part of Shakespeare's works (public domain).\n","metadata":{}},{"id":"3b5ad45b5c49de42","cell_type":"code","source":"# Let's download a small text snippet (if you have no internet, you can just define text manually)\nurl = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\nresponse = requests.get(url)\ntext = response.text\n\nprint(\"Length of text:\", len(text))\nprint(\"Sample text:\\n\", text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:01.458019Z","iopub.execute_input":"2024-12-09T18:07:01.458651Z","iopub.status.idle":"2024-12-09T18:07:01.663198Z","shell.execute_reply.started":"2024-12-09T18:07:01.458622Z","shell.execute_reply":"2024-12-09T18:07:01.662291Z"}},"outputs":[{"name":"stdout","text":"Length of text: 1115394\nSample text:\n First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":41},{"id":"71c70adb27dde62a","cell_type":"markdown","source":"\nWe have a large text (Shakespeare). For demonstration, this will do. If the text is too large for memory or training time, we can truncate it for this example.","metadata":{}},{"id":"de6afc87ee813984","cell_type":"code","source":"# Let's shorten the text to make training faster for demonstration\n# In a real scenario, you'd keep the full text.\ntext = text[:50000]\n\n# Let's consider character-level modeling for simplicity.\n# We'll map each unique character to an integer.\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(\"Unique characters:\", chars)\nprint(\"Vocab size:\", vocab_size)\n\n# Create mappings\nchar_to_idx = { ch:i for i,ch in enumerate(chars) }\nidx_to_char = { i:ch for i,ch in enumerate(chars) }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:01.664845Z","iopub.execute_input":"2024-12-09T18:07:01.665603Z","iopub.status.idle":"2024-12-09T18:07:01.671640Z","shell.execute_reply.started":"2024-12-09T18:07:01.665559Z","shell.execute_reply":"2024-12-09T18:07:01.670772Z"}},"outputs":[{"name":"stdout","text":"Unique characters: ['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nVocab size: 59\n","output_type":"stream"}],"execution_count":42},{"id":"869a3b26ab3e6b8f","cell_type":"markdown","source":"We'll split the text into sequences. For example, we can chunk the text into sequences of a fixed length, and the model will learn to predict the next character from the previous characters.\n","metadata":{}},{"id":"9e99e4d2943834d8","cell_type":"code","source":"seq_length = 100  # length of the input sequence\nstep_size = 1     # how far to step through the text each time\n\ndef create_dataset(text, seq_length, step_size):\n    inputs = []\n    targets = []\n    for i in range(0, len(text)-seq_length, step_size):\n        seq_in = text[i:i+seq_length]\n        seq_out = text[i+seq_length]\n        inputs.append([char_to_idx[ch] for ch in seq_in])\n        targets.append(char_to_idx[seq_out])\n    return np.array(inputs), np.array(targets)\n\nX, Y = create_dataset(text, seq_length, step_size)\nprint(\"Dataset size:\", X.shape, Y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:01.672562Z","iopub.execute_input":"2024-12-09T18:07:01.672783Z","iopub.status.idle":"2024-12-09T18:07:02.271150Z","shell.execute_reply.started":"2024-12-09T18:07:01.672761Z","shell.execute_reply":"2024-12-09T18:07:02.270418Z"}},"outputs":[{"name":"stdout","text":"Dataset size: (49900, 100) (49900,)\n","output_type":"stream"}],"execution_count":43},{"id":"4a49f83257f7163d","cell_type":"markdown","source":"We'll now create a PyTorch `Dataset` and `DataLoader` to handle batching.\n","metadata":{}},{"id":"edbb7545b57a0884","cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, inputs, targets):\n        self.inputs = inputs\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n\ndataset = TextDataset(X, Y)\nbatch_size = 64\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)","metadata":{"ExecuteTime":{"end_time":"2024-12-09T14:27:11.777333Z","start_time":"2024-12-09T14:27:11.494769Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:02.273638Z","iopub.execute_input":"2024-12-09T18:07:02.274285Z","iopub.status.idle":"2024-12-09T18:07:02.281006Z","shell.execute_reply.started":"2024-12-09T18:07:02.274255Z","shell.execute_reply":"2024-12-09T18:07:02.279994Z"}},"outputs":[],"execution_count":44},{"id":"aa6e0748f0a935d8","cell_type":"markdown","source":"\n## Defining the RNN Model\n\nWe will use a simple `nn.LSTM` or `nn.RNN` layer for our model. The model will:\n\n- Take a sequence of character indices as input.\n- Embed them into a vector space.\n- Feed the embeddings into an RNN (LSTM or GRU).\n- Project the output to vocabulary size to predict the next character.\n\nLet's use `nn.LSTM` for better handling of long-term dependencies.","metadata":{}},{"id":"2561a7be55413a17","cell_type":"code","source":"class CharRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n        super(CharRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n    def forward(self, x, hidden=None):\n        x = self.embedding(x)\n        out, hidden = self.lstm(x, hidden)\n        out = self.fc(out)\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        # LSTM hidden state: (num_layers, batch_size, hidden_dim)\n        # LSTM cell state: (num_layers, batch_size, hidden_dim)\n        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:02.282322Z","iopub.execute_input":"2024-12-09T18:07:02.282627Z","iopub.status.idle":"2024-12-09T18:07:02.292367Z","shell.execute_reply.started":"2024-12-09T18:07:02.282602Z","shell.execute_reply":"2024-12-09T18:07:02.291539Z"}},"outputs":[],"execution_count":45},{"id":"46c9e8dd9179454b","cell_type":"code","source":"#Instantiate the model\nmodel = CharRNN(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.002)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:02.293244Z","iopub.execute_input":"2024-12-09T18:07:02.293547Z","iopub.status.idle":"2024-12-09T18:07:02.317660Z","shell.execute_reply.started":"2024-12-09T18:07:02.293496Z","shell.execute_reply":"2024-12-09T18:07:02.316816Z"}},"outputs":[],"execution_count":46},{"id":"a0d3c04a904aa560","cell_type":"markdown","source":"\n## Training the Model\n\nWe'll train the model for a few epochs. Note that training language models can be time-consuming and may require more epochs or larger models for good results. Here, we'll just run a few epochs to illustrate the process.\n","metadata":{}},{"id":"bae83b863d7f813e","cell_type":"code","source":"epochs = 21  # Feel free to increase if you want better results\nmodel.train()\n\nfor epoch in range(epochs):\n    hidden = model.init_hidden(batch_size)\n    total_loss = 0\n    for i, (inp, target) in enumerate(dataloader):\n        inp, target = inp.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        out, hidden = model(inp, hidden)\n        \n        # Detach hidden state to prevent backprop through entire history\n        hidden = (hidden[0].detach(), hidden[1].detach())\n        \n        # Reshape output to (batch*seq_length, vocab_size) and targets to (batch*seq_length)\n        # Actually here seq_length is fixed, and out shape is [batch_size, seq_length, vocab_size]\n        # We only need the last character to predict? Actually we need to predict all next chars.\n        # But in this dataset, we only predicted one next char per sequence. Let's align dimensions:\n        \n        # Wait, we structured dataset so that for each sequence input of length=seq_length\n        # we have one target character (the next char). So we should only consider the last timestep:\n        # The output includes predictions for every timestep in the sequence. We only want the prediction of the last character in the sequence.\n        # The last output is out[:, -1, :]\n        \n        out = out[:, -1, :]  # get the prediction of the last time step\n        loss = criterion(out, target)\n        loss.backward()\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if (i+1) % 100 == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {total_loss/(i+1):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:07:02.318658Z","iopub.execute_input":"2024-12-09T18:07:02.318909Z","iopub.status.idle":"2024-12-09T18:12:30.566187Z","shell.execute_reply.started":"2024-12-09T18:07:02.318886Z","shell.execute_reply":"2024-12-09T18:12:30.565384Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/21], Step [100/779], Loss: 2.9117\nEpoch [1/21], Step [200/779], Loss: 2.6223\nEpoch [1/21], Step [300/779], Loss: 2.4781\nEpoch [1/21], Step [400/779], Loss: 2.3798\nEpoch [1/21], Step [500/779], Loss: 2.3109\nEpoch [1/21], Step [600/779], Loss: 2.2480\nEpoch [1/21], Step [700/779], Loss: 2.1952\nEpoch [2/21], Step [100/779], Loss: 1.7705\nEpoch [2/21], Step [200/779], Loss: 1.7581\nEpoch [2/21], Step [300/779], Loss: 1.7480\nEpoch [2/21], Step [400/779], Loss: 1.7420\nEpoch [2/21], Step [500/779], Loss: 1.7346\nEpoch [2/21], Step [600/779], Loss: 1.7235\nEpoch [2/21], Step [700/779], Loss: 1.7191\nEpoch [3/21], Step [100/779], Loss: 1.5600\nEpoch [3/21], Step [200/779], Loss: 1.5582\nEpoch [3/21], Step [300/779], Loss: 1.5568\nEpoch [3/21], Step [400/779], Loss: 1.5612\nEpoch [3/21], Step [500/779], Loss: 1.5655\nEpoch [3/21], Step [600/779], Loss: 1.5727\nEpoch [3/21], Step [700/779], Loss: 1.5717\nEpoch [4/21], Step [100/779], Loss: 1.4111\nEpoch [4/21], Step [200/779], Loss: 1.4347\nEpoch [4/21], Step [300/779], Loss: 1.4387\nEpoch [4/21], Step [400/779], Loss: 1.4494\nEpoch [4/21], Step [500/779], Loss: 1.4537\nEpoch [4/21], Step [600/779], Loss: 1.4599\nEpoch [4/21], Step [700/779], Loss: 1.4645\nEpoch [5/21], Step [100/779], Loss: 1.3163\nEpoch [5/21], Step [200/779], Loss: 1.3201\nEpoch [5/21], Step [300/779], Loss: 1.3416\nEpoch [5/21], Step [400/779], Loss: 1.3451\nEpoch [5/21], Step [500/779], Loss: 1.3551\nEpoch [5/21], Step [600/779], Loss: 1.3650\nEpoch [5/21], Step [700/779], Loss: 1.3711\nEpoch [6/21], Step [100/779], Loss: 1.2289\nEpoch [6/21], Step [200/779], Loss: 1.2418\nEpoch [6/21], Step [300/779], Loss: 1.2458\nEpoch [6/21], Step [400/779], Loss: 1.2633\nEpoch [6/21], Step [500/779], Loss: 1.2709\nEpoch [6/21], Step [600/779], Loss: 1.2801\nEpoch [6/21], Step [700/779], Loss: 1.2832\nEpoch [7/21], Step [100/779], Loss: 1.1438\nEpoch [7/21], Step [200/779], Loss: 1.1489\nEpoch [7/21], Step [300/779], Loss: 1.1607\nEpoch [7/21], Step [400/779], Loss: 1.1777\nEpoch [7/21], Step [500/779], Loss: 1.1927\nEpoch [7/21], Step [600/779], Loss: 1.2004\nEpoch [7/21], Step [700/779], Loss: 1.2091\nEpoch [8/21], Step [100/779], Loss: 1.0896\nEpoch [8/21], Step [200/779], Loss: 1.0937\nEpoch [8/21], Step [300/779], Loss: 1.0984\nEpoch [8/21], Step [400/779], Loss: 1.1095\nEpoch [8/21], Step [500/779], Loss: 1.1092\nEpoch [8/21], Step [600/779], Loss: 1.1219\nEpoch [8/21], Step [700/779], Loss: 1.1316\nEpoch [9/21], Step [100/779], Loss: 0.9940\nEpoch [9/21], Step [200/779], Loss: 0.9944\nEpoch [9/21], Step [300/779], Loss: 1.0115\nEpoch [9/21], Step [400/779], Loss: 1.0219\nEpoch [9/21], Step [500/779], Loss: 1.0324\nEpoch [9/21], Step [600/779], Loss: 1.0381\nEpoch [9/21], Step [700/779], Loss: 1.0500\nEpoch [10/21], Step [100/779], Loss: 0.9118\nEpoch [10/21], Step [200/779], Loss: 0.9216\nEpoch [10/21], Step [300/779], Loss: 0.9316\nEpoch [10/21], Step [400/779], Loss: 0.9405\nEpoch [10/21], Step [500/779], Loss: 0.9539\nEpoch [10/21], Step [600/779], Loss: 0.9594\nEpoch [10/21], Step [700/779], Loss: 0.9684\nEpoch [11/21], Step [100/779], Loss: 0.8008\nEpoch [11/21], Step [200/779], Loss: 0.8160\nEpoch [11/21], Step [300/779], Loss: 0.8352\nEpoch [11/21], Step [400/779], Loss: 0.8538\nEpoch [11/21], Step [500/779], Loss: 0.8662\nEpoch [11/21], Step [600/779], Loss: 0.8783\nEpoch [11/21], Step [700/779], Loss: 0.8925\nEpoch [12/21], Step [100/779], Loss: 0.7520\nEpoch [12/21], Step [200/779], Loss: 0.7589\nEpoch [12/21], Step [300/779], Loss: 0.7723\nEpoch [12/21], Step [400/779], Loss: 0.7929\nEpoch [12/21], Step [500/779], Loss: 0.8032\nEpoch [12/21], Step [600/779], Loss: 0.8160\nEpoch [12/21], Step [700/779], Loss: 0.8234\nEpoch [13/21], Step [100/779], Loss: 0.6813\nEpoch [13/21], Step [200/779], Loss: 0.6871\nEpoch [13/21], Step [300/779], Loss: 0.6984\nEpoch [13/21], Step [400/779], Loss: 0.7117\nEpoch [13/21], Step [500/779], Loss: 0.7282\nEpoch [13/21], Step [600/779], Loss: 0.7421\nEpoch [13/21], Step [700/779], Loss: 0.7555\nEpoch [14/21], Step [100/779], Loss: 0.6276\nEpoch [14/21], Step [200/779], Loss: 0.6452\nEpoch [14/21], Step [300/779], Loss: 0.6592\nEpoch [14/21], Step [400/779], Loss: 0.6670\nEpoch [14/21], Step [500/779], Loss: 0.6748\nEpoch [14/21], Step [600/779], Loss: 0.6923\nEpoch [14/21], Step [700/779], Loss: 0.7026\nEpoch [15/21], Step [100/779], Loss: 0.5664\nEpoch [15/21], Step [200/779], Loss: 0.5736\nEpoch [15/21], Step [300/779], Loss: 0.5826\nEpoch [15/21], Step [400/779], Loss: 0.5994\nEpoch [15/21], Step [500/779], Loss: 0.6141\nEpoch [15/21], Step [600/779], Loss: 0.6254\nEpoch [15/21], Step [700/779], Loss: 0.6380\nEpoch [16/21], Step [100/779], Loss: 0.5065\nEpoch [16/21], Step [200/779], Loss: 0.5175\nEpoch [16/21], Step [300/779], Loss: 0.5274\nEpoch [16/21], Step [400/779], Loss: 0.5434\nEpoch [16/21], Step [500/779], Loss: 0.5523\nEpoch [16/21], Step [600/779], Loss: 0.5660\nEpoch [16/21], Step [700/779], Loss: 0.5794\nEpoch [17/21], Step [100/779], Loss: 0.4744\nEpoch [17/21], Step [200/779], Loss: 0.4850\nEpoch [17/21], Step [300/779], Loss: 0.4996\nEpoch [17/21], Step [400/779], Loss: 0.5093\nEpoch [17/21], Step [500/779], Loss: 0.5191\nEpoch [17/21], Step [600/779], Loss: 0.5346\nEpoch [17/21], Step [700/779], Loss: 0.5470\nEpoch [18/21], Step [100/779], Loss: 0.4631\nEpoch [18/21], Step [200/779], Loss: 0.4574\nEpoch [18/21], Step [300/779], Loss: 0.4730\nEpoch [18/21], Step [400/779], Loss: 0.4800\nEpoch [18/21], Step [500/779], Loss: 0.4888\nEpoch [18/21], Step [600/779], Loss: 0.4978\nEpoch [18/21], Step [700/779], Loss: 0.5087\nEpoch [19/21], Step [100/779], Loss: 0.4325\nEpoch [19/21], Step [200/779], Loss: 0.4215\nEpoch [19/21], Step [300/779], Loss: 0.4313\nEpoch [19/21], Step [400/779], Loss: 0.4390\nEpoch [19/21], Step [500/779], Loss: 0.4506\nEpoch [19/21], Step [600/779], Loss: 0.4651\nEpoch [19/21], Step [700/779], Loss: 0.4831\nEpoch [20/21], Step [100/779], Loss: 0.3820\nEpoch [20/21], Step [200/779], Loss: 0.3956\nEpoch [20/21], Step [300/779], Loss: 0.4062\nEpoch [20/21], Step [400/779], Loss: 0.4159\nEpoch [20/21], Step [500/779], Loss: 0.4246\nEpoch [20/21], Step [600/779], Loss: 0.4335\nEpoch [20/21], Step [700/779], Loss: 0.4456\nEpoch [21/21], Step [100/779], Loss: 0.3578\nEpoch [21/21], Step [200/779], Loss: 0.3567\nEpoch [21/21], Step [300/779], Loss: 0.3653\nEpoch [21/21], Step [400/779], Loss: 0.3767\nEpoch [21/21], Step [500/779], Loss: 0.3898\nEpoch [21/21], Step [600/779], Loss: 0.4000\nEpoch [21/21], Step [700/779], Loss: 0.4134\n","output_type":"stream"}],"execution_count":47},{"id":"38a89349a1575e0f","cell_type":"markdown","source":"## Generating Text\n\nNow that the model is trained, we can use it to generate text. The process is:\n\n1. **Start** with a seed string (prompt).\n2. **Feed** it into the model and sample from the output distribution to select the next character.\n3. **Append** the sampled character to the seed string and use the last `seq_length` characters as input for the next step.\n4. **Repeat** for as many characters as you want to generate.\n\n**Note:** With such a short training time and a small model, the generated text will likely not be very coherent. But it should reflect some patterns from Shakespeare's text.","metadata":{}},{"id":"524f6d8d7bed9885","cell_type":"code","source":"def generate_text(model, start_str='ROMEO:', length=500, temperature=1.0):\n    model.eval()\n    chars = list(start_str)\n    # We will use the model one character at a time now:\n    # To do that, we must feed the model a sequence of the last seq_length chars seen so far.\n    # If we have fewer than seq_length chars at start, we can pad or just start with fewer characters.\n    \n    # Encode the seed\n    input_seq = torch.tensor([char_to_idx[ch] for ch in chars], dtype=torch.long).unsqueeze(0).to(device)\n    hidden = model.init_hidden(1)\n    \n    for _ in range(length):\n        # If our input_seq is longer than seq_length, we only take the last seq_length characters\n        if input_seq.size(1) < seq_length:\n            inp = input_seq\n        else:\n            inp = input_seq[:, -seq_length:]\n        \n        with torch.no_grad():\n            out, hidden = model(inp, hidden)\n        \n        # Focus on the last character's output\n        out = out[:, -1, :] / temperature\n        probs = torch.softmax(out, dim=1).squeeze()\n        # Sample from the distribution\n        idx = torch.multinomial(probs, 1).item()\n        \n        chars.append(idx_to_char[idx])\n        # Append the new character index to input_seq\n        input_seq = torch.cat([input_seq, torch.tensor([[idx]], device=device)], dim=1)\n    \n    return ''.join(chars)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:12:30.567380Z","iopub.execute_input":"2024-12-09T18:12:30.567981Z","iopub.status.idle":"2024-12-09T18:12:30.575069Z","shell.execute_reply.started":"2024-12-09T18:12:30.567938Z","shell.execute_reply":"2024-12-09T18:12:30.574197Z"}},"outputs":[],"execution_count":48},{"id":"b9983cd2-24da-4359-82b8-81645abc8e6c","cell_type":"code","source":"generated_text = generate_text(model, start_str=\"ROMEO:\", length=500, temperature=0.8)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:12:30.576236Z","iopub.execute_input":"2024-12-09T18:12:30.576768Z","iopub.status.idle":"2024-12-09T18:12:32.056066Z","shell.execute_reply.started":"2024-12-09T18:12:30.576730Z","shell.execute_reply":"2024-12-09T18:12:32.055173Z"}},"outputs":[{"name":"stdout","text":"ROMEO:\nWe have pluck Rome.\n\nBRUTUS:\nCome, come, that were sent to see me honours,\nThat they passing tree, with a man were an-hungry in his former\nTo the icher sound more: whereof, if they were afoot.\n\nSecond Citizen:\nCare for us thou get him and tent thouge heaven, I make a grave.\n\nCOMINIUS:\nIf I should tell you.\n\nBRUTUS:\nIn am our Cominius.\n\nBRUTUS:\nSo it must speaks! restands!\n\nSICINIUS:\nBesisat lamb the cormorn.\n\nBRUTUS:\nI am constant of these shrease, and they true,\nengn entre their like upon him.\n","output_type":"stream"}],"execution_count":49},{"id":"59435250-a42e-49f2-bb6f-1dba86c834ca","cell_type":"code","source":"generated_text = generate_text(model, start_str=\"Caius Marcius\", length=100, temperature=0.4)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:13:35.284280Z","iopub.execute_input":"2024-12-09T18:13:35.284625Z","iopub.status.idle":"2024-12-09T18:13:35.528338Z","shell.execute_reply.started":"2024-12-09T18:13:35.284596Z","shell.execute_reply":"2024-12-09T18:13:35.527499Z"}},"outputs":[{"name":"stdout","text":"Caius Marcius:\nLeave your commissians to visit the only sons,\nWe prove that they would have their love or no.\n\nME\n","output_type":"stream"}],"execution_count":52},{"id":"0b8c130b-1270-4587-b597-1d9e0e9045a3","cell_type":"markdown","source":"","metadata":{}},{"id":"677f9ebf1a9a901","cell_type":"markdown","source":"## Analysis of Results\n\nYou will likely see somewhat \"Shakespeare-like\" text (with letters and some semblance of English structure) but since we only trained briefly and on a small model, don't expect coherent text.\n\nWith longer training (more epochs), larger hidden sizes, and better optimization, the generated text becomes more coherent. The technique demonstrated here is a foundational approach. More advanced techniques (like using LSTM, GRU, or Transformer models trained for longer) can produce very fluent text.\n\n## Further Steps\n\n- **Increase** the number of epochs.\n- **Increase** model complexity (hidden_dim, num_layers).\n- **Try** a word-level model instead of a character-level model.\n- **Experiment** with different temperatures during generation to control randomness.\n\n**Remember:** RNN-based text generation was a breakthrough approach back in the day, but it has been largely surpassed by Transformer-based models (like GPT). Nevertheless, understanding RNNs and LSTMs is a key foundational concept in sequence modeling.\n\n## References\n\n- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  \n- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n```","metadata":{}}]}