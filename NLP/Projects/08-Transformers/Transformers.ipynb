{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Transformer Architecture\n\n## Introduction\n\nThe Transformer architecture introduced in the paper *\"Attention is All You Need\"* (Vaswani et al., 2017) revolutionized the field of Natural Language Processing (NLP) by relying solely on attention mechanisms, dispensing with recurrence and convolutions entirely. This design allows for more parallelization and better handling of long-range dependencies.\n\nIn this notebook, we will:\n\n1. **Explain the core components of the Transformer:**\n   - Multi-Head Self-Attention\n   - Positional Encoding\n   - Feedforward Layers\n\n2. **Show a simplified implementation in PyTorch.**\n\n**Prerequisites:**\n\n- Familiarity with PyTorch tensors and modules.\n- Basic understanding of attention mechanisms from previous studies.\n- Knowledge of linear transformations and embeddings.\n\n---\n\n## 1. The Transformer: A High-Level Overview\n\nA Transformer consists of a stack of identical layers in both the **encoder** and **decoder**. Each encoder layer primarily includes:\n\n1. **Multi-Head Self-Attention:**  \n   Allows the model to focus on different parts of the input sequence, computing attention multiple times in parallel.\n\n2. **Feedforward Layers:**  \n   A fully connected network applied to each position, expanding the representational capacity.\n\n3. **Add & Norm Steps:**  \n   Residual connections and layer normalization are applied after the attention and feedforward sub-layers.\n\nThe decoder has a similar structure but includes a second attention layer for attending to the encoder outputs and uses masking to ensure the decoder only attends to previous tokens when training.\n\n**Key Innovation:**  \nThe Transformer does not rely on recurrence or convolutions. Instead, it uses **attention** to directly relate every token in the input sequence to every other token. This allows parallel processing and better handles long-range dependencies.\n\n---\n\n## 2. Multi-Head Self-Attention\n\n### 2.1 Self-Attention Recap\n\nSelf-attention takes a sequence of embeddings (the input tokens embedded into vector space) and transforms them into a new sequence of the same length. For each position, it computes a weighted combination of all other positions, determining how much each other token influences this one.\n\n**Formula:**\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n$$\n\n- \\( Q \\) (Query): Transformed representation of the token we’re focusing on.\n- \\( K \\) (Key): Transformed representation of the context tokens.\n- \\( V \\) (Value): Transformed representation used to produce the final output.\n- \\( d_k \\): Dimensionality of the keys.\n\n### 2.2 Multi-Head Attention\n\nInstead of computing just one attention function, the Transformer uses **multiple heads**. Each head projects the queries, keys, and values into a different subspace and performs attention in parallel. The results are concatenated and projected again, allowing the model to focus on different types of relationships simultaneously.\n\n**Formula for Multi-Head:**\n\n$$\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O\n$$\n\nwhere each head \\( head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\).\n\n**Benefits:**\n\n- Different attention heads can capture different relations or types of information.\n- More expressive power than a single-head attention.\n\n---\n\n## 3. Positional Encoding\n\nThe Transformer has no inherent notion of sequence order since it does not use recurrence or convolution. To represent the position of each token within the sequence, we add a **positional encoding** to the input embeddings.\n\n**Key Idea:**\n\n- Use trigonometric functions of different frequencies:\n  \n$$\nPE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n$$\n$$\nPE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n$$\n\n- The positional encoding is added to the token embeddings, allowing the model to learn positional information and generalize to sequences longer than those seen during training.\n\n**Properties:**\n\n- Positions that are closer together produce similar positional encodings.\n- The model can learn to attend based on relative positions due to the sinusoidal pattern.\n\n---\n\n## 4. Feedforward Layers\n\nEach position in the sequence is then processed by a two-layer fully connected feedforward network (often with a ReLU or GELU non-linearity) applied identically to each position.\n\n**Formula:**\n\n$$\n\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n$$\n\n- Expands the dimensionality in the intermediate representation, increasing the representational capacity.\n- Applied pointwise to each token’s representation.\n\n**Benefit:**\n\n- Adds more depth and non-linearity to the model.\n- Helps the model capture more complex transformations of the input representations.\n\n---\n\n## 5. Example Implementation in PyTorch\n\nBelow is a simplified example of a few core Transformer components: Multi-Head Attention, Positional Encoding, and a feedforward layer. This is not a full Transformer implementation, but it highlights the core building blocks.\n\n### 5.1 Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.255686Z","iopub.execute_input":"2024-12-09T19:25:05.256215Z","iopub.status.idle":"2024-12-09T19:25:05.263319Z","shell.execute_reply.started":"2024-12-09T19:25:05.256176Z","shell.execute_reply":"2024-12-09T19:25:05.261239Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 5.2 Positional Encoding Module","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        # Create a matrix of shape (max_len, d_model)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, d_model)\n        seq_len = x.size(1)\n        x = x + self.pe[:, :seq_len, :]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.280760Z","iopub.execute_input":"2024-12-09T19:25:05.281211Z","iopub.status.idle":"2024-12-09T19:25:05.292473Z","shell.execute_reply.started":"2024-12-09T19:25:05.281174Z","shell.execute_reply":"2024-12-09T19:25:05.290926Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 5.3 Multi-Head Self-Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        \n        self.out = nn.Linear(d_model, d_model)\n        \n    def forward(self, x, mask=None):\n        # x: (batch, seq_len, d_model)\n        B, L, D = x.size()\n        \n        # Linear projections\n        Q = self.W_q(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)  # (B, heads, L, head_dim)\n        K = self.W_k(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n        V = self.W_v(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n        \n        # Scaled dot product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, heads, L, L)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)  # (B, heads, L, head_dim)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1,2).contiguous().view(B, L, D)\n        \n        # Final linear projection\n        output = self.out(attn_output)\n        return output, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.295701Z","iopub.execute_input":"2024-12-09T19:25:05.296251Z","iopub.status.idle":"2024-12-09T19:25:05.319583Z","shell.execute_reply.started":"2024-12-09T19:25:05.296198Z","shell.execute_reply":"2024-12-09T19:25:05.318101Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 5.4 Feedforward Layer","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, dim_ff=2048):\n        super(PositionwiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, dim_ff)\n        self.fc2 = nn.Linear(dim_ff, d_model)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # x: (batch, seq_len, d_model)\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.365447Z","iopub.execute_input":"2024-12-09T19:25:05.365899Z","iopub.status.idle":"2024-12-09T19:25:05.372238Z","shell.execute_reply.started":"2024-12-09T19:25:05.365862Z","shell.execute_reply":"2024-12-09T19:25:05.370880Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 5.5 Putting It All Together (Single Encoder Layer)\n\nAn encoder layer in a Transformer consists of:\n\n1. Multi-Head Self-Attention + Add & Norm\n2. Feedforward + Add & Norm","metadata":{}},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dim_ff=2048):\n        super(TransformerEncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = PositionwiseFeedForward(d_model, dim_ff)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n    def forward(self, x, mask=None):\n        # Self-attention block\n        attn_output, _ = self.self_attn(x, mask)\n        x = self.norm1(x + attn_output)  # Residual + Norm\n        \n        # Feedforward block\n        ff_output = self.ff(x)\n        x = self.norm2(x + ff_output)     # Residual + Norm\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.374486Z","iopub.execute_input":"2024-12-09T19:25:05.374914Z","iopub.status.idle":"2024-12-09T19:25:05.387051Z","shell.execute_reply.started":"2024-12-09T19:25:05.374866Z","shell.execute_reply":"2024-12-09T19:25:05.385663Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 5.6 Example Usage","metadata":{}},{"cell_type":"code","source":"# Example: Embedding dimension (d_model) = 128, 8 heads, FF dim = 512\nd_model = 128\nnum_heads = 8\ndim_ff = 512\nseq_len = 10\nbatch_size = 2\n\n# Dummy input: (batch, seq_len, d_model)\nx = torch.randn(batch_size, seq_len, d_model)\nmask = None  # no mask for simplicity\n\n# Initialize encoder layer and positional encoding\npe = PositionalEncoding(d_model)\nencoder_layer = TransformerEncoderLayer(d_model, num_heads, dim_ff)\n\n# Add positional encoding\nx = pe(x)\n\n# Pass through the encoder layer\noutput = encoder_layer(x, mask)\nprint(\"Output shape:\", output.shape)  # (2, 10, 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:25:05.549463Z","iopub.execute_input":"2024-12-09T19:25:05.549890Z","iopub.status.idle":"2024-12-09T19:25:05.636032Z","shell.execute_reply.started":"2024-12-09T19:25:05.549853Z","shell.execute_reply":"2024-12-09T19:25:05.634821Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 10, 128])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"\n## 6. Analysis and Further Steps\n---\n\n- **Parallelization:**  \n  Since the Transformer relies on attention rather than recurrence, it can process all tokens in a sequence simultaneously, greatly speeding up training.\n\n- **Long-Range Dependencies:**  \n  With self-attention, every token can attend to any other token, allowing the model to capture long-range dependencies more effectively than RNNs.\n\n- **Scalability:**  \n  Transformers scale well to large datasets and longer sequences, which led to breakthroughs in language models (e.g., BERT, GPT).\n\n**Further Steps:**\n\n- Implement a full Transformer encoder and decoder stack.\n- Experiment with different positional encodings (learned, sinusoidal).\n- Explore how masking is used in the decoder to prevent access to future tokens.\n- Integrate advanced concepts like relative positional encodings, rotary embeddings, or efficient attention variants.\n\n**Remember:** The Transformer architecture and its variants now underpin many state-of-the-art NLP models and have even found applications in vision (ViT) and speech processing. Mastering the fundamentals of Multi-Head Self-Attention, Positional Encoding, and Feedforward Layers is key to understanding modern deep learning models in NLP and beyond.\n\n---\n\n## References\n\n- Vaswani et al. (2017), *\"Attention Is All You Need\"*, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n- The Annotated Transformer: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n- PyTorch Documentation: [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}