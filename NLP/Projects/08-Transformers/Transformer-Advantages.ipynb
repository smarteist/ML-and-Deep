{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Advantages and Variants\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Transformer architecture has significantly advanced the field of NLP. In this section, we'll discuss:\n",
    "\n",
    "- **Advantages over RNNs:**  \n",
    "  How Transformers enable parallelization and handle long-range dependencies more effectively than recurrent models.\n",
    "  \n",
    "- **Transformer Variants and Improvements:**  \n",
    "  Extensions like Transformer-XL, ALBERT, and the GPT series have pushed the boundaries of what Transformers can achieve.\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "- [Attention Is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Advantages of Transformers Over RNNs\n",
    "\n",
    "### 1.1 Parallelization\n",
    "\n",
    "**RNNs:**  \n",
    "- Process tokens sequentially.  \n",
    "- To read the nth token, you must have processed all previous n-1 tokens.\n",
    "- This sequential dependency makes it **difficult to parallelize** computation.\n",
    "\n",
    "**Transformers:**  \n",
    "- Compute attention over all tokens **in parallel**.  \n",
    "- No recurrence means the entire sequence can be processed at once.\n",
    "- Training can leverage hardware acceleration (e.g., GPUs) effectively, speeding up large-scale training.\n",
    "\n",
    "**Illustration:**\n",
    "\n",
    "**RNN Processing:**\n",
    "```\n",
    "Time step:   t=1    t=2    t=3    ...\n",
    "Input seq:   w1 --->w2 --->w3 ---> ...\n",
    "             ^      |\n",
    "             |      waits for output at t=1\n",
    "             waits for output at t=1 & t=2\n",
    "```\n",
    "You must wait for previous steps to finish.\n",
    "\n",
    "**Transformer Processing:**\n",
    "```\n",
    "Input seq: w1, w2, w3, ...\n",
    "All processed simultaneously through attention:\n",
    "   |   |   |\n",
    "   v   v   v\n",
    "Attention computations (parallel)\n",
    "   |   |   |\n",
    "Output representations (parallel)\n",
    "```\n",
    "\n",
    "### 1.2 Handling Long-Range Dependencies\n",
    "\n",
    "**RNNs:**  \n",
    "- Long sequences cause vanishing gradients and make it hard for RNNs to maintain information over long distances.\n",
    "\n",
    "**Transformers:**  \n",
    "- Self-attention directly connects every token to every other token with a single operation.\n",
    "- The attention mechanism can focus on distant parts of the sequence easily, improving model performance on tasks requiring long-range context.\n",
    "\n",
    "**Illustration:**\n",
    "\n",
    "In an RNN, to connect the first and last token in a long sequence, information must pass through all intermediate states:\n",
    "\n",
    "```\n",
    "w1 -> h1 -> h2 -> h3 -> ... -> hN-1 -> wN\n",
    "Gradients and information degrade over many steps\n",
    "```\n",
    "\n",
    "In a Transformer, attention links every token pair directly:\n",
    "\n",
    "```\n",
    "w1 <------------------> wN\n",
    " ^                       ^\n",
    " \\-----------------------/\n",
    "Direct attention paths enable easy long-range interactions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Variants and Improvements\n",
    "\n",
    "The original Transformer design has inspired many variants and improvements. Let's briefly introduce a few:\n",
    "\n",
    "### 2.1 Transformer-XL\n",
    "\n",
    "**Key Idea:**  \n",
    "Transformer-XL introduces a **segment-level recurrence** and **relative positional embeddings**, enabling:\n",
    "\n",
    "- Processing of longer sequences than the standard Transformer.\n",
    "- Better handling of context that spans beyond a single input segment.\n",
    "\n",
    "**Benefit:**\n",
    "- Improves upon the standard Transformer for language modeling tasks, handling longer contexts effectively.\n",
    "\n",
    "### 2.2 ALBERT (A Lite BERT)\n",
    "\n",
    "**Key Idea:**\n",
    "- Reduces the size of the model by sharing parameters across layers.\n",
    "- Factorizes the embedding size to improve parameter efficiency.\n",
    "\n",
    "**Benefits:**\n",
    "- Much fewer parameters than BERT, with minimal performance loss.\n",
    "- Faster training and inference.\n",
    "\n",
    "### 2.3 GPT Series (GPT, GPT-2, GPT-3, GPT-4, etc.)\n",
    "\n",
    "**Key Idea:**\n",
    "- Generative Pre-Training using Transformer decoders.\n",
    "- Train on large amounts of text data to learn general language patterns.\n",
    "- Larger and larger models (GPT-2, GPT-3) show emergent abilities, improved fluency, and generalization.\n",
    "\n",
    "**Benefits:**\n",
    "- Excellent at generating coherent text.\n",
    "- Few-shot and zero-shot learning capabilities (especially in GPT-3 and GPT-4).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Simple Code Visualization for Core Concepts\n",
    "\n",
    "Below is a simplified snippet to recap the Transformerâ€™s self-attention mechanism and demonstrate how easily we can handle parallel sequences. We'll add comments and diagrams directly in the code block to make it easier to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T19:31:10.577083Z",
     "iopub.status.busy": "2024-12-09T19:31:10.576451Z",
     "iopub.status.idle": "2024-12-09T19:31:10.582764Z",
     "shell.execute_reply": "2024-12-09T19:31:10.581407Z",
     "shell.execute_reply.started": "2024-12-09T19:31:10.577043Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T19:31:10.607369Z",
     "iopub.status.busy": "2024-12-09T19:31:10.606820Z",
     "iopub.status.idle": "2024-12-09T19:31:10.736905Z",
     "shell.execute_reply": "2024-12-09T19:31:10.735555Z",
     "shell.execute_reply.started": "2024-12-09T19:31:10.607324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n",
      "Attention weights shape: torch.Size([2, 4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model should be divisible by num_heads.\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer after combining heads\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        B, L, D = x.size()\n",
    "        \n",
    "        # Project into Q, K, V\n",
    "        Q = self.W_q(x) # (B, L, D)\n",
    "        K = self.W_k(x) # (B, L, D)\n",
    "        V = self.W_v(x) # (B, L, D)\n",
    "        \n",
    "        # Reshape for multi-head:\n",
    "        # Split D into (num_heads, head_dim)\n",
    "        # Resulting shape: (B, num_heads, L, head_dim)\n",
    "        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Compute attention scores: QK^T / sqrt(head_dim)\n",
    "        # Shape: (B, num_heads, L, L)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Softmax over the last dimension (L)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # (B, num_heads, L, L)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = torch.matmul(attn_weights, V)   # (B, num_heads, L, head_dim)\n",
    "        \n",
    "        # Concatenate heads back: (B, L, D)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(B, L, D)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.linear_out(attn_output) # (B, L, D)\n",
    "        return output, attn_weights\n",
    "\n",
    "# Example Usage\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, num_heads)\n",
    "output, weights = attention(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)    # (2, 5, 16)\n",
    "print(\"Attention weights shape:\", weights.shape)  # (2, 4, 5, 5)\n",
    "\n",
    "# This code demonstrates:\n",
    "# - Parallelization: The entire batch and sequence length are processed simultaneously.\n",
    "# - Multi-head structure: The data is split into multiple heads and then recombined.\n",
    "# - Easy handling of sequence length: Increase seq_len and model handles it easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "\n",
    "**Understanding the Shapes:**\n",
    "\n",
    "- Input: `(batch_size=2, seq_len=5, d_model=16)`\n",
    "- After splitting into heads: `(2, 4 heads, 5 tokens, 4 head_dim)`\n",
    "- Attention weights: `(2, 4, 5, 5)` â€“ each token attends to all other tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Moving Forward\n",
    "\n",
    "- **Deep Dive into Variants:**  \n",
    "  Read the Transformer-XL paper to understand how it manages even longer contexts.\n",
    "  \n",
    "- **Explore Code Implementations:**  \n",
    "  Check out Hugging Face Transformers library to see how these ideas are implemented in popular models like ALBERT and GPT.\n",
    "\n",
    "- **Experiment in Practice:**  \n",
    "  Try fine-tuning a GPT model on your own dataset to see the power of the Transformer architecture.\n",
    "\n",
    "**Remember:**  \n",
    "The Transformerâ€™s ability to process sequences in parallel and handle long-distance dependencies more effectively than RNN-based models has unlocked many breakthroughs in NLP. As variants like Transformer-XL, ALBERT, and GPT continue to evolve, understanding these core concepts will help you keep pace with the state of the art.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Attention Is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)  \n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  \n",
    "- [Hugging Face Transformers](https://github.com/huggingface/transformers)  "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
