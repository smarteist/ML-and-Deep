{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq) Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq) models are a class of deep learning architectures designed to convert one sequence of elements (e.g., words in a sentence) into another sequence. They have proven highly effective in a variety of Natural Language Processing (NLP) tasks.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- **Understand** the core architecture of Seq2Seq models, known as the Encoder-Decoder framework.\n",
    "- **Discuss** how Seq2Seq models handle variable-length inputs and outputs.\n",
    "- **Explore** key applications of Seq2Seq models in NLP, such as Machine Translation, Text Summarization, and Chatbots.\n",
    "- **Examine** how attention mechanisms can be integrated into Seq2Seq models to improve performance.\n",
    "\n",
    "**Resources for Further Reading:**\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) by Sutskever et al.\n",
    "- [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- Familiarity with RNNs, LSTMs, or GRUs.\n",
    "- Understanding of basic Python and PyTorch fundamentals.\n",
    "- Knowledge of NLP tasks and tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Seq2Seq Architecture\n",
    "\n",
    "### 1.1 Encoder-Decoder Framework\n",
    "\n",
    "The Seq2Seq model typically consists of two main components:\n",
    "\n",
    "- **Encoder:**  \n",
    "  The encoder reads and encodes the input sequence into a fixed-length vector representation (also called the context vector or thought vector). This vector summarizes all the information in the input sequence.\n",
    "\n",
    "- **Decoder:**  \n",
    "  The decoder takes the context vector and generates the output sequence, one token at a time. At each timestep, the decoder predicts the next output token based on the previously generated tokens and the context vector from the encoder.\n",
    "\n",
    "**Key Idea:**  \n",
    "The encoder transforms a variable-length input sequence into a fixed-length representation. The decoder then uses this representation to produce a variable-length output sequence. The encoder and decoder are often implemented using RNN variants like LSTMs or GRUs, but can also be built with Transformers.\n",
    "\n",
    "### 1.2 Handling Variable-Length Inputs and Outputs\n",
    "\n",
    "RNNs inherently process sequences step-by-step, making them suitable for variable-length inputs. By defining the end of a sequence with a special token (e.g., `<EOS>`), the model also handles variable-length outputs. Common practices include:\n",
    "\n",
    "- **Padding and Masking:**  \n",
    "  For batching, we often pad sequences to the same length. A masking mechanism ignores the padded positions during computation.\n",
    "\n",
    "- **Special Tokens:**  \n",
    "  Use `<SOS>` (start-of-sequence) and `<EOS>` (end-of-sequence) tokens to indicate where the decoder should begin and stop generating output.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Applications of Seq2Seq Models\n",
    "\n",
    "### 2.1 Machine Translation\n",
    "\n",
    "**Example:** Translating an English sentence into French.  \n",
    "The Seq2Seq model reads the entire English sentence (e.g., \"I love cats\") and then generates the French translation (e.g., \"J'aime les chats\").\n",
    "\n",
    "### 2.2 Text Summarization\n",
    "\n",
    "**Example:** Given a long document (e.g., a news article), the Seq2Seq model can produce a short summary. It reads the entire article as input and outputs a concise summary capturing the main points.\n",
    "\n",
    "### 2.3 Chatbots\n",
    "\n",
    "**Example:** The Seq2Seq model can be used for conversational agents. Given a userâ€™s query, the model produces a coherent and contextually relevant response. Over multiple turns, this can simulate a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implementing a Basic Seq2Seq Model in PyTorch\n",
    "\n",
    "**Note:** The following code is a simplified example to illustrate the Seq2Seq architecture. Training this model on real data would require a prepared dataset and possibly a more complex setup.\n",
    "\n",
    "### 3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.051176Z",
     "iopub.status.busy": "2024-12-09T18:53:56.050759Z",
     "iopub.status.idle": "2024-12-09T18:53:56.057530Z",
     "shell.execute_reply": "2024-12-09T18:53:56.056060Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.051143Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example Data\n",
    "\n",
    "For demonstration, let's consider a toy dataset of simple \"translation\"-like tasks. We'll create a small dataset where the input is a sequence of integers and the output is that sequence reversed. Although trivial, this demonstrates the Seq2Seq model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.071699Z",
     "iopub.status.busy": "2024-12-09T18:53:56.071276Z",
     "iopub.status.idle": "2024-12-09T18:53:56.107725Z",
     "shell.execute_reply": "2024-12-09T18:53:56.106226Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.071666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Toy dataset: input is a sequence of numbers [1,2,3,...] and output is the reversed sequence.\n",
    "# For example, input: [1, 2, 3], output: [3, 2, 1]\n",
    "\n",
    "def create_toy_dataset(n_samples=1000, seq_len=5, vocab_size=10):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(n_samples):\n",
    "        seq = np.random.randint(1, vocab_size, size=seq_len).tolist()\n",
    "        rev = seq[::-1]\n",
    "        inputs.append(seq)\n",
    "        targets.append(rev)\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = create_toy_dataset(n_samples=1000, seq_len=6, vocab_size=20)\n",
    "\n",
    "# Create vocab mappings\n",
    "# In a real scenario, you'd have predefined vocabularies for source and target languages.\n",
    "# Here, we just have numbers as tokens.\n",
    "input_vocab_size = 21  # 1-20 plus 0 for padding\n",
    "target_vocab_size = 21\n",
    "SOS_token = 0  # start-of-sequence\n",
    "EOS_token = 0  # end-of-sequence (for simplicity, let's reuse 0 as EOS in this toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.110680Z",
     "iopub.status.busy": "2024-12-09T18:53:56.110173Z",
     "iopub.status.idle": "2024-12-09T18:53:56.120379Z",
     "shell.execute_reply": "2024-12-09T18:53:56.119052Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.110628Z"
    }
   },
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.inputs[idx]\n",
    "        tgt = self.targets[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "dataset = SeqDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Encoder and Decoder Definitions\n",
    "\n",
    "**Encoder:** Takes an input sequence and produces a context vector (the final hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.123259Z",
     "iopub.status.busy": "2024-12-09T18:53:56.122769Z",
     "iopub.status.idle": "2024-12-09T18:53:56.141284Z",
     "shell.execute_reply": "2024-12-09T18:53:56.139869Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.123211Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        outputs, (h, c) = self.rnn(embedded)  # h,c: (num_layers, batch_size, hidden_dim)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder:** Uses the context vector from the encoder to generate the output sequence one token at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.144972Z",
     "iopub.status.busy": "2024-12-09T18:53:56.143991Z",
     "iopub.status.idle": "2024-12-09T18:53:56.156008Z",
     "shell.execute_reply": "2024-12-09T18:53:56.154588Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.144921Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        # x: (batch_size) - single token input\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.embedding(x)  # (batch_size, 1, embed_dim)\n",
    "        output, (h, c) = self.rnn(embedded, (h, c))  # (batch_size, 1, hidden_dim)\n",
    "        logits = self.fc(output.squeeze(1))  # (batch_size, output_dim)\n",
    "        return logits, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seq2Seq Model:** Combines the encoder and decoder. During training, we can use \"teacher forcing\" where we feed the target token at the previous timestep as input to the next decoder step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.158228Z",
     "iopub.status.busy": "2024-12-09T18:53:56.157680Z",
     "iopub.status.idle": "2024-12-09T18:53:56.169667Z",
     "shell.execute_reply": "2024-12-09T18:53:56.168444Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.158180Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        output_dim = self.decoder.fc.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(device)\n",
    "        \n",
    "        h, c = self.encoder(src)\n",
    "        \n",
    "        # First input to decoder is the <SOS> token.\n",
    "        input_tok = torch.zeros(batch_size, dtype=torch.long).to(device)  # <SOS> token = 0 here\n",
    "        for t in range(trg_len):\n",
    "            logits, h, c = self.decoder(input_tok, h, c)\n",
    "            outputs[:, t, :] = logits\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            top1 = logits.argmax(1)\n",
    "            input_tok = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:53:56.195378Z",
     "iopub.status.busy": "2024-12-09T18:53:56.194885Z",
     "iopub.status.idle": "2024-12-09T18:54:06.951813Z",
     "shell.execute_reply": "2024-12-09T18:54:06.950782Z",
     "shell.execute_reply.started": "2024-12-09T18:53:56.195315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 3.0035\n",
      "Epoch 2/30, Loss: 2.8352\n",
      "Epoch 3/30, Loss: 2.5144\n",
      "Epoch 4/30, Loss: 2.2524\n",
      "Epoch 5/30, Loss: 2.0697\n",
      "Epoch 6/30, Loss: 1.9232\n",
      "Epoch 7/30, Loss: 1.7965\n",
      "Epoch 8/30, Loss: 1.6745\n",
      "Epoch 9/30, Loss: 1.5690\n",
      "Epoch 10/30, Loss: 1.4711\n",
      "Epoch 11/30, Loss: 1.3606\n",
      "Epoch 12/30, Loss: 1.2822\n",
      "Epoch 13/30, Loss: 1.2178\n",
      "Epoch 14/30, Loss: 1.1608\n",
      "Epoch 15/30, Loss: 1.0738\n",
      "Epoch 16/30, Loss: 1.0124\n",
      "Epoch 17/30, Loss: 0.9337\n",
      "Epoch 18/30, Loss: 0.8508\n",
      "Epoch 19/30, Loss: 0.8189\n",
      "Epoch 20/30, Loss: 0.7538\n",
      "Epoch 21/30, Loss: 0.7440\n",
      "Epoch 22/30, Loss: 0.6229\n",
      "Epoch 23/30, Loss: 0.6018\n",
      "Epoch 24/30, Loss: 0.5460\n",
      "Epoch 25/30, Loss: 0.5123\n",
      "Epoch 26/30, Loss: 0.4501\n",
      "Epoch 27/30, Loss: 0.4257\n",
      "Epoch 28/30, Loss: 0.3657\n",
      "Epoch 29/30, Loss: 0.3400\n",
      "Epoch 30/30, Loss: 0.3120\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "hidden_dim = 64\n",
    "encoder = Encoder(input_vocab_size, embed_dim, hidden_dim).to(device)\n",
    "decoder = Decoder(target_vocab_size, embed_dim, hidden_dim).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignoring pad/EOS token\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 30\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inp, tgt in dataloader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inp, tgt)\n",
    "        # output: (batch_size, trg_len, output_dim)\n",
    "        # tgt: (batch_size, trg_len)\n",
    "        # reshape\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:54:06.954521Z",
     "iopub.status.busy": "2024-12-09T18:54:06.954029Z",
     "iopub.status.idle": "2024-12-09T18:54:06.966505Z",
     "shell.execute_reply": "2024-12-09T18:54:06.965420Z",
     "shell.execute_reply.started": "2024-12-09T18:54:06.954472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [2, 5, 8, 3, 7, 9]\n",
      "Decoded sequence: [9, 7, 3, 8, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.tensor([2,5,8,3,7,9], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # We know the reversed sequence should be [9,7,3,8,5,2]\n",
    "    h, c = model.encoder(test_seq)\n",
    "    input_tok = torch.zeros(1, dtype=torch.long).to(device)  # <SOS>\n",
    "    decoded = []\n",
    "    for _ in range(6):\n",
    "        logits, h, c = model.decoder(input_tok, h, c)\n",
    "        top1 = logits.argmax(1)\n",
    "        decoded.append(top1.item())\n",
    "        input_tok = top1\n",
    "    print(\"Input sequence:\", test_seq.squeeze().tolist())\n",
    "    print(\"Decoded sequence:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "You should see that the model attempts to produce the reversed sequence. With only 5 epochs and a tiny dataset, results might not be perfect, but you should see some improvement if you train longer or with more data.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Enhancements: Incorporating Attention Mechanisms\n",
    "\n",
    "**Issue with Vanilla Seq2Seq:**  \n",
    "The encoder compresses the entire input sequence into a single fixed-length vector. For long sequences, this can become a \"bottleneck,\" making it difficult for the decoder to extract all necessary information.\n",
    "\n",
    "**Solution: Attention Mechanisms:**  \n",
    "Attention allows the decoder to look back at the encoder outputs and focus on the most relevant parts of the input at each decoding step, rather than relying solely on a single context vector.\n",
    "\n",
    "### 4.1 Integration\n",
    "\n",
    "To incorporate attention:\n",
    "\n",
    "- Store all encoder hidden states rather than just the final one.\n",
    "- Compute attention weights over these encoder states for each decoder step.\n",
    "- Take a weighted sum of the encoder outputs to form a context vector dynamically.\n",
    "\n",
    "### 4.2 Benefit\n",
    "\n",
    "- **Improved Performance:** Especially on longer sequences.\n",
    "- **Interpretability:** Attention weights provide insight into which input tokens the model focused on.\n",
    "\n",
    "---\n",
    "\n",
    "## Further Steps and Resources\n",
    "\n",
    "1. **Try Different Architectures:**  \n",
    "   Experiment with GRUs, LSTMs, or even Transformers for encoder-decoder models.\n",
    "\n",
    "2. **Add Attention:**  \n",
    "   Implement additive attention (Bahdanau) or multiplicative attention (Luong) to improve performance on longer sequences.\n",
    "\n",
    "3. **Use Real Datasets:**  \n",
    "   Apply the Seq2Seq model to a machine translation dataset (e.g., WMT) or a text summarization dataset.\n",
    "\n",
    "4. **Learn From Tutorials:**  \n",
    "   The [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) and PyTorch tutorials offer hands-on guidance.\n",
    "\n",
    "**Remember:** Seq2Seq models form the foundation of many advanced NLP applications. With attention and transformer architectures, Seq2Seq models have become even more powerful and efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) by Sutskever, Vinyals, and Le (2014)  \n",
    "- [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
