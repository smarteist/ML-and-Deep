{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sequence-to-Sequence (Seq2Seq) Models\n\n## Introduction\n\nSequence-to-Sequence (Seq2Seq) models are a class of deep learning architectures designed to convert one sequence of elements (e.g., words in a sentence) into another sequence. They have proven highly effective in a variety of Natural Language Processing (NLP) tasks.\n\nIn this notebook, we will:\n\n- **Understand** the core architecture of Seq2Seq models, known as the Encoder-Decoder framework.\n- **Discuss** how Seq2Seq models handle variable-length inputs and outputs.\n- **Explore** key applications of Seq2Seq models in NLP, such as Machine Translation, Text Summarization, and Chatbots.\n- **Examine** how attention mechanisms can be integrated into Seq2Seq models to improve performance.\n\n**Resources for Further Reading:**\n\n- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) by Sutskever et al.\n- [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n\n**Prerequisites:**\n\n- Familiarity with RNNs, LSTMs, or GRUs.\n- Understanding of basic Python and PyTorch fundamentals.\n- Knowledge of NLP tasks and tokenization.\n\n---\n\n## 1. The Seq2Seq Architecture\n\n### 1.1 Encoder-Decoder Framework\n\nThe Seq2Seq model typically consists of two main components:\n\n- **Encoder:**  \n  The encoder reads and encodes the input sequence into a fixed-length vector representation (also called the context vector or thought vector). This vector summarizes all the information in the input sequence.\n\n- **Decoder:**  \n  The decoder takes the context vector and generates the output sequence, one token at a time. At each timestep, the decoder predicts the next output token based on the previously generated tokens and the context vector from the encoder.\n\n**Key Idea:**  \nThe encoder transforms a variable-length input sequence into a fixed-length representation. The decoder then uses this representation to produce a variable-length output sequence. The encoder and decoder are often implemented using RNN variants like LSTMs or GRUs, but can also be built with Transformers.\n\n### 1.2 Handling Variable-Length Inputs and Outputs\n\nRNNs inherently process sequences step-by-step, making them suitable for variable-length inputs. By defining the end of a sequence with a special token (e.g., `<EOS>`), the model also handles variable-length outputs. Common practices include:\n\n- **Padding and Masking:**  \n  For batching, we often pad sequences to the same length. A masking mechanism ignores the padded positions during computation.\n\n- **Special Tokens:**  \n  Use `<SOS>` (start-of-sequence) and `<EOS>` (end-of-sequence) tokens to indicate where the decoder should begin and stop generating output.\n\n---\n\n## 2. Applications of Seq2Seq Models\n\n### 2.1 Machine Translation\n\n**Example:** Translating an English sentence into French.  \nThe Seq2Seq model reads the entire English sentence (e.g., \"I love cats\") and then generates the French translation (e.g., \"J'aime les chats\").\n\n### 2.2 Text Summarization\n\n**Example:** Given a long document (e.g., a news article), the Seq2Seq model can produce a short summary. It reads the entire article as input and outputs a concise summary capturing the main points.\n\n### 2.3 Chatbots\n\n**Example:** The Seq2Seq model can be used for conversational agents. Given a userâ€™s query, the model produces a coherent and contextually relevant response. Over multiple turns, this can simulate a conversation.\n\n---\n\n## 3. Implementing a Basic Seq2Seq Model in PyTorch\n\n**Note:** The following code is a simplified example to illustrate the Seq2Seq architecture. Training this model on real data would require a prepared dataset and possibly a more complex setup.\n\n### 3.1 Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.050759Z","iopub.execute_input":"2024-12-09T18:53:56.051176Z","iopub.status.idle":"2024-12-09T18:53:56.057530Z","shell.execute_reply.started":"2024-12-09T18:53:56.051143Z","shell.execute_reply":"2024-12-09T18:53:56.056060Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### 3.2 Example Data\n\nFor demonstration, let's consider a toy dataset of simple \"translation\"-like tasks. We'll create a small dataset where the input is a sequence of integers and the output is that sequence reversed. Although trivial, this demonstrates the Seq2Seq model structure.","metadata":{}},{"cell_type":"code","source":"# Toy dataset: input is a sequence of numbers [1,2,3,...] and output is the reversed sequence.\n# For example, input: [1, 2, 3], output: [3, 2, 1]\n\ndef create_toy_dataset(n_samples=1000, seq_len=5, vocab_size=10):\n    inputs = []\n    targets = []\n    for _ in range(n_samples):\n        seq = np.random.randint(1, vocab_size, size=seq_len).tolist()\n        rev = seq[::-1]\n        inputs.append(seq)\n        targets.append(rev)\n    return inputs, targets\n\ninputs, targets = create_toy_dataset(n_samples=1000, seq_len=6, vocab_size=20)\n\n# Create vocab mappings\n# In a real scenario, you'd have predefined vocabularies for source and target languages.\n# Here, we just have numbers as tokens.\ninput_vocab_size = 21  # 1-20 plus 0 for padding\ntarget_vocab_size = 21\nSOS_token = 0  # start-of-sequence\nEOS_token = 0  # end-of-sequence (for simplicity, let's reuse 0 as EOS in this toy example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.071276Z","iopub.execute_input":"2024-12-09T18:53:56.071699Z","iopub.status.idle":"2024-12-09T18:53:56.107725Z","shell.execute_reply.started":"2024-12-09T18:53:56.071666Z","shell.execute_reply":"2024-12-09T18:53:56.106226Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 3.3 Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class SeqDataset(Dataset):\n    def __init__(self, inputs, targets):\n        self.inputs = inputs\n        self.targets = targets\n    def __len__(self):\n        return len(self.inputs)\n    def __getitem__(self, idx):\n        inp = self.inputs[idx]\n        tgt = self.targets[idx]\n        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n\ndataset = SeqDataset(inputs, targets)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.110173Z","iopub.execute_input":"2024-12-09T18:53:56.110680Z","iopub.status.idle":"2024-12-09T18:53:56.120379Z","shell.execute_reply.started":"2024-12-09T18:53:56.110628Z","shell.execute_reply":"2024-12-09T18:53:56.119052Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### 3.4 Encoder and Decoder Definitions\n\n**Encoder:** Takes an input sequence and produces a context vector (the final hidden state).","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len)\n        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n        outputs, (h, c) = self.rnn(embedded)  # h,c: (num_layers, batch_size, hidden_dim)\n        return h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.122769Z","iopub.execute_input":"2024-12-09T18:53:56.123259Z","iopub.status.idle":"2024-12-09T18:53:56.141284Z","shell.execute_reply.started":"2024-12-09T18:53:56.123211Z","shell.execute_reply":"2024-12-09T18:53:56.139869Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"**Decoder:** Uses the context vector from the encoder to generate the output sequence one token at a time.","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x, h, c):\n        # x: (batch_size) - single token input\n        x = x.unsqueeze(1)  # (batch_size, 1)\n        embedded = self.embedding(x)  # (batch_size, 1, embed_dim)\n        output, (h, c) = self.rnn(embedded, (h, c))  # (batch_size, 1, hidden_dim)\n        logits = self.fc(output.squeeze(1))  # (batch_size, output_dim)\n        return logits, h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.143991Z","iopub.execute_input":"2024-12-09T18:53:56.144972Z","iopub.status.idle":"2024-12-09T18:53:56.156008Z","shell.execute_reply.started":"2024-12-09T18:53:56.144921Z","shell.execute_reply":"2024-12-09T18:53:56.154588Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"**Seq2Seq Model:** Combines the encoder and decoder. During training, we can use \"teacher forcing\" where we feed the target token at the previous timestep as input to the next decoder step.","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        output_dim = self.decoder.fc.out_features\n        \n        outputs = torch.zeros(batch_size, trg_len, output_dim).to(device)\n        \n        h, c = self.encoder(src)\n        \n        # First input to decoder is the <SOS> token.\n        input_tok = torch.zeros(batch_size, dtype=torch.long).to(device)  # <SOS> token = 0 here\n        for t in range(trg_len):\n            logits, h, c = self.decoder(input_tok, h, c)\n            outputs[:, t, :] = logits\n            # Decide whether to use teacher forcing\n            teacher_force = np.random.random() < teacher_forcing_ratio\n            top1 = logits.argmax(1)\n            input_tok = trg[:, t] if teacher_force else top1\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.157680Z","iopub.execute_input":"2024-12-09T18:53:56.158228Z","iopub.status.idle":"2024-12-09T18:53:56.169667Z","shell.execute_reply.started":"2024-12-09T18:53:56.158180Z","shell.execute_reply":"2024-12-09T18:53:56.168444Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### 3.5 Training the Seq2Seq Model","metadata":{}},{"cell_type":"code","source":"embed_dim = 32\nhidden_dim = 64\nencoder = Encoder(input_vocab_size, embed_dim, hidden_dim).to(device)\ndecoder = Decoder(target_vocab_size, embed_dim, hidden_dim).to(device)\nmodel = Seq2Seq(encoder, decoder).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # ignoring pad/EOS token\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 30\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for inp, tgt in dataloader:\n        inp, tgt = inp.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(inp, tgt)\n        # output: (batch_size, trg_len, output_dim)\n        # tgt: (batch_size, trg_len)\n        # reshape\n        output_dim = output.shape[-1]\n        output = output.contiguous().view(-1, output_dim)\n        tgt = tgt.contiguous().view(-1)\n        \n        loss = criterion(output, tgt)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:53:56.194885Z","iopub.execute_input":"2024-12-09T18:53:56.195378Z","iopub.status.idle":"2024-12-09T18:54:06.951813Z","shell.execute_reply.started":"2024-12-09T18:53:56.195315Z","shell.execute_reply":"2024-12-09T18:54:06.950782Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30, Loss: 3.0035\nEpoch 2/30, Loss: 2.8352\nEpoch 3/30, Loss: 2.5144\nEpoch 4/30, Loss: 2.2524\nEpoch 5/30, Loss: 2.0697\nEpoch 6/30, Loss: 1.9232\nEpoch 7/30, Loss: 1.7965\nEpoch 8/30, Loss: 1.6745\nEpoch 9/30, Loss: 1.5690\nEpoch 10/30, Loss: 1.4711\nEpoch 11/30, Loss: 1.3606\nEpoch 12/30, Loss: 1.2822\nEpoch 13/30, Loss: 1.2178\nEpoch 14/30, Loss: 1.1608\nEpoch 15/30, Loss: 1.0738\nEpoch 16/30, Loss: 1.0124\nEpoch 17/30, Loss: 0.9337\nEpoch 18/30, Loss: 0.8508\nEpoch 19/30, Loss: 0.8189\nEpoch 20/30, Loss: 0.7538\nEpoch 21/30, Loss: 0.7440\nEpoch 22/30, Loss: 0.6229\nEpoch 23/30, Loss: 0.6018\nEpoch 24/30, Loss: 0.5460\nEpoch 25/30, Loss: 0.5123\nEpoch 26/30, Loss: 0.4501\nEpoch 27/30, Loss: 0.4257\nEpoch 28/30, Loss: 0.3657\nEpoch 29/30, Loss: 0.3400\nEpoch 30/30, Loss: 0.3120\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### 3.6 Testing the Model","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_seq = torch.tensor([2,5,8,3,7,9], dtype=torch.long).unsqueeze(0).to(device)\n    # We know the reversed sequence should be [9,7,3,8,5,2]\n    h, c = model.encoder(test_seq)\n    input_tok = torch.zeros(1, dtype=torch.long).to(device)  # <SOS>\n    decoded = []\n    for _ in range(6):\n        logits, h, c = model.decoder(input_tok, h, c)\n        top1 = logits.argmax(1)\n        decoded.append(top1.item())\n        input_tok = top1\n    print(\"Input sequence:\", test_seq.squeeze().tolist())\n    print(\"Decoded sequence:\", decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:54:06.954029Z","iopub.execute_input":"2024-12-09T18:54:06.954521Z","iopub.status.idle":"2024-12-09T18:54:06.966505Z","shell.execute_reply.started":"2024-12-09T18:54:06.954472Z","shell.execute_reply":"2024-12-09T18:54:06.965420Z"}},"outputs":[{"name":"stdout","text":"Input sequence: [2, 5, 8, 3, 7, 9]\nDecoded sequence: [9, 7, 3, 8, 5, 2]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"You should see that the model attempts to produce the reversed sequence. With only 5 epochs and a tiny dataset, results might not be perfect, but you should see some improvement if you train longer or with more data.\n\n---\n\n## 4. Enhancements: Incorporating Attention Mechanisms\n\n**Issue with Vanilla Seq2Seq:**  \nThe encoder compresses the entire input sequence into a single fixed-length vector. For long sequences, this can become a \"bottleneck,\" making it difficult for the decoder to extract all necessary information.\n\n**Solution: Attention Mechanisms:**  \nAttention allows the decoder to look back at the encoder outputs and focus on the most relevant parts of the input at each decoding step, rather than relying solely on a single context vector.\n\n### 4.1 Integration\n\nTo incorporate attention:\n\n- Store all encoder hidden states rather than just the final one.\n- Compute attention weights over these encoder states for each decoder step.\n- Take a weighted sum of the encoder outputs to form a context vector dynamically.\n\n### 4.2 Benefit\n\n- **Improved Performance:** Especially on longer sequences.\n- **Interpretability:** Attention weights provide insight into which input tokens the model focused on.\n\n---\n\n## Further Steps and Resources\n\n1. **Try Different Architectures:**  \n   Experiment with GRUs, LSTMs, or even Transformers for encoder-decoder models.\n\n2. **Add Attention:**  \n   Implement additive attention (Bahdanau) or multiplicative attention (Luong) to improve performance on longer sequences.\n\n3. **Use Real Datasets:**  \n   Apply the Seq2Seq model to a machine translation dataset (e.g., WMT) or a text summarization dataset.\n\n4. **Learn From Tutorials:**  \n   The [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) and PyTorch tutorials offer hands-on guidance.\n\n**Remember:** Seq2Seq models form the foundation of many advanced NLP applications. With attention and transformer architectures, Seq2Seq models have become even more powerful and efficient.\n\n---\n\n## References\n\n- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) by Sutskever, Vinyals, and Le (2014)  \n- [TensorFlow Seq2Seq Tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}