{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7d9d00",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00571,
     "end_time": "2024-12-11T17:45:41.843581",
     "exception": false,
     "start_time": "2024-12-11T17:45:41.837871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Attention Mechanisms in Deep Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- **Introduce** the concept of Attention Mechanisms and understand why they are pivotal in modern deep learning models.\n",
    "- **Explore** the various types of attention, including Soft Attention, Hard Attention, and Self-Attention.\n",
    "- **Implement** key attention mechanisms such as Scaled Dot-Product Attention and Additive Attention using PyTorch.\n",
    "- **Provide** resources for further reading to deepen your understanding.\n",
    "\n",
    "**Resources for Further Reading:**\n",
    "\n",
    "- [Attention Mechanism Explained](https://towardsdatascience.com/attention-mechanism-explained-8f96b26ebae)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- Familiarity with Python and PyTorch.\n",
    "- Understanding of neural network fundamentals, especially sequence models like RNNs.\n",
    "\n",
    "**Note:** Attention mechanisms have revolutionized the field of Natural Language Processing (NLP) and are integral to architectures like Transformers. They help models focus on relevant parts of the input sequence, addressing limitations inherent in traditional RNNs.\n",
    "\n",
    "## Why Attention?: Overcoming Limitations of RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are powerful for modeling sequential data, but they have notable limitations:\n",
    "\n",
    "- **Long-Term Dependencies:** RNNs struggle to capture dependencies between distant elements in a sequence due to issues like vanishing and exploding gradients.\n",
    "- **Sequential Processing:** RNNs process data sequentially, making it difficult to parallelize computations.\n",
    "- **Fixed-Size Context:** The hidden state in RNNs acts as a bottleneck, limiting the amount of information that can be retained from the input.\n",
    "\n",
    "**Attention Mechanisms** address these challenges by allowing models to dynamically focus on different parts of the input sequence when producing each element of the output. This leads to better performance, especially in tasks requiring the integration of information from various parts of the input.\n",
    "\n",
    "## Types of Attention\n",
    "\n",
    "### 1. Soft vs. Hard Attention\n",
    "\n",
    "- **Soft Attention:**\n",
    "  - **Deterministic and Differentiable:** Allows the model to consider all parts of the input with varying degrees of importance.\n",
    "  - **Weighted Sum:** Computes a weighted average of the input features.\n",
    "  - **Backpropagation-Friendly:** Can be trained end-to-end using gradient-based optimization.\n",
    "\n",
    "- **Hard Attention:**\n",
    "  - **Stochastic and Non-Differentiable:** Selects specific parts of the input, often requiring reinforcement learning techniques for training.\n",
    "  - **Discrete Selection:** Chooses exact elements or regions to focus on.\n",
    "  - **Less Common:** Due to training difficulties, hard attention is less frequently used in practice.\n",
    "\n",
    "### 2. Self-Attention\n",
    "\n",
    "- **Definition:** A mechanism where different positions of a single sequence are related to each other to compute a representation of the sequence.\n",
    "- **Usage:** Fundamental to Transformer architectures, enabling the model to capture dependencies regardless of their distance in the sequence.\n",
    "- **Benefits:**\n",
    "  - **Parallelization:** Unlike RNNs, self-attention allows for parallel processing of sequence elements.\n",
    "  - **Long-Range Dependencies:** Effectively captures relationships between distant elements in the sequence.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We will implement two primary attention mechanisms:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**\n",
    "2. **Additive Attention**\n",
    "\n",
    "Both implementations will be in PyTorch.\n",
    "\n",
    "### 1. Scaled Dot-Product Attention\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Scaled Dot-Product Attention computes the attention weights using the dot product of queries and keys, scales them, applies a softmax to obtain probabilities, and then uses these to weight the values.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ Q $ = Query matrix\n",
    "- $ K $ = Key matrix\n",
    "- $ V $ = Value matrix\n",
    "- $ d_k $ = Dimension of the keys\n",
    "\n",
    "**Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2708cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:41.857001Z",
     "iopub.status.busy": "2024-12-11T17:45:41.856702Z",
     "iopub.status.idle": "2024-12-11T17:45:44.908314Z",
     "shell.execute_reply": "2024-12-11T17:45:44.907648Z"
    },
    "papermill": {
     "duration": 3.061445,
     "end_time": "2024-12-11T17:45:44.910212",
     "exception": false,
     "start_time": "2024-12-11T17:45:41.848767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Define Attention Mechanisms\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = head_dim ** 0.5  # Correct scaling based on head_dim\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V: [batch_size, num_heads, seq_len, head_dim]\n",
    "        mask: [batch_size, num_heads, seq_len, seq_len] or similar\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        output = torch.matmul(attn, V)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec5366",
   "metadata": {
    "papermill": {
     "duration": 0.004826,
     "end_time": "2024-12-11T17:45:44.920161",
     "exception": false,
     "start_time": "2024-12-11T17:45:44.915335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69246b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:44.930948Z",
     "iopub.status.busy": "2024-12-11T17:45:44.930589Z",
     "iopub.status.idle": "2024-12-11T17:45:45.003738Z",
     "shell.execute_reply": "2024-12-11T17:45:45.002671Z"
    },
    "papermill": {
     "duration": 0.080643,
     "end_time": "2024-12-11T17:45:45.005492",
     "exception": false,
     "start_time": "2024-12-11T17:45:44.924849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 6])\n",
      "Attention weights shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "batch_size = 2\n",
    "seq_len_q = 3\n",
    "seq_len_k = 4\n",
    "d_k = 5\n",
    "d_v = 6\n",
    "\n",
    "# Random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len_q, d_k)\n",
    "K = torch.randn(batch_size, seq_len_k, d_k)\n",
    "V = torch.randn(batch_size, seq_len_k, d_v)\n",
    "\n",
    "# Initialize attention module\n",
    "attention = ScaledDotProductAttention(d_k)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = attention(Q, K, V)\n",
    "\n",
    "print(\"Output shape:\", output.shape)          # Expected: (2, 3, 6)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55337a",
   "metadata": {
    "papermill": {
     "duration": 0.004762,
     "end_time": "2024-12-11T17:45:45.015308",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.010546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Additive Attention\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Additive Attention, introduced by Bahdanau et al., computes attention scores by applying a feed-forward network to the concatenation of queries and keys, followed by a non-linear activation (usually $ \\tanh $).\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\text{score}(Q, K))V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\text{score}(Q, K) = \\mathbf{v}^T \\tanh(\\mathbf{W}_q Q + \\mathbf{W}_k K)\n",
    "$$\n",
    "\n",
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72303f60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.026754Z",
     "iopub.status.busy": "2024-12-11T17:45:45.026086Z",
     "iopub.status.idle": "2024-12-11T17:45:45.033924Z",
     "shell.execute_reply": "2024-12-11T17:45:45.033146Z"
    },
    "papermill": {
     "duration": 0.015448,
     "end_time": "2024-12-11T17:45:45.035647",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.020199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d_q, d_k, d_attn):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W_q = nn.Linear(d_q, d_attn)\n",
    "        self.W_k = nn.Linear(d_k, d_attn)\n",
    "        self.v = nn.Linear(d_attn, 1, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q: Queries shape (batch_size, seq_len_q, d_q)\n",
    "        K: Keys shape (batch_size, seq_len_k, d_k)\n",
    "        V: Values shape (batch_size, seq_len_k, d_v)\n",
    "        mask: Optional mask tensor\n",
    "        \"\"\"\n",
    "        # Expand Q and K for addition\n",
    "        # Q: (batch_size, seq_len_q, 1, d_q)\n",
    "        # K: (batch_size, 1, seq_len_k, d_k)\n",
    "        Q_expanded = Q.unsqueeze(2)\n",
    "        K_expanded = K.unsqueeze(1)\n",
    "        \n",
    "        # Apply linear layers and activation\n",
    "        energy = torch.tanh(self.W_q(Q_expanded) + self.W_k(K_expanded))  # (batch_size, seq_len_q, seq_len_k, d_attn)\n",
    "        scores = self.v(energy).squeeze(-1)  # (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)  # (batch_size, seq_len_q, seq_len_k)\n",
    "        output = torch.matmul(attn, V)     # (batch_size, seq_len_q, d_v)\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768e37d",
   "metadata": {
    "papermill": {
     "duration": 0.004683,
     "end_time": "2024-12-11T17:45:45.045238",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.040555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4557ee08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.056642Z",
     "iopub.status.busy": "2024-12-11T17:45:45.055992Z",
     "iopub.status.idle": "2024-12-11T17:45:45.124173Z",
     "shell.execute_reply": "2024-12-11T17:45:45.123495Z"
    },
    "papermill": {
     "duration": 0.075885,
     "end_time": "2024-12-11T17:45:45.125941",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.050056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 6])\n",
      "Attention weights shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "batch_size = 2\n",
    "seq_len_q = 3\n",
    "seq_len_k = 4\n",
    "d_q = 5\n",
    "d_k = 5\n",
    "d_v = 6\n",
    "d_attn = 10\n",
    "\n",
    "# Random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len_q, d_q)\n",
    "K = torch.randn(batch_size, seq_len_k, d_k)\n",
    "V = torch.randn(batch_size, seq_len_k, d_v)\n",
    "\n",
    "# Initialize attention module\n",
    "additive_attention = AdditiveAttention(d_q, d_k, d_attn)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = additive_attention(Q, K, V)\n",
    "\n",
    "print(\"Output shape:\", output.shape)          # Expected: (2, 3, 6)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09269cc1",
   "metadata": {
    "papermill": {
     "duration": 0.004734,
     "end_time": "2024-12-11T17:45:45.135642",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.130908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Self-Attention allows a sequence to interact with itself (i.e., different positions within the same sequence) to compute a representation of the sequence. This mechanism is pivotal in Transformer models.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Queries, Keys, Values:** Derived from the same input sequence.\n",
    "- **Multi-Head Attention:** Extends self-attention by running multiple attention mechanisms in parallel, allowing the model to focus on different representation subspaces.\n",
    "\n",
    "**Implementation Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8ba660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.146563Z",
     "iopub.status.busy": "2024-12-11T17:45:45.146207Z",
     "iopub.status.idle": "2024-12-11T17:45:45.156711Z",
     "shell.execute_reply": "2024-12-11T17:45:45.155949Z"
    },
    "papermill": {
     "duration": 0.017775,
     "end_time": "2024-12-11T17:45:45.158289",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.140514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(self.head_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, E = x.size()\n",
    "        \n",
    "        Q = self.q_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)  # (B, num_heads, T, head_dim)\n",
    "        K = self.k_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (B, 1, 1, T)\n",
    "        \n",
    "        attended, attn_weights = self.attention(Q, K, V, mask)\n",
    "        attended = attended.transpose(1,2).contiguous().view(B, T, E)\n",
    "        output = self.fc_out(attended)\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692e4e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.169089Z",
     "iopub.status.busy": "2024-12-11T17:45:45.168816Z",
     "iopub.status.idle": "2024-12-11T17:45:45.180307Z",
     "shell.execute_reply": "2024-12-11T17:45:45.179498Z"
    },
    "papermill": {
     "duration": 0.018942,
     "end_time": "2024-12-11T17:45:45.182059",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.163117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n",
      "Attention weights shape: torch.Size([2, 4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Random input tensor\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Initialize self-attention module\n",
    "self_attention = SelfAttention(embed_dim, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = self_attention(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected: (2, 5, 16)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 4, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b534c1",
   "metadata": {
    "papermill": {
     "duration": 0.004754,
     "end_time": "2024-12-11T17:45:45.191713",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.186959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MultiHeadAttention \n",
    "\n",
    "Facilitates the model's ability to focus on different parts of the input sequence simultaneously by splitting the embedding space into multiple heads, each performing its own attention computation. This multi-faceted attention mechanism enriches the model's capacity to capture diverse contextual relationships within the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f544e989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.203477Z",
     "iopub.status.busy": "2024-12-11T17:45:45.203182Z",
     "iopub.status.idle": "2024-12-11T17:45:45.212539Z",
     "shell.execute_reply": "2024-12-11T17:45:45.211737Z"
    },
    "papermill": {
     "duration": 0.017527,
     "end_time": "2024-12-11T17:45:45.214224",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.196697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(self.head_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch_size, query_len, embed_dim]\n",
    "        key: [batch_size, key_len, embed_dim]\n",
    "        value: [batch_size, key_len, embed_dim]\n",
    "        mask: [batch_size, num_heads, query_len, key_len] or similar\n",
    "        \"\"\"\n",
    "        B, Tq, E = query.size()\n",
    "        Bk, Tk, Ek = key.size()\n",
    "        \n",
    "        assert B == Bk and E == Ek, \"Batch size and embedding dimensions must match between query and key/value.\"\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.q_linear(query).view(B, Tq, self.num_heads, self.head_dim).transpose(1,2)  # [B, num_heads, Tq, head_dim]\n",
    "        K = self.k_linear(key).view(B, Tk, self.num_heads, self.head_dim).transpose(1,2)    # [B, num_heads, Tk, head_dim]\n",
    "        V = self.v_linear(value).view(B, Tk, self.num_heads, self.head_dim).transpose(1,2)  # [B, num_heads, Tk, head_dim]\n",
    "        \n",
    "        # Apply attention\n",
    "        attended, attn_weights = self.attention(Q, K, V, mask)  # [B, num_heads, Tq, head_dim]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1,2).contiguous().view(B, Tq, E)  # [B, Tq, E]\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.fc_out(attended)  # [B, Tq, E]\n",
    "        \n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bca4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.229694Z",
     "iopub.status.busy": "2024-12-11T17:45:45.229402Z",
     "iopub.status.idle": "2024-12-11T17:45:45.237141Z",
     "shell.execute_reply": "2024-12-11T17:45:45.236346Z"
    },
    "papermill": {
     "duration": 0.017325,
     "end_time": "2024-12-11T17:45:45.238741",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.221416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n",
      "Attention weights shape: torch.Size([2, 4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "batch_size = 2\n",
    "query_len = 5\n",
    "key_len = 5\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Random input tensors\n",
    "query = torch.randn(batch_size, query_len, embed_dim)  # Shape: [2, 5, 16]\n",
    "key = torch.randn(batch_size, key_len, embed_dim)      # Shape: [2, 5, 16]\n",
    "value = torch.randn(batch_size, key_len, embed_dim)    # Shape: [2, 5, 16]\n",
    "\n",
    "# Optional mask (set to None for simplicity)\n",
    "mask = None  # Alternatively, define a mask tensor with shape [batch_size, num_heads, query_len, key_len]\n",
    "\n",
    "# Initialize multi-head attention module\n",
    "multi_head_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = multi_head_attn(query, key, value, mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)            # Expected: [2, 5, 16]\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected: [2, 4, 5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d664c6",
   "metadata": {
    "papermill": {
     "duration": 0.004749,
     "end_time": "2024-12-11T17:45:45.248472",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.243723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Practical Example: Implementing Attention in a Sequence-to-Sequence Model\n",
    "\n",
    "To solidify our understanding, let's implement a simple sequence-to-sequence (Seq2Seq) model with attention for a translation task. We'll use the Scaled Dot-Product Attention mechanism.\n",
    "\n",
    "**Note:** This is a simplified example for educational purposes.\n",
    "\n",
    "### 1. Preparing the Data\n",
    "\n",
    "For demonstration, we'll use dummy data. In practice, you'd use a dataset like the English-French sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e887e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.259416Z",
     "iopub.status.busy": "2024-12-11T17:45:45.259145Z",
     "iopub.status.idle": "2024-12-11T17:45:45.265542Z",
     "shell.execute_reply": "2024-12-11T17:45:45.264758Z"
    },
    "papermill": {
     "duration": 0.013763,
     "end_time": "2024-12-11T17:45:45.267199",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.253436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_sentences = [\n",
    "    \"hello\",\n",
    "    \"how are you\",\n",
    "    \"good morning\",\n",
    "    \"good night\",\n",
    "    \"thank you\"\n",
    "]\n",
    "\n",
    "target_sentences = [\n",
    "    \"bonjour\",\n",
    "    \"comment ça va\",\n",
    "    \"bonjour\",\n",
    "    \"bonne nuit\",\n",
    "    \"merci\"\n",
    "]\n",
    "\n",
    "# Create vocabulary\n",
    "source_vocab = sorted(list(set(\" \".join(source_sentences))))\n",
    "target_vocab = sorted(list(set(\" \".join(target_sentences))))\n",
    "\n",
    "# Add special tokens\n",
    "source_vocab = ['<pad>', '<sos>', '<eos>'] + source_vocab\n",
    "target_vocab = ['<pad>', '<sos>', '<eos>'] + target_vocab\n",
    "\n",
    "source_char2idx = { ch:i for i,ch in enumerate(source_vocab) }\n",
    "source_idx2char = { i:ch for i,ch in enumerate(source_vocab) }\n",
    "\n",
    "target_char2idx = { ch:i for i,ch in enumerate(target_vocab) }\n",
    "target_idx2char = { i:ch for i,ch in enumerate(target_vocab) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665038b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.278116Z",
     "iopub.status.busy": "2024-12-11T17:45:45.277881Z",
     "iopub.status.idle": "2024-12-11T17:45:45.285888Z",
     "shell.execute_reply": "2024-12-11T17:45:45.285030Z"
    },
    "papermill": {
     "duration": 0.015478,
     "end_time": "2024-12-11T17:45:45.287610",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.272132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sources: [[1, 8, 6, 11, 11, 14, 2], [1, 8, 14, 18, 3, 4, 15, 6, 3, 19, 14, 17, 2], [1, 7, 14, 14, 5, 3, 12, 14, 15, 13, 9, 13, 7, 2], [1, 7, 14, 14, 5, 3, 13, 9, 7, 8, 16, 2], [1, 16, 8, 4, 13, 10, 3, 19, 14, 17, 2]]\n",
      "Encoded Targets: [[1, 5, 12, 11, 9, 12, 15, 13, 2], [1, 6, 12, 10, 10, 7, 11, 14, 3, 17, 4, 3, 16, 4, 2], [1, 5, 12, 11, 9, 12, 15, 13, 2], [1, 5, 12, 11, 11, 7, 3, 11, 15, 8, 14, 2], [1, 10, 7, 13, 6, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Convert sentences to indices\n",
    "def encode_sentence(sentence, char2idx):\n",
    "    return [char2idx['<sos>']] + [char2idx[ch] for ch in sentence] + [char2idx['<eos>']]\n",
    "\n",
    "encoded_sources = [encode_sentence(s, source_char2idx) for s in source_sentences]\n",
    "encoded_targets = [encode_sentence(s, target_char2idx) for s in target_sentences]\n",
    "\n",
    "print(\"Encoded Sources:\", encoded_sources)\n",
    "print(\"Encoded Targets:\", encoded_targets)\n",
    "\n",
    "# Define maximum sequence lengths\n",
    "MAX_SOURCE_LEN = max(len(s) for s in encoded_sources)\n",
    "MAX_TARGET_LEN = max(len(t) for t in encoded_targets)\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    return seq + [pad_idx] * (max_len - len(seq))\n",
    "\n",
    "padded_sources = [pad_sequence(s, MAX_SOURCE_LEN, source_char2idx['<pad>']) for s in encoded_sources]\n",
    "padded_targets = [pad_sequence(s, MAX_TARGET_LEN, target_char2idx['<pad>']) for s in encoded_targets]\n",
    "\n",
    "# Convert to tensors\n",
    "source_tensor = torch.tensor(padded_sources, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_targets, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create a simple Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228f54d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.298411Z",
     "iopub.status.busy": "2024-12-11T17:45:45.298199Z",
     "iopub.status.idle": "2024-12-11T17:45:45.302161Z",
     "shell.execute_reply": "2024-12-11T17:45:45.301367Z"
    },
    "papermill": {
     "duration": 0.011238,
     "end_time": "2024-12-11T17:45:45.303850",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.292612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple Dataset\n",
    "dataset = TranslationDataset(source_tensor, target_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15330e3e",
   "metadata": {
    "papermill": {
     "duration": 0.006872,
     "end_time": "2024-12-11T17:45:45.318250",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.311378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 2. Defining the Encoder\n",
    "\n",
    "The encoder processes the input sequence and produces key and value vectors for attention.\n",
    "\n",
    "#### PositionwiseFeedForward \n",
    "\n",
    "Enhances feature representations at each sequence position independently through two linear transformations with an activation function, allowing the model to learn complex, non-linear transformations of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2ee0f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.330011Z",
     "iopub.status.busy": "2024-12-11T17:45:45.329291Z",
     "iopub.status.idle": "2024-12-11T17:45:45.334477Z",
     "shell.execute_reply": "2024-12-11T17:45:45.333805Z"
    },
    "papermill": {
     "duration": 0.012364,
     "end_time": "2024-12-11T17:45:45.335990",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.323626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de964d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.346724Z",
     "iopub.status.busy": "2024-12-11T17:45:45.346465Z",
     "iopub.status.idle": "2024-12-11T17:45:45.353169Z",
     "shell.execute_reply": "2024-12-11T17:45:45.352401Z"
    },
    "papermill": {
     "duration": 0.013889,
     "end_time": "2024-12-11T17:45:45.354723",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.340834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)  # [B, T, E]\n",
    "        for layer in self.layers:\n",
    "            embedded = layer(embedded, mask)  # [B, T, E]\n",
    "        output = self.fc_out(embedded)          # [B, T, E]\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = PositionwiseFeedForward(embed_dim, ff_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)  # [B, T, E]\n",
    "        x = self.layernorm1(x + attn_output)          # [B, T, E]\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.ff(x)                        # [B, T, E]\n",
    "        x = self.layernorm2(x + ff_output)            # [B, T, E]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b8ce2",
   "metadata": {
    "papermill": {
     "duration": 0.004884,
     "end_time": "2024-12-11T17:45:45.364585",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.359701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Defining the Decoder\n",
    "\n",
    "The decoder generates the output sequence, attending to the encoder's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf09292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.377018Z",
     "iopub.status.busy": "2024-12-11T17:45:45.376759Z",
     "iopub.status.idle": "2024-12-11T17:45:45.384420Z",
     "shell.execute_reply": "2024-12-11T17:45:45.383730Z"
    },
    "papermill": {
     "duration": 0.016536,
     "end_time": "2024-12-11T17:45:45.386034",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.369498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        embedded = self.embedding(x)  # [B, T, E]\n",
    "        for layer in self.layers:\n",
    "            embedded = layer(embedded, encoder_output, src_mask, tgt_mask)  # [B, T, E]\n",
    "        output = self.fc_out(embedded)  # [B, T, output_dim]\n",
    "        return output\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = PositionwiseFeedForward(embed_dim, ff_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask) # [B, T, E]\n",
    "        x = self.layernorm1(x + attn_output) # [B, T, E]\n",
    "        \n",
    "        # Cross-attention\n",
    "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)  # [B, T, E]\n",
    "        x = self.layernorm2(x + attn_output)  # [B, T, E]\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.ff(x)  # [B, T, E]\n",
    "        x = self.layernorm3(x + ff_output)  # [B, T, E]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc047ba",
   "metadata": {
    "papermill": {
     "duration": 0.004793,
     "end_time": "2024-12-11T17:45:45.395885",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.391092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Define the Seq2Seq Model\n",
    "\n",
    "Combines the encoder and decoder into a single model. The forward pass involves encoding the source sequence and then decoding the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81356534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.406940Z",
     "iopub.status.busy": "2024-12-11T17:45:45.406662Z",
     "iopub.status.idle": "2024-12-11T17:45:45.412031Z",
     "shell.execute_reply": "2024-12-11T17:45:45.411154Z"
    },
    "papermill": {
     "duration": 0.013169,
     "end_time": "2024-12-11T17:45:45.413914",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.400745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
    "        encoder_output = self.encoder(src, src_mask)      # [B, src_T, E]\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)  # [B, trg_T, output_dim]\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307df262",
   "metadata": {
    "papermill": {
     "duration": 0.004737,
     "end_time": "2024-12-11T17:45:45.423837",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.419100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Training the Model\n",
    "\n",
    "In practice, you'd define a loss function (e.g., CrossEntropyLoss), an optimizer, and iterate over epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96d34dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:45.434635Z",
     "iopub.status.busy": "2024-12-11T17:45:45.434378Z",
     "iopub.status.idle": "2024-12-11T17:45:46.819269Z",
     "shell.execute_reply": "2024-12-11T17:45:46.818380Z"
    },
    "papermill": {
     "duration": 1.392444,
     "end_time": "2024-12-11T17:45:46.821267",
     "exception": false,
     "start_time": "2024-12-11T17:45:45.428823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define model hyperparameters\n",
    "INPUT_DIM = len(source_vocab)\n",
    "OUTPUT_DIM = len(target_vocab)\n",
    "EMBED_DIM = 32      # Set embed_dim equal to hidden_dim\n",
    "NUM_HEADS = 4       # Adjusted number of heads for better representation\n",
    "FF_DIM = 64         # Feed-forward network dimension\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMBED_DIM, NUM_HEADS, FF_DIM, NUM_ENCODER_LAYERS)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBED_DIM, NUM_HEADS, FF_DIM, NUM_DECODER_LAYERS)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_char2idx['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb23159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:46.833256Z",
     "iopub.status.busy": "2024-12-11T17:45:46.832911Z",
     "iopub.status.idle": "2024-12-11T17:45:54.561151Z",
     "shell.execute_reply": "2024-12-11T17:45:54.560177Z"
    },
    "papermill": {
     "duration": 7.735857,
     "end_time": "2024-12-11T17:45:54.562799",
     "exception": false,
     "start_time": "2024-12-11T17:45:46.826942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.9783\n",
      "Epoch: 10, Loss: 1.7617\n",
      "Epoch: 20, Loss: 1.1020\n",
      "Epoch: 30, Loss: 0.5524\n",
      "Epoch: 40, Loss: 0.2994\n",
      "Epoch: 50, Loss: 0.1741\n",
      "Epoch: 60, Loss: 0.1579\n",
      "Epoch: 70, Loss: 0.0988\n",
      "Epoch: 80, Loss: 0.0630\n",
      "Epoch: 90, Loss: 0.0480\n",
      "Epoch: 100, Loss: 0.0399\n",
      "Epoch: 110, Loss: 0.0323\n",
      "Epoch: 120, Loss: 0.0264\n",
      "Epoch: 130, Loss: 0.0235\n",
      "Epoch: 140, Loss: 0.0198\n",
      "Epoch: 150, Loss: 0.0184\n",
      "Epoch: 160, Loss: 0.0160\n",
      "Epoch: 170, Loss: 0.0150\n",
      "Epoch: 180, Loss: 0.0127\n",
      "Epoch: 190, Loss: 0.0114\n",
      "Epoch: 200, Loss: 0.0108\n"
     ]
    }
   ],
   "source": [
    "# Function to create masks\n",
    "def create_masks(src, decoder_input, num_heads=4):\n",
    "    \"\"\"\n",
    "    Create source and target masks.\n",
    "    \n",
    "    Args:\n",
    "        src (Tensor): Source sequences [B, src_T]\n",
    "        decoder_input (Tensor): Decoder input sequences [B, trg_T -1]\n",
    "        num_heads (int): Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        src_mask (Tensor): [B, num_heads, 1, src_T]\n",
    "        trg_mask (Tensor): [B, num_heads, trg_T -1, trg_T -1]\n",
    "    \"\"\"\n",
    "    # Source mask: [B,1,1,src_T] -> [B,num_heads,1,src_T]\n",
    "    src_mask = (src != source_char2idx['<pad>']).unsqueeze(1).unsqueeze(2)  # [B,1,1,src_T]\n",
    "    src_mask = src_mask.repeat(1, num_heads, 1, 1)  # [B,num_heads,1,src_T]\n",
    "    \n",
    "    # Target mask: [B,1,T,1] & [T,T] -> [B,1,T,T] -> [B,num_heads,T,T]\n",
    "    trg_pad_mask = (decoder_input != target_char2idx['<pad>']).unsqueeze(1).unsqueeze(3)  # [B,1,T,1]\n",
    "    trg_len = decoder_input.size(1)\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()  # [T,T]\n",
    "    trg_mask = trg_pad_mask & trg_sub_mask  # [B,1,T,T]\n",
    "    trg_mask = trg_mask.repeat(1, num_heads, 1, 1)  # [B,num_heads,T,T]\n",
    "    \n",
    "    return src_mask, trg_mask\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src_batch, trg_batch in dataloader:\n",
    "        src_batch = src_batch.to(device)  # [B, src_T=14]\n",
    "        trg_batch = trg_batch.to(device)  # [B, trg_T=14]\n",
    "        \n",
    "        # Prepare decoder input and target\n",
    "        decoder_input = trg_batch[:, :-1]  # [B, trg_T -1 =13]\n",
    "        target = trg_batch[:, 1:].contiguous().view(-1)  # [B *13]\n",
    "        \n",
    "        # Create masks based on decoder_input\n",
    "        src_mask, trg_mask = create_masks(src_batch, decoder_input, num_heads=NUM_HEADS)  # trg_mask: [B,4,13,13]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src_batch, decoder_input, src_mask, trg_mask)  # [B, trg_T -1, OUTPUT_DIM]\n",
    "        \n",
    "        # Reshape output to [B * (trg_T -1), OUTPUT_DIM]\n",
    "        output = output.view(-1, OUTPUT_DIM)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print loss every 10 epochs and the first epoch\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch: {epoch+1}, Loss: {epoch_loss / len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7d1989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:54.576130Z",
     "iopub.status.busy": "2024-12-11T17:45:54.575878Z",
     "iopub.status.idle": "2024-12-11T17:45:54.583218Z",
     "shell.execute_reply": "2024-12-11T17:45:54.582519Z"
    },
    "papermill": {
     "duration": 0.015947,
     "end_time": "2024-12-11T17:45:54.584788",
     "exception": false,
     "start_time": "2024-12-11T17:45:54.568841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, source_char2idx, target_idx2char, max_len=MAX_TARGET_LEN):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the source sentence\n",
    "        encoded_src = encode_sentence(sentence, source_char2idx)\n",
    "        encoded_src = pad_sequence(encoded_src, MAX_SOURCE_LEN, source_char2idx['<pad>'])\n",
    "        src_tensor = torch.tensor(encoded_src, dtype=torch.long).unsqueeze(0).to(device)  # [1, src_seq_len]\n",
    "        \n",
    "        # Create source mask\n",
    "        src_mask = (src_tensor != source_char2idx['<pad>']).unsqueeze(1).unsqueeze(2)  # [1,1,1,src_T]\n",
    "        src_mask = src_mask.repeat(1, NUM_HEADS, 1, 1)  # [1,num_heads,1,src_T]\n",
    "        \n",
    "        # Encoder output\n",
    "        encoder_out = encoder(src_tensor, src_mask)  # [1, src_T, E]\n",
    "        \n",
    "        # Initialize decoder input with <sos>\n",
    "        decoder_input = torch.tensor([target_char2idx['<sos>']], dtype=torch.long).unsqueeze(0).to(device)  # [1,1]\n",
    "        \n",
    "        translated_sentence = \"\"\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # Create target mask\n",
    "            trg_pad_mask = (decoder_input != target_char2idx['<pad>']).unsqueeze(1).unsqueeze(3)  # [1,1,T,1]\n",
    "            trg_len = decoder_input.size(1)\n",
    "            trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()  # [T,T]\n",
    "            trg_mask = trg_pad_mask & trg_sub_mask  # [1,1,T,T]\n",
    "            trg_mask = trg_mask.repeat(1, NUM_HEADS, 1, 1)  # [1,num_heads,T,T]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = decoder(decoder_input, encoder_out, src_mask, trg_mask)  # [1, T, OUTPUT_DIM]\n",
    "            \n",
    "            # Get the last time step\n",
    "            output = output[:, -1, :]  # [1, OUTPUT_DIM]\n",
    "            \n",
    "            # Get the predicted token\n",
    "            pred_token = output.argmax(1).item()\n",
    "            pred_char = target_idx2char[pred_token]\n",
    "            \n",
    "            if pred_char == '<eos>':\n",
    "                break\n",
    "            translated_sentence += pred_char\n",
    "            decoder_input = torch.cat([decoder_input, torch.tensor([[pred_token]], dtype=torch.long).to(device)], dim=1)  # [1, T+1]\n",
    "        \n",
    "        return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "687fec98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T17:45:54.597234Z",
     "iopub.status.busy": "2024-12-11T17:45:54.596992Z",
     "iopub.status.idle": "2024-12-11T17:45:54.747013Z",
     "shell.execute_reply": "2024-12-11T17:45:54.746068Z"
    },
    "papermill": {
     "duration": 0.158055,
     "end_time": "2024-12-11T17:45:54.748596",
     "exception": false,
     "start_time": "2024-12-11T17:45:54.590541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello -> bonjour\n",
      "good night -> bonne nuit\n",
      "thank you -> merci\n",
      "how are you -> comment ça va\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"hello\",\n",
    "    \"good night\",\n",
    "    \"thank you\",\n",
    "    \"how are you\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate(sentence, encoder, decoder, source_char2idx, target_idx2char)\n",
    "    print(f\"{sentence} -> {translation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd39598",
   "metadata": {
    "papermill": {
     "duration": 0.005681,
     "end_time": "2024-12-11T17:45:54.760417",
     "exception": false,
     "start_time": "2024-12-11T17:45:54.754736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analysis of Attention Mechanisms\n",
    "\n",
    "Attention mechanisms allow models to dynamically focus on relevant parts of the input, enhancing performance in tasks like machine translation, text summarization, and more. They alleviate the limitations of RNNs by:\n",
    "\n",
    "- **Capturing Long-Range Dependencies:** By directly connecting any two positions in the input, regardless of their distance.\n",
    "- **Improving Parallelization:** Especially in Transformer models, attention enables parallel processing of sequence elements.\n",
    "- **Enhancing Interpretability:** Attention weights can provide insights into which parts of the input the model focuses on during prediction.\n",
    "\n",
    "## Further Steps\n",
    "\n",
    "- **Explore Multi-Head Attention:** Understand how multiple attention heads can capture diverse aspects of the input.\n",
    "- **Implement Transformer Models:** Dive deeper into Transformer architectures, which rely heavily on attention mechanisms.\n",
    "- **Experiment with Different Attention Types:** Implement and compare soft attention, hard attention, and self-attention in various tasks.\n",
    "- **Visualize Attention Weights:** Gain insights by visualizing where the model is focusing its attention during predictions.\n",
    "\n",
    "**Remember:** Attention mechanisms are foundational to many state-of-the-art models in NLP and beyond. Mastering them will significantly enhance your ability to design and understand complex neural network architectures.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Attention Mechanism Explained](https://towardsdatascience.com/attention-mechanism-explained-8f96b26ebae)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.491209,
   "end_time": "2024-12-11T17:45:55.885894",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T17:45:39.394685",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
