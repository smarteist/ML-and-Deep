{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Mechanisms in Deep Learning\n\n## Introduction\n\nIn this notebook, we will:\n\n- **Introduce** the concept of Attention Mechanisms and understand why they are pivotal in modern deep learning models.\n- **Explore** the various types of attention, including Soft Attention, Hard Attention, and Self-Attention.\n- **Implement** key attention mechanisms such as Scaled Dot-Product Attention and Additive Attention using PyTorch.\n- **Provide** resources for further reading to deepen your understanding.\n\n**Resources for Further Reading:**\n\n- [Attention Mechanism Explained](https://towardsdatascience.com/attention-mechanism-explained-8f96b26ebae)\n- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n\n**Prerequisites:**\n\n- Familiarity with Python and PyTorch.\n- Understanding of neural network fundamentals, especially sequence models like RNNs.\n\n**Note:** Attention mechanisms have revolutionized the field of Natural Language Processing (NLP) and are integral to architectures like Transformers. They help models focus on relevant parts of the input sequence, addressing limitations inherent in traditional RNNs.\n\n## Why Attention?: Overcoming Limitations of RNNs\n\nRecurrent Neural Networks (RNNs) are powerful for modeling sequential data, but they have notable limitations:\n\n- **Long-Term Dependencies:** RNNs struggle to capture dependencies between distant elements in a sequence due to issues like vanishing and exploding gradients.\n- **Sequential Processing:** RNNs process data sequentially, making it difficult to parallelize computations.\n- **Fixed-Size Context:** The hidden state in RNNs acts as a bottleneck, limiting the amount of information that can be retained from the input.\n\n**Attention Mechanisms** address these challenges by allowing models to dynamically focus on different parts of the input sequence when producing each element of the output. This leads to better performance, especially in tasks requiring the integration of information from various parts of the input.\n\n## Types of Attention\n\n### 1. Soft vs. Hard Attention\n\n- **Soft Attention:**\n  - **Deterministic and Differentiable:** Allows the model to consider all parts of the input with varying degrees of importance.\n  - **Weighted Sum:** Computes a weighted average of the input features.\n  - **Backpropagation-Friendly:** Can be trained end-to-end using gradient-based optimization.\n\n- **Hard Attention:**\n  - **Stochastic and Non-Differentiable:** Selects specific parts of the input, often requiring reinforcement learning techniques for training.\n  - **Discrete Selection:** Chooses exact elements or regions to focus on.\n  - **Less Common:** Due to training difficulties, hard attention is less frequently used in practice.\n\n### 2. Self-Attention\n\n- **Definition:** A mechanism where different positions of a single sequence are related to each other to compute a representation of the sequence.\n- **Usage:** Fundamental to Transformer architectures, enabling the model to capture dependencies regardless of their distance in the sequence.\n- **Benefits:**\n  - **Parallelization:** Unlike RNNs, self-attention allows for parallel processing of sequence elements.\n  - **Long-Range Dependencies:** Effectively captures relationships between distant elements in the sequence.\n\n## Implementation\n\nWe will implement two primary attention mechanisms:\n\n1. **Scaled Dot-Product Attention**\n2. **Additive Attention**\n\nBoth implementations will be in PyTorch.\n\n### 1. Scaled Dot-Product Attention\n\n**Overview:**\n\nScaled Dot-Product Attention computes the attention weights using the dot product of queries and keys, scales them, applies a softmax to obtain probabilities, and then uses these to weight the values.\n\n**Formula:**\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere:\n- \\( Q \\) = Query matrix\n- \\( K \\) = Key matrix\n- \\( V \\) = Value matrix\n- \\( d_k \\) = Dimension of the keys\n\n**Implementation:**\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, d_k):\n        super(ScaledDotProductAttention, self).__init__()\n        self.d_k = d_k\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        Q: Queries shape (..., seq_len_q, d_k)\n        K: Keys shape (..., seq_len_k, d_k)\n        V: Values shape (..., seq_len_v, d_v)\n        mask: Optional mask tensor\n        \"\"\"\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (..., seq_len_q, seq_len_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn = F.softmax(scores, dim=-1)  # (..., seq_len_q, seq_len_k)\n        output = torch.matmul(attn, V)     # (..., seq_len_q, d_v)\n        return output, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:14.909952Z","iopub.execute_input":"2024-12-09T18:36:14.910409Z","iopub.status.idle":"2024-12-09T18:36:18.049241Z","shell.execute_reply.started":"2024-12-09T18:36:14.910356Z","shell.execute_reply":"2024-12-09T18:36:18.048109Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**Example Usage:**","metadata":{}},{"cell_type":"code","source":"# Example parameters\nbatch_size = 2\nseq_len_q = 3\nseq_len_k = 4\nd_k = 5\nd_v = 6\n\n# Random tensors for Q, K, V\nQ = torch.randn(batch_size, seq_len_q, d_k)\nK = torch.randn(batch_size, seq_len_k, d_k)\nV = torch.randn(batch_size, seq_len_k, d_v)\n\n# Initialize attention module\nattention = ScaledDotProductAttention(d_k)\n\n# Forward pass\noutput, attn_weights = attention(Q, K, V)\n\nprint(\"Output shape:\", output.shape)          # Expected: (2, 3, 6)\nprint(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.050988Z","iopub.execute_input":"2024-12-09T18:36:18.051418Z","iopub.status.idle":"2024-12-09T18:36:18.145525Z","shell.execute_reply.started":"2024-12-09T18:36:18.051386Z","shell.execute_reply":"2024-12-09T18:36:18.144458Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 3, 6])\nAttention weights shape: torch.Size([2, 3, 4])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 2. Additive Attention\n\n**Overview:**\n\nAdditive Attention, introduced by Bahdanau et al., computes attention scores by applying a feed-forward network to the concatenation of queries and keys, followed by a non-linear activation (usually \\( \\tanh \\)).\n\n**Formula:**\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}(\\text{score}(Q, K))V\n$$\n\nwhere:\n\n$$\n\\text{score}(Q, K) = \\mathbf{v}^T \\tanh(\\mathbf{W}_q Q + \\mathbf{W}_k K)\n$$\n\n**Implementation:**","metadata":{}},{"cell_type":"code","source":"class AdditiveAttention(nn.Module):\n    def __init__(self, d_q, d_k, d_attn):\n        super(AdditiveAttention, self).__init__()\n        self.W_q = nn.Linear(d_q, d_attn)\n        self.W_k = nn.Linear(d_k, d_attn)\n        self.v = nn.Linear(d_attn, 1, bias=False)\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        Q: Queries shape (batch_size, seq_len_q, d_q)\n        K: Keys shape (batch_size, seq_len_k, d_k)\n        V: Values shape (batch_size, seq_len_k, d_v)\n        mask: Optional mask tensor\n        \"\"\"\n        # Expand Q and K for addition\n        # Q: (batch_size, seq_len_q, 1, d_q)\n        # K: (batch_size, 1, seq_len_k, d_k)\n        Q_expanded = Q.unsqueeze(2)\n        K_expanded = K.unsqueeze(1)\n        \n        # Apply linear layers and activation\n        energy = torch.tanh(self.W_q(Q_expanded) + self.W_k(K_expanded))  # (batch_size, seq_len_q, seq_len_k, d_attn)\n        scores = self.v(energy).squeeze(-1)  # (batch_size, seq_len_q, seq_len_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn = F.softmax(scores, dim=-1)  # (batch_size, seq_len_q, seq_len_k)\n        output = torch.matmul(attn, V)     # (batch_size, seq_len_q, d_v)\n        return output, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.146736Z","iopub.execute_input":"2024-12-09T18:36:18.147115Z","iopub.status.idle":"2024-12-09T18:36:18.155998Z","shell.execute_reply.started":"2024-12-09T18:36:18.147083Z","shell.execute_reply":"2024-12-09T18:36:18.154590Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Example Usage:**","metadata":{}},{"cell_type":"code","source":"# Example parameters\nbatch_size = 2\nseq_len_q = 3\nseq_len_k = 4\nd_q = 5\nd_k = 5\nd_v = 6\nd_attn = 10\n\n# Random tensors for Q, K, V\nQ = torch.randn(batch_size, seq_len_q, d_q)\nK = torch.randn(batch_size, seq_len_k, d_k)\nV = torch.randn(batch_size, seq_len_k, d_v)\n\n# Initialize attention module\nadditive_attention = AdditiveAttention(d_q, d_k, d_attn)\n\n# Forward pass\noutput, attn_weights = additive_attention(Q, K, V)\n\nprint(\"Output shape:\", output.shape)          # Expected: (2, 3, 6)\nprint(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.158312Z","iopub.execute_input":"2024-12-09T18:36:18.158654Z","iopub.status.idle":"2024-12-09T18:36:18.215153Z","shell.execute_reply.started":"2024-12-09T18:36:18.158621Z","shell.execute_reply":"2024-12-09T18:36:18.214068Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 3, 6])\nAttention weights shape: torch.Size([2, 3, 4])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Self-Attention\n\n**Overview:**\n\nSelf-Attention allows a sequence to interact with itself (i.e., different positions within the same sequence) to compute a representation of the sequence. This mechanism is pivotal in Transformer models.\n\n**Key Components:**\n\n- **Queries, Keys, Values:** Derived from the same input sequence.\n- **Multi-Head Attention:** Extends self-attention by running multiple attention mechanisms in parallel, allowing the model to focus on different representation subspaces.\n\n**Implementation Example:**","metadata":{}},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(SelfAttention, self).__init__()\n        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n        \n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_linear = nn.Linear(embed_dim, embed_dim)\n        self.k_linear = nn.Linear(embed_dim, embed_dim)\n        self.v_linear = nn.Linear(embed_dim, embed_dim)\n        self.out_linear = nn.Linear(embed_dim, embed_dim)\n        \n        self.attention = ScaledDotProductAttention(self.head_dim)\n        \n    def forward(self, x, mask=None):\n        batch_size, seq_len, embed_dim = x.size()\n        \n        # Linear projections\n        Q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)  # (batch, heads, seq_len, head_dim)\n        K = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n        V = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n        \n        # Apply attention on all the projected vectors in batch\n        attn_output, attn_weights = self.attention(Q, K, V, mask=mask)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, embed_dim)\n        \n        # Final linear layer\n        output = self.out_linear(attn_output)\n        return output, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.216413Z","iopub.execute_input":"2024-12-09T18:36:18.216746Z","iopub.status.idle":"2024-12-09T18:36:18.226416Z","shell.execute_reply.started":"2024-12-09T18:36:18.216714Z","shell.execute_reply":"2024-12-09T18:36:18.225220Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Example Usage:**","metadata":{}},{"cell_type":"code","source":"# Example parameters\nbatch_size = 2\nseq_len = 5\nembed_dim = 16\nnum_heads = 4\n\n# Random input tensor\nx = torch.randn(batch_size, seq_len, embed_dim)\n\n# Initialize self-attention module\nself_attention = SelfAttention(embed_dim, num_heads)\n\n# Forward pass\noutput, attn_weights = self_attention(x)\n\nprint(\"Output shape:\", output.shape)            # Expected: (2, 5, 16)\nprint(\"Attention weights shape:\", attn_weights.shape)  # Expected: (2, 4, 5, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.227896Z","iopub.execute_input":"2024-12-09T18:36:18.228281Z","iopub.status.idle":"2024-12-09T18:36:18.257762Z","shell.execute_reply.started":"2024-12-09T18:36:18.228246Z","shell.execute_reply":"2024-12-09T18:36:18.256392Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 5, 16])\nAttention weights shape: torch.Size([2, 4, 5, 5])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Practical Example: Implementing Attention in a Sequence-to-Sequence Model\n\nTo solidify our understanding, let's implement a simple sequence-to-sequence (Seq2Seq) model with attention for a translation task. We'll use the Scaled Dot-Product Attention mechanism.\n\n**Note:** This is a simplified example for educational purposes.\n\n### 1. Preparing the Data\n\nFor demonstration, we'll use dummy data. In practice, you'd use a dataset like the English-French sentence pairs.","metadata":{}},{"cell_type":"code","source":"# Sample data: pairs of sentences\nsource_sentences = [\n    \"hello\",\n    \"how are you\",\n    \"good morning\",\n    \"good night\",\n    \"thank you\"\n]\n\ntarget_sentences = [\n    \"bonjour\",\n    \"comment ça va\",\n    \"bonjour\",\n    \"bonne nuit\",\n    \"merci\"\n]\n\n# Create vocabulary\nsource_vocab = sorted(list(set(\" \".join(source_sentences))))\ntarget_vocab = sorted(list(set(\" \".join(target_sentences))))\n\nsource_char2idx = { ch:i for i,ch in enumerate(source_vocab) }\nsource_idx2char = { i:ch for i,ch in enumerate(source_vocab) }\n\ntarget_char2idx = { ch:i for i,ch in enumerate(target_vocab) }\ntarget_idx2char = { i:ch for i,ch in enumerate(target_vocab) }\n\n# Convert sentences to indices\ndef encode_sentence(sentence, char2idx):\n    return [char2idx[ch] for ch in sentence]\n\nencoded_sources = [encode_sentence(s, source_char2idx) for s in source_sentences]\nencoded_targets = [encode_sentence(s, target_char2idx) for s in target_sentences]\n\nprint(\"Encoded Sources:\", encoded_sources)\nprint(\"Encoded Targets:\", encoded_targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.259161Z","iopub.execute_input":"2024-12-09T18:36:18.259564Z","iopub.status.idle":"2024-12-09T18:36:18.270534Z","shell.execute_reply.started":"2024-12-09T18:36:18.259529Z","shell.execute_reply":"2024-12-09T18:36:18.269136Z"}},"outputs":[{"name":"stdout","text":"Encoded Sources: [[5, 3, 8, 8, 11], [5, 11, 15, 0, 1, 12, 3, 0, 16, 11, 14], [4, 11, 11, 2, 0, 9, 11, 12, 10, 6, 10, 4], [4, 11, 11, 2, 0, 10, 6, 4, 5, 13], [13, 5, 1, 10, 7, 0, 16, 11, 14]]\nEncoded Targets: [[2, 9, 8, 6, 9, 12, 10], [3, 9, 7, 7, 4, 8, 11, 0, 14, 1, 0, 13, 1], [2, 9, 8, 6, 9, 12, 10], [2, 9, 8, 8, 4, 0, 8, 12, 5, 11], [7, 4, 10, 3, 5]]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"\n### 2. Defining the Encoder\n\nThe encoder processes the input sequence and produces key and value vectors for attention.\n","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, num_heads):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim)\n        self.self_attention = SelfAttention(embed_dim, num_heads)\n        self.fc = nn.Linear(embed_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x, mask=None):\n        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n        attn_output, attn_weights = self.self_attention(embedded, mask)\n        output = self.relu(self.fc(attn_output))\n        return output, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.271903Z","iopub.execute_input":"2024-12-09T18:36:18.272247Z","iopub.status.idle":"2024-12-09T18:36:18.294349Z","shell.execute_reply.started":"2024-12-09T18:36:18.272215Z","shell.execute_reply":"2024-12-09T18:36:18.293135Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 3. Defining the Decoder\n\nThe decoder generates the output sequence, attending to the encoder's outputs.","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, embed_dim, hidden_dim, num_heads):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, embed_dim)\n        self.self_attention = SelfAttention(embed_dim, num_heads)\n        self.encoder_attention = ScaledDotProductAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x, encoder_output, mask=None):\n        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n        dec_attn_output, dec_attn_weights = self.self_attention(embedded, mask)\n        \n        # Compute attention with encoder outputs\n        Q = dec_attn_output\n        K = encoder_output\n        V = encoder_output\n        attn_output, attn_weights = self.encoder_attention(Q, K, V, mask)\n        \n        output = self.relu(self.fc(attn_output))\n        return output, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:36:18.296017Z","iopub.execute_input":"2024-12-09T18:36:18.296387Z","iopub.status.idle":"2024-12-09T18:36:18.310854Z","shell.execute_reply.started":"2024-12-09T18:36:18.296340Z","shell.execute_reply":"2024-12-09T18:36:18.309556Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 4. Training the Model\n\nDue to the simplicity of our data, we'll skip the training loop. In practice, you'd define a loss function (e.g., CrossEntropyLoss), an optimizer, and iterate over epochs to train the model.\n\n## Analysis of Attention Mechanisms\n\nAttention mechanisms allow models to dynamically focus on relevant parts of the input, enhancing performance in tasks like machine translation, text summarization, and more. They alleviate the limitations of RNNs by:\n\n- **Capturing Long-Range Dependencies:** By directly connecting any two positions in the input, regardless of their distance.\n- **Improving Parallelization:** Especially in Transformer models, attention enables parallel processing of sequence elements.\n- **Enhancing Interpretability:** Attention weights can provide insights into which parts of the input the model focuses on during prediction.\n\n## Further Steps\n\n- **Explore Multi-Head Attention:** Understand how multiple attention heads can capture diverse aspects of the input.\n- **Implement Transformer Models:** Dive deeper into Transformer architectures, which rely heavily on attention mechanisms.\n- **Experiment with Different Attention Types:** Implement and compare soft attention, hard attention, and self-attention in various tasks.\n- **Visualize Attention Weights:** Gain insights by visualizing where the model is focusing its attention during predictions.\n\n**Remember:** Attention mechanisms are foundational to many state-of-the-art models in NLP and beyond. Mastering them will significantly enhance your ability to design and understand complex neural network architectures.\n\n## References\n\n- [Attention Mechanism Explained](https://towardsdatascience.com/attention-mechanism-explained-8f96b26ebae)\n- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)","metadata":{}}]}