{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e8a3e8-5515-4cf7-b84c-25bf72e1f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/ali/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cdda50-c8e5-41e2-96a3-a1ea1e156c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
    "#!mkdir reuters21578\n",
    "#!tar -xzvf reuters21578.tar.gz -C ./reuters21578\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!mkdir aclImdb_v1\n",
    "#!tar -xzvf aclImdb_v1.tar.gz -C ./aclImdb_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd22f92-8bde-424a-8f3b-ba6479ce9dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[well, ,, here, ', s, a, distasteful, ,, thoro...</td>\n",
       "      <td>neg</td>\n",
       "      <td>well , here ' s a distasteful , thoroughly ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[an, attempt, at, florida, film, noir, ,, palm...</td>\n",
       "      <td>neg</td>\n",
       "      <td>an attempt at florida film noir , palmetto fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[by, trying, to, satisfy, every, kind, of, vie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>by trying to satisfy every kind of viewer , it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[as, a, revolutionary, war, hero, in, the, pat...</td>\n",
       "      <td>pos</td>\n",
       "      <td>as a revolutionary war hero in the patriot , m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\", gordy, \", is, not, a, movie, ,, it, is, a,...</td>\n",
       "      <td>neg</td>\n",
       "      <td>\" gordy \" is not a movie , it is a 90 - minute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words sentiment  \\\n",
       "0  [well, ,, here, ', s, a, distasteful, ,, thoro...       neg   \n",
       "1  [an, attempt, at, florida, film, noir, ,, palm...       neg   \n",
       "2  [by, trying, to, satisfy, every, kind, of, vie...       neg   \n",
       "3  [as, a, revolutionary, war, hero, in, the, pat...       pos   \n",
       "4  [\", gordy, \", is, not, a, movie, ,, it, is, a,...       neg   \n",
       "\n",
       "                                                text  \n",
       "0  well , here ' s a distasteful , thoroughly ama...  \n",
       "1  an attempt at florida film noir , palmetto fai...  \n",
       "2  by trying to satisfy every kind of viewer , it...  \n",
       "3  as a revolutionary war hero in the patriot , m...  \n",
       "4  \" gordy \" is not a movie , it is a 90 - minute...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load IMDb Movie Reviews from NLTK\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(documents, columns=['words', 'sentiment'])\n",
    "\n",
    "# Combine words back into a single string\n",
    "df['text'] = df['words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Display the first few entries\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe0b608-32a5-4685-886e-03b44e33899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " well , here ' s a distasteful , thoroughly amateurish item that , surprisingly , was actually a box - office hit at the time of its release . after just viewing the film for the first time , my primary question is how did anyone with an iq north of 35 enjoy this movie ? it is cheap , idiotic , unfunny , and not nearly as raunchy as i had heard it was . at least some smut would have livened things up a bit . \" porky ' s , \" tells the story ( if you can call it that ) of four clueless high school  ...\n",
      "\n",
      "Tokenized Text:\n",
      " ['well', ',', 'here', \"'\", 's', 'a', 'distasteful', ',', 'thoroughly', 'amateurish', 'item', 'that', ',', 'surprisingly', ',', 'was', 'actually', 'a', 'box', '-', 'office', 'hit', 'at', 'the', 'time', 'of', 'its', 'release', '.', 'after', 'just', 'viewing', 'the', 'film', 'for', 'the', 'first', 'time', ',', 'my', 'primary', 'question', 'is', 'how', 'did', 'anyone', 'with', 'an', 'iq', 'north'] ...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example\n",
    "sample_text = df['text'][0]\n",
    "print(\"Original Text:\\n\", sample_text[:500], \"...\\n\")\n",
    "print(\"Tokenized Text:\\n\", tokenize_text(sample_text)[:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34815555-fc4f-4af0-b98b-7cc13f9321a4",
   "metadata": {},
   "source": [
    "### Lemmatization/Stemming\n",
    "- Lemmatization reduces words to their base or dictionary form.\n",
    "- Stemming reduces words to their root form, which may not be a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c548f2-3ef1-4d6d-abc9-71f491ed2c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      " ['well', ',', 'here', \"'\", 's', 'a', 'distasteful', ',', 'thoroughly', 'amateurish', 'item', 'that', ',', 'surprisingly', ',', 'wa', 'actually', 'a', 'box', '-', 'office', 'hit', 'at', 'the', 'time', 'of', 'it', 'release', '.', 'after', 'just', 'viewing', 'the', 'film', 'for', 'the', 'first', 'time', ',', 'my', 'primary', 'question', 'is', 'how', 'did', 'anyone', 'with', 'an', 'iq', 'north'] ...\n",
      "Stemmed Tokens:\n",
      " ['well', ',', 'here', \"'\", 's', 'a', 'distast', ',', 'thoroughli', 'amateurish', 'item', 'that', ',', 'surprisingli', ',', 'wa', 'actual', 'a', 'box', '-', 'offic', 'hit', 'at', 'the', 'time', 'of', 'it', 'releas', '.', 'after', 'just', 'view', 'the', 'film', 'for', 'the', 'first', 'time', ',', 'my', 'primari', 'question', 'is', 'how', 'did', 'anyon', 'with', 'an', 'iq', 'north'] ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer and Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed\n",
    "\n",
    "# Example\n",
    "tokens = tokenize_text(sample_text)\n",
    "print(\"Lemmatized Tokens:\\n\", lemmatize_tokens(tokens)[:50], \"...\")\n",
    "print(\"Stemmed Tokens:\\n\", stem_tokens(tokens)[:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd809a-3888-4120-bb11-9b9ff24d824c",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "- Stop words are common words that carry minimal meaningful information. Removing them can reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9fc712-4fcf-4351-a149-b865728414d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stop Words Removal:\n",
      " ['well', ',', \"'\", 'distasteful', ',', 'thoroughly', 'amateurish', 'item', ',', 'surprisingly', ',', 'actually', 'box', '-', 'office', 'hit', 'time', 'release', '.', 'viewing', 'film', 'first', 'time', ',', 'primary', 'question', 'anyone', 'iq', 'north', '35', 'enjoy', 'movie', '?', 'cheap', ',', 'idiotic', ',', 'unfunny', ',', 'nearly', 'raunchy', 'heard', '.', 'least', 'smut', 'would', 'livened', 'things', 'bit', '.'] ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "# Example\n",
    "print(\"After Stop Words Removal:\\n\", remove_stopwords(tokens)[:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cbed8-2e6e-4c9a-9129-3268fe3861ae",
   "metadata": {},
   "source": [
    "### Punctuation Removal\n",
    "- Removing punctuation helps in cleaning the text and focusing on the actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9441bc2-991d-47d2-a7f9-860ad1576d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Punctuation Removal:\n",
      " ['well', 'here', 's', 'a', 'distasteful', 'thoroughly', 'amateurish', 'item', 'that', 'surprisingly', 'was', 'actually', 'a', 'box', 'office', 'hit', 'at', 'the', 'time', 'of', 'its', 'release', 'after', 'just', 'viewing', 'the', 'film', 'for', 'the', 'first', 'time', 'my', 'primary', 'question', 'is', 'how', 'did', 'anyone', 'with', 'an', 'iq', 'north', 'of', '35', 'enjoy', 'this', 'movie', 'it', 'is', 'cheap'] ...\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(tokens):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [token.translate(table) for token in tokens]\n",
    "    # Remove tokens that are empty after stripping punctuation\n",
    "    stripped = [token for token in stripped if token]\n",
    "    return stripped\n",
    "\n",
    "# Example\n",
    "print(\"After Punctuation Removal:\\n\", remove_punctuation(tokens)[:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d587f67-380a-4efe-a677-37d9999e515b",
   "metadata": {},
   "source": [
    "## Pipeline Integration\n",
    "We'll combine all the preprocessing steps into a reusable class called TextPreprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4b46e8-2c86-4353-a4c6-e90f6f50f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens:\n",
      " ['well', 'distasteful', 'thoroughly', 'amateurish', 'item', 'surprisingly', 'actually', 'box', 'office', 'hit', 'time', 'release', 'viewing', 'film', 'first', 'time', 'primary', 'question', 'anyone', 'iq', 'north', '35', 'enjoy', 'movie', 'cheap', 'idiotic', 'unfunny', 'nearly', 'raunchy', 'heard', 'least', 'smut', 'would', 'livened', 'thing', 'bit', 'porky', 'tell', 'story', 'call', 'four', 'clueless', 'high', 'school', 'buddy', 'pee', 'wee', 'dan', 'monahan', 'billy'] ...\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, method='lemmatize'):\n",
    "        self.method = method\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
    "    \n",
    "    def remove_punctuation(self, tokens):\n",
    "        stripped = [token.translate(self.punctuation_table) for token in tokens]\n",
    "        return [token for token in stripped if token]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        tokens = self.remove_punctuation(tokens)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        if self.method == 'lemmatize':\n",
    "            tokens = self.lemmatize(tokens)\n",
    "        elif self.method == 'stem':\n",
    "            tokens = self.stem(tokens)\n",
    "        return tokens\n",
    "\n",
    "# Initialize Preprocessor\n",
    "preprocessor = TextPreprocessor(method='lemmatize')\n",
    "\n",
    "# Apply preprocessing to the first sample\n",
    "processed_tokens = preprocessor.preprocess(sample_text)\n",
    "print(\"Processed Tokens:\\n\", processed_tokens[:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66edb6-fe12-4d03-811d-aeaa51253057",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We'll compare the original text with the preprocessed text to demonstrate the effectiveness of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e50d9a1-6b49-4fc3-be59-39a62bd35486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "\n",
      "Original Text:\n",
      " i had a chance to see a sneak preview of city slickers ii on campus last night . i went in with the expectation of a film with the similar flavor which made the original such a success : personal growth and insightful humor about life . i came away somewhat disappointed in this regard , getting some of the latter and not much of the former . the basic plot revolves around billy crystal , who plays mitch robbins , turning forty . he ' s now become the station manager of the radio station in which ...\n",
      "\n",
      "Processed Text:\n",
      " chance see sneak preview city slicker ii campus last night went expectation film similar flavor made original success personal growth insightful humor life came away somewhat disappointed regard getting latter much former basic plot revolves around billy crystal play mitch robbins turning forty become station manager radio station worked original given job radio station best friend played daniel stern sympathy stern character work divorce loser younger brother john lovitz come mitch house bum mo ...\n",
      "\n",
      "\n",
      "\n",
      "--- Sample 2 ---\n",
      "\n",
      "Original Text:\n",
      " working in the motion picture industry must be a constant source of frustration for a front - line african american actress like jada pinkett smith . despite being one of the freshest talents available , pinkett smith has often been relegated to playing thankless supporting parts ( a low down dirty shame , the nutty professor ) . the problem is , of course , that there aren ' t many good roles available for black women . take away the likes of waiting to exhale , set it off , soul food , and eve ...\n",
      "\n",
      "Processed Text:\n",
      " working motion picture industry must constant source frustration front line african american actress like jada pinkett smith despite one freshest talent available pinkett smith often relegated playing thankless supporting part low dirty shame nutty professor problem course many good role available black woman take away like waiting exhale set soul food eve bayou left chance someone girlfriend local whore murder victim result pinkett smith first opportunity atop marquee stuck stupid formulaic rom ...\n",
      "\n",
      "\n",
      "\n",
      "--- Sample 3 ---\n",
      "\n",
      "Original Text:\n",
      " this has some major spoilers for the film , so be forwarned . the won ' t appear till later in the article , so you ' re safe for now . eyes wide shot ( 1999 ) running time : 2 hours 39 minutes 2 . 35 : 1 theatrical aspect starring : tom cruise ( dr . bill hardford ) , nicole kidman ( alice hardford ) , sidney pollack , leelee sobieski , todd field ( nick nightingale ) directed , co - written , produced by : stanley kubrick inspired by : \" traumnovelle \" by : arthur schnitzer this is an interest ...\n",
      "\n",
      "Processed Text:\n",
      " major spoiler film forwarned appear till later article safe eye wide shot 1999 running time 2 hour 39 minute 2 35 1 theatrical aspect starring tom cruise dr bill hardford nicole kidman alice hardford sidney pollack leelee sobieski todd field nick nightingale directed co written produced stanley kubrick inspired traumnovelle arthur schnitzer interesting film must first state musn believe rumor film cruise cross dresser kidman shoot heroin scene two would suggest aided sex therapist look film sugg ...\n",
      "\n",
      "\n",
      "\n",
      "--- Sample 4 ---\n",
      "\n",
      "Original Text:\n",
      " the thirteenth floor , the third in what i would call \" the reality check movie series \" , is very similar to the other reality check movies released this year , the matrix and existenz . all three made you think , made you wonder what is real , what isn ' t , and if our world is just a huge game . the thirteenth floor doesn ' t reach the level of originality , creativity , and curiosity sparked by the matrix and existenz , but it certainly gives a great shot at it . in the matrix , we were told ...\n",
      "\n",
      "Processed Text:\n",
      " thirteenth floor third would call reality check movie series similar reality check movie released year matrix existenz three made think made wonder real world huge game thirteenth floor reach level originality creativity curiosity sparked matrix existenz certainly give great shot matrix told human simply virus existenz learned life could game thirteenth floor learn electronic device living another world electronic device one world top another everything fake electronically generated thirteenth f ...\n",
      "\n",
      "\n",
      "\n",
      "--- Sample 5 ---\n",
      "\n",
      "Original Text:\n",
      " capsule : bleak and point - blank -- just the way it should be . makes its case with cold - blooded precision and intelligence . \" serial killer chic \" is a relatively new entry to the cultural vocabulary , and one of the more stomach - turning ones . i ' ve never found someone automatically admirable or even interesting because they killed creatively or in great numbers , and because of that i had apprehensions about seeing henry . i wasn ' t sure i wanted to see the story of multiple murderer  ...\n",
      "\n",
      "Processed Text:\n",
      " capsule bleak point blank way make case cold blooded precision intelligence serial killer chic relatively new entry cultural vocabulary one stomach turning one never found someone automatically admirable even interesting killed creatively great number apprehension seeing henry sure wanted see story multiple murderer henry lee lucas portrayed kind nihilistic hero antihero thankfully way henry work director john mcnaughton took small budget gallery actor friend created chilling intelligent piece w ...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and return text\n",
    "def preprocess_text(preprocessor, text):\n",
    "    tokens = preprocessor.preprocess(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Select 5 samples\n",
    "samples = df.sample(5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preprocess samples\n",
    "samples['processed'] = samples['text'].apply(lambda x: preprocess_text(preprocessor, x))\n",
    "\n",
    "# Display Before and After\n",
    "for i in range(5):\n",
    "    print(f\"--- Sample {i+1} ---\\n\")\n",
    "    print(\"Original Text:\\n\", samples['text'][i][:500], \"...\\n\")\n",
    "    print(\"Processed Text:\\n\", samples['processed'][i][:500], \"...\\n\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea33dbe-9d27-4ed1-9c2e-83db2096507c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
