{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225e5214-75ed-4291-8252-e3d753877aff",
   "metadata": {},
   "source": [
    "# **NLP Evaluation Metrics: BLEU, ROUGE, and Perplexity**\n",
    "---\n",
    "\n",
    "Evaluation metrics are crucial in assessing the performance of NLP models. They help in quantifying how well a model's output aligns with the expected results. This project focuses on three widely-used evaluation metrics:\n",
    "\n",
    "- **BLEU (Bilingual Evaluation Understudy)**\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "- **Perplexity**\n",
    "\n",
    "Each metric serves different purposes and is suitable for various NLP tasks such as machine translation, text summarization, and language modeling.\n",
    "\n",
    "Before diving into the implementation, ensure that you have the necessary libraries installed. You can install them using `pip`.\n",
    "\n",
    "```python\n",
    "!pip install nltk\n",
    "!pip install rouge\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97be660f-a654-4874-8407-25186cc28e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe73c95-025b-489f-a571-b9815768c629",
   "metadata": {},
   "source": [
    "## **BLEU Score**\n",
    "\n",
    "### **What is BLEU?**\n",
    "\n",
    "**BLEU** (Bilingual Evaluation Understudy) is a metric for evaluating the quality of text which has been machine-translated from one language to another. It measures how closely a candidate translation matches one or more reference translations by calculating the precision of n-grams.\n",
    "\n",
    "**Key Points:**\n",
    "- Primarily used for machine translation.\n",
    "- Considers up to 4-grams.\n",
    "- Applies a brevity penalty to prevent short translations from scoring artificially high.\n",
    "\n",
    "### **Implementing BLEU in Python**\n",
    "\n",
    "Let's implement BLEU score calculation using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8374d9-61dc-4825-beeb-97e49c543260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Example reference and candidate sentences\n",
    "reference = [\n",
    "    'The cat is on the mat'.split(),\n",
    "    'There is a cat on the mat'.split()\n",
    "]\n",
    "candidate = 'The cat is on the mat'.split()\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526560-27f7-41d9-b395-6a8b88b63c41",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- A BLEU score of `1.0` indicates a perfect match between the candidate and reference sentences.\n",
    "- In real-world scenarios, scores range between `0` and `1`.\n",
    "\n",
    "#### **Practical Example: Machine Translation**\n",
    "\n",
    "Let's consider a simple example of evaluating machine-translated sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ca6ec4-caa6-451d-a879-e1a7d496ce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3976\n"
     ]
    }
   ],
   "source": [
    "# References (human translations)\n",
    "references = [\n",
    "    ['this', 'is', 'a', 'test'],\n",
    "    ['this', 'is', 'test']\n",
    "]\n",
    "\n",
    "# Candidate (machine translation)\n",
    "candidate = ['it', 'is', 'a', 'test']\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = sentence_bleu(references, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "print(f\"BLEU Score: {bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b135e-a288-4bd3-9932-313cb0d7bfca",
   "metadata": {},
   "source": [
    "## **ROUGE Score**\n",
    "\n",
    "### **What is ROUGE?**\n",
    "\n",
    "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating automatic summarization and machine translation that compares an automatically produced summary or translation against a set of reference summaries (typically human-produced).\n",
    "\n",
    "**Key Variants:**\n",
    "- **ROUGE-N**: Overlap of n-grams.\n",
    "- **ROUGE-L**: Longest Common Subsequence.\n",
    "- **ROUGE-W**: Weighted longest common subsequence.\n",
    "- **ROUGE-S**: Skip-bigram.\n",
    "\n",
    "### **Implementing ROUGE in Python**\n",
    "\n",
    "We'll use the `rouge` library to calculate ROUGE scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9554eeb1-33bc-4dc0-9351-a751dbc70859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.8333333333333334, 'p': 0.7142857142857143, 'f': 0.7692307642603551}, 'rouge-2': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596}, 'rouge-l': {'r': 0.6666666666666666, 'p': 0.5714285714285714, 'f': 0.6153846104142012}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Initialize Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "# Example summaries\n",
    "reference_summary = \"The cat is on the mat.\"\n",
    "candidate_summary = \"There is a cat on the mat.\"\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(candidate_summary, reference_summary)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410aac4-988e-42be-8a37-0ec185f76a13",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "- **ROUGE-1**: Unigram overlap.\n",
    "- **ROUGE-2**: Bigram overlap.\n",
    "- **ROUGE-L**: Longest Common Subsequence.\n",
    "\n",
    "#### **Practical Example: Text Summarization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc73db4-ae35-4af1-bd13-d6cce08dbd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1 - Precision: 0.2857, Recall: 0.2500, F1-Score: 0.2667\n",
      "rouge-2 - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "rouge-l - Precision: 0.2857, Recall: 0.2500, F1-Score: 0.2667\n"
     ]
    }
   ],
   "source": [
    "# Initialize Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "# Reference summary (human-generated)\n",
    "reference = \"Natural language processing enables computers to understand human language.\"\n",
    "\n",
    "# Candidate summary (machine-generated)\n",
    "candidate = \"NLP allows machines to comprehend human languages.\"\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(candidate, reference)\n",
    "for metric, score in scores[0].items():\n",
    "    print(f\"{metric} - Precision: {score['p']:.4f}, Recall: {score['r']:.4f}, F1-Score: {score['f']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9c9db-5553-4ff3-90e6-ce20ab2e1c48",
   "metadata": {},
   "source": [
    "## **Perplexity**\n",
    "\n",
    "### **What is Perplexity?**\n",
    "\n",
    "**Perplexity** is a measurement of how well a probability model predicts a sample. In the context of language models, it quantifies how well the model predicts the next word in a sequence. Lower perplexity indicates better performance.\n",
    "\n",
    "**Key Points:**\n",
    "- Commonly used to evaluate language models.\n",
    "- Related to the entropy of the model.\n",
    "- Lower perplexity implies the model is more confident in its predictions.\n",
    "\n",
    "### **Implementing Perplexity in Python**\n",
    "\n",
    "We'll use the `transformers` library with a pre-trained GPT-2 model to calculate perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21b303e-c7ca-48ed-b045-afbcad590617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebafe51453a4019a92bdd68ee227fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0dae3ba45649fabfa7b4c8a8d85779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d9c763702c4ebda3304908c81e97a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cf74e9a3714b8e8c7ad2beea6a258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0888b5b4824ab28efe43c31a8bdf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cfd33dc7f847b7a700d66863e5762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca4f621e90c4e8498a5277cbe94cb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 45.53\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "# Calculate loss\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6215f-9fdd-45c3-9bb5-79da81073eb5",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The perplexity value indicates how well the model predicts the given sentence.\n",
    "- A lower value suggests better predictive performance.\n",
    "\n",
    "#### **Practical Example: Comparing Perplexity**\n",
    "\n",
    "Let's compare the perplexity of two different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7d67405-a470-4c56-b1ac-0ec7577e2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 Perplexity: 162.47\n",
      "Sentence 2 Perplexity: 386.45\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(sentence):\n",
    "    inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"asdfghjkl qwertyuiop zxcvbnm.\"\n",
    "\n",
    "# Calculate perplexities\n",
    "perplexity1 = calculate_perplexity(sentence1)\n",
    "perplexity2 = calculate_perplexity(sentence2)\n",
    "\n",
    "print(f\"Sentence 1 Perplexity: {perplexity1:.2f}\")\n",
    "print(f\"Sentence 2 Perplexity: {perplexity2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32037c-54de-4433-b0bd-0c2d4b276c59",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The first sentence is grammatically correct and coherent, resulting in lower perplexity.\n",
    "- The second sentence contains random characters, leading to extremely high perplexity, indicating poor predictability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this mini project, we've explored three fundamental NLP evaluation metrics:\n",
    "\n",
    "1. **BLEU**: Suitable for evaluating machine translation by measuring n-gram overlaps between candidate and reference translations.\n",
    "2. **ROUGE**: Ideal for assessing text summarization by comparing overlaps in n-grams, longest common subsequences, etc.\n",
    "3. **Perplexity**: Essential for evaluating language models by quantifying how well a model predicts a sample.\n",
    "\n",
    "Understanding and implementing these metrics is crucial for developing and fine-tuning NLP models effectively. By leveraging libraries like NLTK, Rouge, and Transformers, you can seamlessly integrate these evaluation techniques into your NLP projects.\n",
    "\n",
    "Feel free to expand this project by applying these metrics to larger datasets, experimenting with different models, or exploring additional evaluation metrics!\n",
    "\n",
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [ROUGE Paper](https://www.aclweb.org/anthology/W04-1013/)\n",
    "- [Transformers Documentation](https://huggingface.co/transformers/)\n",
    "- [BLEU Score](https://en.wikipedia.org/wiki/BLEU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
