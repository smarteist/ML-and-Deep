{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning BERT and Exploring Its Variants\n\n## Introduction\n\nBidirectional Encoder Representations from Transformers (BERT) has significantly advanced Natural Language Processing (NLP) by providing powerful, pre-trained language representations. Fine-tuning BERT for specific downstream tasks enables leveraging its deep understanding of language to achieve state-of-the-art performance.\n\nIn this notebook, we will:\n\n1. **Fine-Tune BERT for Various NLP Tasks:**\n   - **Question Answering (QA)**\n   - **Named Entity Recognition (NER)**\n\n2. **Explore BERT Variants and Extensions:**\n   - **RoBERTa**\n   - **DistilBERT**\n   - **ALBERT**\n   - **Domain-Specific BERT Models**\n\nBy the end of this notebook, you'll have hands-on experience fine-tuning BERT for different applications and an understanding of its powerful variants.\n\n**Resources for Further Reading:**\n\n- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Devlin et al.\n- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)\n- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n\n**Prerequisites:**\n\n- Basic understanding of Python and PyTorch\n- Familiarity with neural network concepts\n- Knowledge of NLP tasks and tokenization\n\n---\n\n## 1. Fine-Tuning BERT\n\nFine-tuning involves taking a pre-trained BERT model and training it on a specific task with task-specific data. We'll explore fine-tuning BERT for three popular NLP tasks: Text Classification, Question Answering, and Named Entity Recognition.\n\n### 1.1 Fine-Tuning BERT for Question Answering (QA)\n\n**Objective:** Given a context paragraph and a question, predict the span of text in the context that answers the question using the SQuAD (Stanford Question Answering Dataset).\n\n#### 1.1.1 Setup\n\nFirst, ensure that the necessary libraries are installed. We'll primarily use Hugging Face's `transformers` library, which provides easy-to-use interfaces for BERT and its variants.\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch datasets evaluate seqeval matplotlib scikit-learn datasets seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:16:56.039264Z","iopub.execute_input":"2024-12-11T10:16:56.039610Z","iopub.status.idle":"2024-12-11T10:17:04.599240Z","shell.execute_reply.started":"2024-12-11T10:16:56.039577Z","shell.execute_reply":"2024-12-11T10:17:04.598317Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#### 1.1.1 Loading and Preprocessing the SQuAD Dataset","metadata":{"execution":{"iopub.status.busy":"2024-12-10T19:56:54.551039Z","iopub.execute_input":"2024-12-10T19:56:54.551470Z","iopub.status.idle":"2024-12-10T19:56:54.585341Z","shell.execute_reply.started":"2024-12-10T19:56:54.551431Z","shell.execute_reply":"2024-12-10T19:56:54.583876Z"}}},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, AutoTokenizer\nfrom datasets import load_dataset\n\n# Load the SQuAD dataset\nsquad = load_dataset('squad')\n\n# Inspect the dataset\nprint(squad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:17:04.601227Z","iopub.execute_input":"2024-12-11T10:17:04.601534Z","iopub.status.idle":"2024-12-11T10:17:07.557792Z","shell.execute_reply.started":"2024-12-11T10:17:04.601506Z","shell.execute_reply":"2024-12-11T10:17:07.557021Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062aecf1f4cb4d01bc7184506580b3e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d40ea9b8b346cc9544e87a9de610f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df68857907549c3ac42c023dae98225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"994c4757284b4b1587bf6492b7c8c2b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5438d3c52ebc44e996a5574ed2b1ee2e"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 1.1.2 Tokenization and Alignment\n\nTokenize the inputs, ensuring that the model can predict the start and end positions of the answer.","metadata":{}},{"cell_type":"code","source":"import torch\n\n\n# Initialize the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nqa_model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n\n# Move the model to the desired device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nqa_model.to(device)\n\ndef tokenize_qa(examples):\n    return tokenizer(\n        examples['question'],\n        examples['context'],\n        truncation=\"only_second\",  # Truncate only the context\n        padding='max_length',\n        max_length=512,\n        return_offsets_mapping=True  # Needed for answer alignment\n    )\n\n# Apply tokenization WITHOUT removing 'id' and 'answers'\ntokenized_squad = squad.map(\n    tokenize_qa,\n    batched=True,\n    remove_columns=['title', 'context', 'question']  # Keep 'id' and 'answers'\n)\n\ndef align_answers(examples):\n    start_positions = []\n    end_positions = []\n    for i in range(len(examples['input_ids'])):\n        # Get the answer's character start and end positions\n        answer_start_char = examples['answers'][i]['answer_start'][0]\n        answer_text = examples['answers'][i]['text'][0]\n        answer_end_char = answer_start_char + len(answer_text)\n\n        # Get the offset mappings for the current example\n        offsets = examples['offset_mapping'][i]\n\n        # Initialize start and end token positions\n        start_pos = 0\n        end_pos = 0\n\n        # Find the start token\n        for idx, (start, end) in enumerate(offsets):\n            if start <= answer_start_char < end:\n                start_pos = idx\n                break\n\n        # Find the end token\n        for idx, (start, end) in enumerate(offsets):\n            if start < answer_end_char <= end:\n                end_pos = idx\n                break\n\n        # Handle cases where the answer might not be fully contained within the max_length\n        if not (start_pos and end_pos):\n            start_pos = 0\n            end_pos = 0\n\n        start_positions.append(start_pos)\n        end_positions.append(end_pos)\n\n    examples['start_positions'] = start_positions\n    examples['end_positions'] = end_positions\n    return examples\n\n# Apply answer alignment WITHOUT removing 'answers' yet\ntokenized_squad = tokenized_squad.map(\n    align_answers,\n    batched=True,\n    remove_columns=['offset_mapping']  # Remove offset_mapping after alignment\n)\n\n# Verify columns\nprint(\"Columns in the dataset:\", tokenized_squad['validation'].column_names)\n# Expected Output: ['id', 'input_ids', 'token_type_ids', 'attention_mask', 'answers', 'start_positions', 'end_positions']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:18:53.121431Z","iopub.execute_input":"2024-12-11T10:18:53.121866Z","iopub.status.idle":"2024-12-11T10:23:43.431931Z","shell.execute_reply.started":"2024-12-11T10:18:53.121833Z","shell.execute_reply":"2024-12-11T10:23:43.430996Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a641d27fba4bc4b020b23848db8704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040effc6ec9a4e9faa8b8687d1c5a037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3e3aaba1ca42f2b1b64c3c7f31062f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361208a8957f44c6be86e8ac642aa0f9"}},"metadata":{}},{"name":"stdout","text":"Columns in the dataset: ['id', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"*Note:* Aligning the character positions of answers to token positions is non-trivial and requires handling the mapping between tokens and original text. Hugging Face provides tools to facilitate this, but for brevity, we'll assume the data is already aligned.\n\n#### 1.1.3 Creating DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# Set format for PyTorch, including 'id' and 'answers' without converting them to tensors\ntokenized_squad.set_format(\n    type='torch',\n    columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'id', 'answers']\n)\n\n# Use a subset for demonstration to reduce training time\ntrain_qa = tokenized_squad['train'].select(range(1000))\ntest_qa = tokenized_squad['validation'].select(range(100))\n\ntrain_loader_qa = DataLoader(train_qa, batch_size=4, shuffle=True)\ntest_loader_qa = DataLoader(test_qa, batch_size=4)\n\nprint(\"Number of training batches (QA):\", len(train_loader_qa))\nprint(\"Number of testing batches (QA):\", len(test_loader_qa))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:56.191714Z","iopub.execute_input":"2024-12-11T10:24:56.192553Z","iopub.status.idle":"2024-12-11T10:24:56.206334Z","shell.execute_reply.started":"2024-12-11T10:24:56.192518Z","shell.execute_reply":"2024-12-11T10:24:56.205400Z"}},"outputs":[{"name":"stdout","text":"Number of training batches (QA): 250\nNumber of testing batches (QA): 25\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"#### 1.1.4 Defining the QA Model\n\nLoad the pre-trained BERT model for question answering.","metadata":{}},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering\n\n\n# Verify that 'id' is present in batches\nfor batch in test_loader_qa:\n    print(\"Batch keys:\", batch.keys())\n    print(\"Batch 'id' example:\", batch['id'][0])\n    break  # Only inspect the first batch\n\n# Reload the model (if not already loaded)\nqa_model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\nqa_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:26:13.647906Z","iopub.execute_input":"2024-12-11T10:26:13.648718Z","iopub.status.idle":"2024-12-11T10:26:14.082617Z","shell.execute_reply.started":"2024-12-11T10:26:13.648684Z","shell.execute_reply":"2024-12-11T10:26:14.081759Z"}},"outputs":[{"name":"stdout","text":"Batch keys: dict_keys(['id', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'])\nBatch 'id' example: 56be4db0acb8001400a502ec\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"#### 1.1.5 Setting Up the Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\n\n# Setup optimizer and scheduler\nepochs = 2\ntotal_steps = len(train_loader_qa) * epochs\n\noptimizer_qa = AdamW(qa_model.parameters(), lr=3e-5)\n\nscheduler_qa = get_linear_schedule_with_warmup(optimizer_qa,\n                                              num_warmup_steps=0,\n                                              num_training_steps=total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:26:18.750091Z","iopub.execute_input":"2024-12-11T10:26:18.750425Z","iopub.status.idle":"2024-12-11T10:26:18.768121Z","shell.execute_reply.started":"2024-12-11T10:26:18.750398Z","shell.execute_reply":"2024-12-11T10:26:18.767262Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"#### 1.1.6 Training the QA Model","metadata":{}},{"cell_type":"code","source":"import evaluate\nfrom tqdm.auto import tqdm\n\n# Initialize the SQuAD metric\nsquad_metric = evaluate.load('squad')\n\n# Function to calculate QA accuracy (simplified)\ndef qa_accuracy(preds, labels):\n    # This is a simplified version; proper evaluation requires exact match and F1\n    return (preds == labels).sum().item() / len(labels)\n\n# Training Loop\nqa_model.train()\n\nfor epoch in range(epochs):\n    print(f'\\nEpoch {epoch + 1}/{epochs}')\n    total_loss = 0\n    progress_bar = tqdm(train_loader_qa, desc=\"Training QA\")\n    \n    for batch in progress_bar:\n        optimizer_qa.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        \n        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask,\n                           start_positions=start_positions, end_positions=end_positions)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_qa.step()\n        scheduler_qa.step()\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix({'loss': loss.item()})\n    \n    avg_loss = total_loss / len(train_loader_qa)\n    print(f'Training Loss: {avg_loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:26:27.105797Z","iopub.execute_input":"2024-12-11T10:26:27.106474Z","iopub.status.idle":"2024-12-11T10:29:47.405919Z","shell.execute_reply.started":"2024-12-11T10:26:27.106439Z","shell.execute_reply":"2024-12-11T10:29:47.405135Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"875d17b5b7764201a20375b4814b38da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1de0580a71441cebdc161b8ffd228f1"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training QA:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba31b607b3b4bd5a26b6c055a5e38a0"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 3.9648\n\nEpoch 2/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training QA:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23dbaffb156a42aca1cc5fc07bcb7bca"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 2.1461\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### 1.1.7 Evaluating the QA Model\n\nAssess the model's performance on the test set using metrics like Exact Match (EM) and F1 score.","metadata":{}},{"cell_type":"code","source":"# Evaluation Code Starts Here\nqa_model.eval()  # Set model to evaluation mode\n\n# Initialize lists to store predictions and references\npredictions = []\nreferences = []\n\n# Create a mapping from 'id' to the original example for quick lookup\noriginal_validation = load_dataset('squad', split='validation').select(range(100))\nid_to_example = {example['id']: example for example in original_validation}\n\nfor batch in tqdm(test_loader_qa, desc=\"Evaluating QA\"):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    ids = batch['id']  # This is a list of strings\n\n    with torch.no_grad():\n        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n    # Move logits to CPU and convert to numpy\n    start_logits = start_logits.cpu().numpy()\n    end_logits = end_logits.cpu().numpy()\n\n    # Iterate over each example in the batch\n    for i in range(len(ids)):\n        id_ = ids[i]  # Already a string, no need for .item()\n\n        # Retrieve the original example using the 'id'\n        example = id_to_example[id_]\n        context = example['context']\n        question = example['question']\n        answers = example['answers']['text']\n\n        # Get the most likely start and end positions\n        start_idx = start_logits[i].argmax()\n        end_idx = end_logits[i].argmax()\n\n        # Decode the tokens to get the answer string\n        tokens = tokenizer.convert_ids_to_tokens(input_ids[i])\n\n        # Handle cases where end_idx is before start_idx\n        if end_idx < start_idx:\n            answer = \"\"\n        else:\n            # Join the tokens from start_idx to end_idx\n            answer_tokens = tokens[start_idx:end_idx + 1]\n            # Clean up tokens\n            answer = tokenizer.convert_tokens_to_string(answer_tokens)\n\n        # Append only 'id' and 'prediction_text' to predictions\n        predictions.append({\n            'id': id_,\n            'prediction_text': answer\n        })\n\n        # Append to references as per SQuAD metric requirements\n        references.append({\n            'id': id_,\n            'answers': {\n                'text': answers,\n                'answer_start': example['answers']['answer_start']\n            }\n        })\n\n# Compute the metrics\nresults = squad_metric.compute(predictions=predictions, references=references)\n\nprint(f\"\\nExact Match: {results['exact_match']:.2f}\")\nprint(f\"F1 Score: {results['f1']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:30:18.839231Z","iopub.execute_input":"2024-12-11T10:30:18.840373Z","iopub.status.idle":"2024-12-11T10:30:23.446863Z","shell.execute_reply.started":"2024-12-11T10:30:18.840335Z","shell.execute_reply":"2024-12-11T10:30:23.445846Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating QA:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a50f8007194d6fb6765c058733e689"}},"metadata":{}},{"name":"stdout","text":"\nExact Match: 49.00\nF1 Score: 53.09\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"*Note:* For a comprehensive evaluation, use the full SQuAD dataset and proper answer alignment.\n\n---\n\n### 1.2 Fine-Tuning BERT for Named Entity Recognition (NER)\n\n**Objective:** Identify and classify named entities (e.g., person, organization, location) in text using the CoNLL-2003 dataset.\n\n#### 1.2.1 Loading and Preprocessing the CoNLL-2003 Dataset\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertForTokenClassification\n\n# Load the CoNLL-2003 NER dataset\nner = load_dataset('conll2003')\n\n# Inspect the dataset\nprint(ner)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:30:30.524309Z","iopub.execute_input":"2024-12-11T10:30:30.524998Z","iopub.status.idle":"2024-12-11T10:30:46.096191Z","shell.execute_reply.started":"2024-12-11T10:30:30.524963Z","shell.execute_reply":"2024-12-11T10:30:46.095448Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7576f076da447b872c47b396411e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7a70c4da0e457b8f20b6404c29da58"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7056949edd04a79b591295b11e52541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab45dfd973404af0be3434e4477bbf16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372fc4ca1b9e4695941370123e210131"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54da550660454af48465cd343958e415"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"#### 1.2.2 Tokenization and Alignment\n\nTokenize the inputs, ensuring that the labels align with the tokenized words.\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import DataLoader\n\n# Get the label list from the dataset\nlabel_list = ner['train'].features['ner_tags'].feature.names\nnum_labels = len(label_list)\nprint(\"NER Labels:\", label_list)\n\n# Initialize the fast tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'],\n        truncation=True,\n        is_split_into_words=True,\n        padding='max_length',\n        max_length=128\n    )\n    \n    labels = []\n    for i, label in enumerate(examples['ner_tags']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Special tokens\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)  # Subsequent tokens in a word\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    \n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs\n\n# Apply tokenization and label alignment\ntokenized_ner = ner.map(tokenize_and_align_labels, batched=True)\n\n# Set format to PyTorch tensors\ntokenized_ner.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Create DataLoaders\ntrain_ner = tokenized_ner['train']\nvalidation_ner = tokenized_ner['validation']\ntest_ner = tokenized_ner['test']\n\ntrain_loader_ner = DataLoader(train_ner, batch_size=8, shuffle=True)\nvalidation_loader_ner = DataLoader(validation_ner, batch_size=8)\ntest_loader_ner = DataLoader(test_ner, batch_size=8)\n\nprint(\"Number of training batches (NER):\", len(train_loader_ner))\nprint(\"Number of testing batches (NER):\", len(test_loader_ner))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:33:12.635858Z","iopub.execute_input":"2024-12-11T10:33:12.636717Z","iopub.status.idle":"2024-12-11T10:33:16.205728Z","shell.execute_reply.started":"2024-12-11T10:33:12.636682Z","shell.execute_reply":"2024-12-11T10:33:16.204841Z"}},"outputs":[{"name":"stdout","text":"NER Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f77370ec19bc44ecae30f0ec15d2fb39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b378bed46a41aeb5ca9b24f49ee93e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f60653c41b41dca1a997e9373cb931"}},"metadata":{}},{"name":"stdout","text":"Number of training batches (NER): 1756\nNumber of testing batches (NER): 432\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"#### 1.2.3 Defining the NER Model\n\nLoad the pre-trained BERT model for token classification.\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertForTokenClassification\n\nner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n\n# Move the model to the desired device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nner_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:34:50.179200Z","iopub.execute_input":"2024-12-11T10:34:50.179913Z","iopub.status.idle":"2024-12-11T10:34:50.532898Z","shell.execute_reply.started":"2024-12-11T10:34:50.179877Z","shell.execute_reply":"2024-12-11T10:34:50.532035Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=9, bias=True)\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"#### 1.2.4 Setting Up the Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nepochs = 3\ntotal_steps = len(train_loader_ner) * epochs\n\noptimizer_ner = AdamW(ner_model.parameters(), lr=2e-5)\n\nscheduler_ner = get_linear_schedule_with_warmup(optimizer_ner,\n                                                num_warmup_steps=0,\n                                                num_training_steps=total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:35:11.738022Z","iopub.execute_input":"2024-12-11T10:35:11.738858Z","iopub.status.idle":"2024-12-11T10:35:11.745919Z","shell.execute_reply.started":"2024-12-11T10:35:11.738823Z","shell.execute_reply":"2024-12-11T10:35:11.745210Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"#### 1.2.5 Training the NER Model","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\n# Function to calculate NER accuracy (token-level)\ndef ner_accuracy(preds, labels):\n    preds = torch.argmax(preds, dim=2)\n    valid = labels != -100\n    correct = (preds == labels) & valid\n    return correct.sum().item() / valid.sum().item()\n\n# Set the model to training mode\nner_model.train()\n\nfor epoch in range(epochs):\n    print(f'\\nEpoch {epoch + 1}/{epochs}')\n    total_loss = 0\n    total_acc = 0\n    num_batches = 0\n    progress_bar = tqdm(train_loader_ner, desc=\"Training NER\")\n    \n    for batch in progress_bar:\n        optimizer_ner.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        \n        loss.backward()\n        optimizer_ner.step()\n        scheduler_ner.step()\n        \n        total_loss += loss.item()\n        total_acc += ner_accuracy(logits, labels)\n        num_batches += 1\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': loss.item(),\n            'accuracy': total_acc / num_batches  # Avoid division by zero\n        })\n    \n    avg_loss = total_loss / len(train_loader_ner)\n    avg_acc = total_acc / len(train_loader_ner)\n    print(f'Training Loss: {avg_loss:.4f}, Training NER Accuracy: {avg_acc:.4f}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:35:32.532032Z","iopub.execute_input":"2024-12-11T10:35:32.532353Z","iopub.status.idle":"2024-12-11T10:51:39.702747Z","shell.execute_reply.started":"2024-12-11T10:35:32.532328Z","shell.execute_reply":"2024-12-11T10:51:39.701814Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training NER:   0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9b7592339e4a5f86c21798e11a7f9d"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 0.1157, Training NER Accuracy: 0.9679\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training NER:   0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e21fea3952b4cd089c5367c31261b6f"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 0.0322, Training NER Accuracy: 0.9912\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training NER:   0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf79af51bcf481db99390c42bdf8737"}},"metadata":{}},{"name":"stdout","text":"Training Loss: 0.0176, Training NER Accuracy: 0.9955\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"#### 1.2.6 Evaluating the NER Model\n\nAssess the model's performance on the test set using metrics like Precision, Recall, and F1 score.\n","metadata":{}},{"cell_type":"code","source":"import evaluate\n\ndef safe_label_mapping(label_indices, label_list):\n    mapped_labels = []\n    for idx in label_indices:\n        try:\n            mapped_labels.append(label_list[idx])\n        except IndexError:\n            # Handle unexpected label indices\n            mapped_labels.append(\"O\")\n    return mapped_labels\n\nmetric = evaluate.load(\"seqeval\")\n\n# Set ner_model to evaluation mode\nner_model.eval()\n\n# Initialize lists to store predictions and references\nall_predictions = []\nall_references = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(test_loader_ner, desc=\"Evaluating NER\")):\n        try:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n        except KeyError as e:\n            print(f\"Missing key in batch {batch_idx}: {e}\")\n            continue\n        \n        try:\n            # Forward pass\n            outputs = ner_model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            \n            # Get the predicted class by taking the argmax\n            predictions = torch.argmax(logits, dim=-1)\n        except Exception as e:\n            print(f\"Error during model inference on batch {batch_idx}: {e}\")\n            continue\n        \n        # Move tensors to CPU and convert to numpy arrays\n        predictions = predictions.cpu().numpy()\n        true_labels = labels.cpu().numpy()\n        masks = attention_mask.cpu().numpy()\n        \n        for i in range(len(input_ids)):\n            # Apply the attention mask to filter out padding tokens\n            active_indices = masks[i] == 1\n            pred_labels = predictions[i][active_indices]\n            true_label_ids = true_labels[i][active_indices]\n            \n            # Filter out labels with value -100 (ignored index)\n            valid_indices = true_label_ids != -100\n            pred_labels = pred_labels[valid_indices]\n            true_label_ids = true_label_ids[valid_indices]\n            \n            # Safely map label IDs to label names\n            pred_label_names = safe_label_mapping(pred_labels, label_list)\n            true_label_names = safe_label_mapping(true_label_ids, label_list)\n            \n            all_predictions.append(pred_label_names)\n            all_references.append(true_label_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:51:39.704204Z","iopub.execute_input":"2024-12-11T10:51:39.704482Z","iopub.status.idle":"2024-12-11T10:52:03.563930Z","shell.execute_reply.started":"2024-12-11T10:51:39.704455Z","shell.execute_reply":"2024-12-11T10:52:03.563012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9169b40d61f149c4990f954f95632e8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating NER:   0%|          | 0/432 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cedd32a6d523476d887ce7bfd947edbb"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"results = metric.compute(predictions=all_predictions, references=all_references)\n\n\n# Display evaluation results\nprint(\"\\nNER Evaluation Results:\")\nprint(f\"Precision: {results.get('overall_precision', 0):.4f}\")\nprint(f\"Recall: {results.get('overall_recall', 0):.4f}\")\nprint(f\"F1 Score: {results.get('overall_f1', 0):.4f}\")\nprint(f\"Accuracy: {results.get('overall_accuracy', 0):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:53:24.573440Z","iopub.execute_input":"2024-12-11T10:53:24.573847Z","iopub.status.idle":"2024-12-11T10:53:25.525919Z","shell.execute_reply.started":"2024-12-11T10:53:24.573814Z","shell.execute_reply":"2024-12-11T10:53:25.524836Z"}},"outputs":[{"name":"stdout","text":"\nNER Evaluation Results:\nPrecision: 0.8945\nRecall: 0.9111\nF1 Score: 0.9027\nAccuracy: 0.9804\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"*Note:* For more detailed metrics, refer to the `seqeval` documentation.\n\n---\n\n## 2. Variants and Extensions of BERT\n\nBERT's architecture has inspired numerous variants aimed at improving efficiency, scalability, and performance. We'll explore some of the most notable ones: RoBERTa, DistilBERT, ALBERT, and Domain-Specific BERT models.\n\n### 2.1 RoBERTa (Robustly Optimized BERT Pretraining Approach)\n\n**Key Enhancements:**\n\n- **More Training Data:** Trains on a larger corpus compared to BERT.\n- **Dynamic Masking:** Applies masking dynamically during training rather than using a fixed masking pattern.\n- **No Next Sentence Prediction (NSP):** Removes the NSP task, focusing solely on Masked Language Modeling (MLM).\n- **Larger Batch Sizes and Learning Rates:** Utilizes larger mini-batches and higher learning rates for more efficient training.\n\n**Benefits:**\n\n- Achieves better performance on various NLP benchmarks.\n- More robust representations due to optimized training strategies.\n\n**Implementation Example:**","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import pipeline\n\n# Load RoBERTa tokenizer and model\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base')\nroberta_model.to(device)\n\n# Example usage with a classification pipeline\nroberta_classifier = pipeline('sentiment-analysis', model=roberta_model, tokenizer=roberta_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = roberta_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:54:14.154501Z","iopub.execute_input":"2024-12-11T10:54:14.154886Z","iopub.status.idle":"2024-12-11T10:54:15.106348Z","shell.execute_reply.started":"2024-12-11T10:54:14.154852Z","shell.execute_reply":"2024-12-11T10:54:15.105372Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_1', 'score': 0.522160530090332}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_1', 'score': 0.5255693197250366}\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### 2.2 DistilBERT\n\n**Key Enhancements:**\n\n- **Model Compression:** Reduces the size of BERT by 40% while retaining 97% of its language understanding capabilities.\n- **Knowledge Distillation:** Trains a smaller model (student) to replicate the behavior of a larger model (teacher).\n\n**Benefits:**\n\n- Faster inference times.\n- Reduced computational and memory requirements.\n- Suitable for deployment in resource-constrained environments.\n\n**Implementation Example:**\n","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\n# Load DistilBERT tokenizer and model\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\ndistilbert_model.to(device)\n\n# Example usage with a classification pipeline\ndistilbert_classifier = pipeline('sentiment-analysis', model=distilbert_model, tokenizer=distilbert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = distilbert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:54:20.819629Z","iopub.execute_input":"2024-12-11T10:54:20.820282Z","iopub.status.idle":"2024-12-11T10:54:22.862317Z","shell.execute_reply.started":"2024-12-11T10:54:20.820252Z","shell.execute_reply":"2024-12-11T10:54:22.861316Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c754f836e54499c992c15faea662db1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54635ce76d744d59b6045766f4da15ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a777ba34e1b141a4a02fa6612cd40e53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df4c1ab2d2d467caf8ca84d970cbbb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e17e188f344d489d85398be2779cba"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_0', 'score': 0.5197321176528931}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_0', 'score': 0.5332385301589966}\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### 2.3 ALBERT (A Lite BERT)\n\n**Key Enhancements:**\n\n- **Parameter Sharing:** Shares parameters across layers to significantly reduce the total number of parameters.\n- **Factorized Embedding Parameterization:** Separates the size of hidden layers from the size of embeddings, allowing for smaller embedding sizes without compromising model capacity.\n\n**Benefits:**\n\n- Significantly fewer parameters compared to BERT.\n- Comparable or better performance with a reduced memory footprint.\n- Faster training and inference.\n\n**Implementation Example:**","metadata":{}},{"cell_type":"code","source":"from transformers import AlbertTokenizer, AlbertForSequenceClassification\n\n# Load ALBERT tokenizer and model\nalbert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nalbert_model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')\nalbert_model.to(device)\n\n# Example usage with a classification pipeline\nalbert_classifier = pipeline('sentiment-analysis', model=albert_model, tokenizer=albert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = albert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:54:42.417778Z","iopub.execute_input":"2024-12-11T10:54:42.418172Z","iopub.status.idle":"2024-12-11T10:54:43.801881Z","shell.execute_reply.started":"2024-12-11T10:54:42.418139Z","shell.execute_reply":"2024-12-11T10:54:43.801014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd17aa41e3e4c69933a0300f5427066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ce0a5f2c154c8eab4a3205a3df27b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb31dab5de049e09265498fa08d90b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df2702b2d1664ba190cb83916650b727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b071ec619254f5f90e6e7cd2688d846"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_1', 'score': 0.6166632175445557}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_1', 'score': 0.6308166980743408}\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### 2.4 Domain-Specific BERT Models\n\nPre-trained BERT models tailored to specific domains can outperform general-purpose BERT models on tasks within those domains.\n\n**Examples:**\n\n- **BioBERT:** Specialized for biomedical text mining tasks.\n- **SciBERT:** Designed for scientific publications.\n- **FinBERT:** Tailored for financial text analysis.\n\n**Implementation Example with BioBERT:**\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Load BioBERT tokenizer and model\nbiobert_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nbiobert_model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nbiobert_model.to(device)\n\n# Example usage with a classification pipeline (assuming a relevant task)\nbiobert_classifier = pipeline('sentiment-analysis', model=biobert_model, tokenizer=biobert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# Example biomedical sentences\nsentences = [\n    \"The patient was diagnosed with hypertension.\",\n    \"CRISPR-Cas9 is a revolutionary gene-editing tool.\"\n]\n\nresults = biobert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:55:11.316534Z","iopub.execute_input":"2024-12-11T10:55:11.316910Z","iopub.status.idle":"2024-12-11T10:55:14.660098Z","shell.execute_reply.started":"2024-12-11T10:55:11.316877Z","shell.execute_reply":"2024-12-11T10:55:14.659002Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e11a1f4d9f4ad08a796e7ee2760529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a67589878f4a80a60433ef868d5cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17242f252a4f463389b0446fabc00201"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: The patient was diagnosed with hypertension.\nSentiment: {'label': 'LABEL_1', 'score': 0.505736768245697}\n\nSentence: CRISPR-Cas9 is a revolutionary gene-editing tool.\nSentiment: {'label': 'LABEL_1', 'score': 0.5348502397537231}\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## 3. Exploring BERT Variants and Extensions\n\nBERT's architecture has inspired numerous variants aimed at improving efficiency, scalability, and performance. We'll delve into some prominent variants: RoBERTa, DistilBERT, ALBERT, and Domain-Specific BERT models.\n\n### 3.1 RoBERTa (Robustly Optimized BERT Pretraining Approach)\n\n**Key Enhancements:**\n\n- **More Training Data:** Trains on a significantly larger corpus compared to BERT.\n- **Dynamic Masking:** Applies masking dynamically during training rather than using a fixed masking pattern.\n- **No Next Sentence Prediction (NSP):** Removes the NSP task, focusing solely on Masked Language Modeling (MLM).\n- **Larger Batch Sizes and Learning Rates:** Utilizes larger mini-batches and higher learning rates for more efficient training.\n\n**Benefits:**\n\n- Achieves better performance on various NLP benchmarks.\n- More robust representations due to optimized training strategies.\n\n**Implementation Example:**\n","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load RoBERTa tokenizer and model\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base')\nroberta_model.to(device)\n\n# Example usage with a classification pipeline\nroberta_classifier = pipeline('sentiment-analysis', model=roberta_model, tokenizer=roberta_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = roberta_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:55:21.643220Z","iopub.execute_input":"2024-12-11T10:55:21.643918Z","iopub.status.idle":"2024-12-11T10:55:22.153632Z","shell.execute_reply.started":"2024-12-11T10:55:21.643883Z","shell.execute_reply":"2024-12-11T10:55:22.152696Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_1', 'score': 0.5146975517272949}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_1', 'score': 0.5187569856643677}\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### 3.2 DistilBERT\n\n**Key Enhancements:**\n\n- **Model Compression:** Reduces the size of BERT by 40% while retaining 97% of its language understanding capabilities.\n- **Knowledge Distillation:** Trains a smaller model (student) to replicate the behavior of a larger model (teacher).\n\n**Benefits:**\n\n- Faster inference times.\n- Reduced computational and memory requirements.\n- Suitable for deployment in resource-constrained environments.\n\n**Implementation Example:**\n","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\n# Load DistilBERT tokenizer and model\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\ndistilbert_model.to(device)\n\n# Example usage with a classification pipeline\ndistilbert_classifier = pipeline('sentiment-analysis', model=distilbert_model, tokenizer=distilbert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = distilbert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:55:33.813234Z","iopub.execute_input":"2024-12-11T10:55:33.813605Z","iopub.status.idle":"2024-12-11T10:55:34.160206Z","shell.execute_reply.started":"2024-12-11T10:55:33.813567Z","shell.execute_reply":"2024-12-11T10:55:34.159387Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_1', 'score': 0.5239751935005188}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_1', 'score': 0.5243285298347473}\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### 3.3 ALBERT (A Lite BERT)\n\n**Key Enhancements:**\n\n- **Parameter Sharing:** Shares parameters across layers to significantly reduce the total number of parameters.\n- **Factorized Embedding Parameterization:** Separates the size of hidden layers from the size of embeddings, allowing for smaller embedding sizes without compromising model capacity.\n\n**Benefits:**\n\n- Significantly fewer parameters compared to BERT.\n- Comparable or better performance with a reduced memory footprint.\n- Faster training and inference.\n\n**Implementation Example:**","metadata":{}},{"cell_type":"code","source":"from transformers import AlbertTokenizer, AlbertForSequenceClassification\n\n# Load ALBERT tokenizer and model\nalbert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nalbert_model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')\nalbert_model.to(device)\n\n# Example usage with a classification pipeline\nalbert_classifier = pipeline('sentiment-analysis', model=albert_model, tokenizer=albert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\nsentences = [\n    \"I absolutely loved this movie!\",\n    \"This was the worst film I have ever seen.\"\n]\n\nresults = albert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:55:40.753740Z","iopub.execute_input":"2024-12-11T10:55:40.754137Z","iopub.status.idle":"2024-12-11T10:55:41.154229Z","shell.execute_reply.started":"2024-12-11T10:55:40.754103Z","shell.execute_reply":"2024-12-11T10:55:41.153077Z"}},"outputs":[{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: I absolutely loved this movie!\nSentiment: {'label': 'LABEL_0', 'score': 0.5040313005447388}\n\nSentence: This was the worst film I have ever seen.\nSentiment: {'label': 'LABEL_1', 'score': 0.5025427341461182}\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### 3.4 Domain-Specific BERT Models\n\nPre-trained BERT models tailored to specific domains can outperform general-purpose BERT models on tasks within those domains.\n\n**Examples:**\n\n- **BioBERT:** Specialized for biomedical text mining tasks.\n- **SciBERT:** Designed for scientific publications.\n- **FinBERT:** Tailored for financial text analysis.\n\n**Implementation Example with BioBERT:**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Load BioBERT tokenizer and model\nbiobert_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nbiobert_model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nbiobert_model.to(device)\n\n# Example usage with a classification pipeline (assuming a relevant task)\nbiobert_classifier = pipeline('sentiment-analysis', model=biobert_model, tokenizer=biobert_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# Example biomedical sentences\nsentences = [\n    \"The patient was diagnosed with hypertension.\",\n    \"CRISPR-Cas9 is a revolutionary gene-editing tool.\"\n]\n\nresults = biobert_classifier(sentences)\nfor sentence, sentiment in zip(sentences, results):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:55:59.380707Z","iopub.execute_input":"2024-12-11T10:55:59.381410Z","iopub.status.idle":"2024-12-11T10:56:00.178712Z","shell.execute_reply.started":"2024-12-11T10:55:59.381374Z","shell.execute_reply":"2024-12-11T10:56:00.177826Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sentence: The patient was diagnosed with hypertension.\nSentiment: {'label': 'LABEL_1', 'score': 0.6780614852905273}\n\nSentence: CRISPR-Cas9 is a revolutionary gene-editing tool.\nSentiment: {'label': 'LABEL_1', 'score': 0.6932359337806702}\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"#### *Note:* Domain-specific models may require task-specific fine-tuning to achieve optimal performance.\n\n---\n\n## 4. Analysis and Insights\n\n### 4.1 Advantages of BERT Over Traditional RNNs\n\n- **Bidirectional Contextualization:** BERT considers both left and right context simultaneously, enabling a deeper understanding of language nuances.\n- **Pre-training with MLM and NSP:** BERT's pre-training tasks allow it to learn rich language representations that can be fine-tuned for various downstream tasks.\n- **Transfer Learning:** Fine-tuning pre-trained BERT models on specific tasks often leads to state-of-the-art performance with relatively small task-specific datasets.\n- **Handling Long-Range Dependencies:** The self-attention mechanism in BERT effectively captures dependencies between distant tokens in a sequence.\n\n### 4.2 Challenges and Considerations\n\n- **Computational Resources:** BERT models are large and require significant memory and computational power, especially during training.\n- **Fine-Tuning Sensitivity:** BERT can be sensitive to hyperparameters during fine-tuning, necessitating careful tuning for optimal performance.\n- **Interpretability:** While attention mechanisms provide some interpretability, understanding the full decision-making process of BERT remains complex.\n\n---\n\n## 5. Further Steps and Resources\n\n### 5.1 Experiment with Different Tasks\n\n- **Named Entity Recognition (NER)**\n- **Question Answering (QA)**\n- **Text Summarization**\n\n### 5.2 Explore BERT Variants\n\n- **RoBERTa:** Explore its improved training methodology.\n- **DistilBERT:** Implement a distilled version for efficiency.\n- **ALBERT:** Experiment with parameter-efficient BERT variants.\n\n### 5.3 Dive Deeper into Transformers\n\n- **Transformer-XL:** Understand its approach to handling longer sequences.\n- **GPT Series:** Explore generative capabilities using decoder-only models.\n\n### 5.4 Utilize Hugging Face Resources\n\n- **Hugging Face Models:** Explore a wide range of pre-trained models.\n- **Hugging Face Tutorials:** Engage with comprehensive tutorials for various NLP tasks.\n\n**Remember:** Mastering BERT and its variants is pivotal for advancing in modern NLP. Leveraging pre-trained models and understanding their architecture enables you to tackle complex language understanding tasks with efficiency and effectiveness.\n\n---\n\n## References\n\n- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Devlin et al.\n- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)\n- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}