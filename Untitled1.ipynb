{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431ed025-0d15-42e9-89ee-d2496d1cb314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18898224 0.18898224 0.37796447 0.37796447 0.18898224 0.18898224\n",
      "  0.75592895]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define corpus\n",
    "corpus = [\n",
    "    [\"the cat sat on the mat and the cat ate the mat\"],\n",
    "    [\"the dog sat on the mat and the dog ate the mat\"]\n",
    "]\n",
    "\n",
    "# Initialize a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the corpus into TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Show TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4925aac-f8a9-4b54-aa28-1e9e5348fd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"cat\", \"ate\", \"the\", \"mat\"]\n",
    "tf_the = tokens.count(\"the\") / len(tokens)  # 4/12 = 0.333\n",
    "tf_the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57111ac8-27ff-419c-9d8e-fcb2272d7a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Documents:\n",
      "[1, 2, 3, 4, 1, 5, 6, 1, 2, 7, 1, 5]\n",
      "[1, 8, 3, 4, 1, 5, 6, 1, 8, 7, 1, 5]\n",
      "[8, 6, 2]\n",
      "\n",
      "Vocabulary Size: 8\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def oov_tokenizer(documents):\n",
    "    word_to_index = defaultdict(lambda: len(word_to_index)+1)\n",
    "    indexed_documents = []\n",
    "\n",
    "    for document in documents:\n",
    "        indexed_doc = [word_to_index[word] for word in document]\n",
    "        indexed_documents.append(indexed_doc)\n",
    "\n",
    "    vocab_size = len(word_to_index)\n",
    "\n",
    "    return indexed_documents, vocab_size\n",
    "\n",
    "# Example usage:\n",
    "documents = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"cat\", \"ate\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"dog\", \"ate\", \"the\", \"mat\"],\n",
    "    [\"dog\", \"and\", \"cat\"]\n",
    "]\n",
    "\n",
    "indexed_docs, vocab_size = oov_tokenizer(documents)\n",
    "\n",
    "print(\"Indexed Documents:\")\n",
    "for doc in indexed_docs:\n",
    "    print(doc)\n",
    "\n",
    "print(f\"\\nVocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec1586a8-eb73-4030-bc72-bc70ecf74dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 7, 8, 2, 3, 5, 2, 4, 9, 2, 3],\n",
       " [2, 6, 7, 8, 2, 3, 5, 2, 6, 9, 2, 3],\n",
       " [6, 5, 4]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(documents)\n",
    "tokenizer.texts_to_sequences(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
