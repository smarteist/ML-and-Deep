{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67cc1acae8e48474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:17.932053Z",
     "start_time": "2024-09-21T20:38:15.307128Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:18.633069Z",
     "start_time": "2024-09-21T20:38:17.936673Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)  # Remove non-alphabetic characters\n",
    "            filtered_text = re.sub(r\"\\b[a-zA-Z']\\b\", \"\", cleaned_text)  # Remove single characters\n",
    "            cleaned_text = re.sub(r\"\\n\", \" \", filtered_text)  # Replace new lines with space\n",
    "            cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # Replace multiple spaces with single space\n",
    "            docs.append(cleaned_text)\n",
    "    return docs\n",
    "\n",
    "neg_dir = '../NLP/TEXT/txt_sentoken/neg/'\n",
    "pos_dir = '../NLP/TEXT/txt_sentoken/pos/'\n",
    "\n",
    "# Load negative and positive documents\n",
    "neg_docs = load_docs(neg_dir)\n",
    "pos_docs = load_docs(pos_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d28d4887d438f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:18.775771Z",
     "start_time": "2024-09-21T20:38:18.769600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the line of duty is the critically praised series of television movies dealing with the reallife incidents that claimed lives of law enforcement officers in usa the twilight murders another one from the series is dealing with the case of gordon kahl played by rod steiger old farmer from north dakota who would rather spend year in prison than pay taxes to the despised government after being released he still refuses to pay taxes and the warrant is issued for his arrest when the marshals come to arrest him it turns out that kahl isn alone many poor farmers in rural northwest share his extremist antigovernment beliefs and the routine operation turns into shootout that would leave federal officers that that brings fbi on the scene and agent mayberly michael gross is supervising the manhunt however his efforts seem fruitless since kahl still has many supporters some of them even in the local law enforcement after betrayed thriller by costa gavras hollywood mostly ignored the disturbing trends of rising rightwing extremism in the american northwest and that remained so until oklahoma city bombing when media hype brought rightwingers back into spotlight until that time only the television movies like this one bothered to pay attention to that phenomenon unfortunately the twilight murders is still routine and formulaic television film that uses sensationalist real life content in order to cover lack of originality in the script sometimes like many american tv movies it creates drama where drama shouldn be on the other hand film is steadily directed by dick lowry and rod steiger does good job potraying gordon kahl on the other hand michael gross as his pursuer is quite wooden and his interaction with some of the actors is terrible however film does deal with potentially thoughtprovoking social and political issues so hour and half spent in front of the screen shouldn be the total waste of time '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c55022cedfbde46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:18.863288Z",
     "start_time": "2024-09-21T20:38:18.858519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_docs), len(pos_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5648e7a34cbdd71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:18.928944Z",
     "start_time": "2024-09-21T20:38:18.921178Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words = pos_docs + neg_docs\n",
    "labels = [1] * len(pos_docs) + [0] * len(neg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795b9a024dff984c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:18.995108Z",
     "start_time": "2024-09-21T20:38:18.983519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(all_words, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "print(len(train_y),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04ca16feffb5d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:38:23.068746Z",
     "start_time": "2024-09-21T20:38:19.041321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 1: Tokenize the sentences using nltk\n",
    "train_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in train_x]\n",
    "test_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in test_x]\n",
    "\n",
    "# Step 2: Build a simple vocabulary from the tokenized data\n",
    "def build_vocab(tokenized_sentences, min_freq=1):\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    # Only keep words that appear more than `min_freq` times\n",
    "    vocab = {word: idx + 1 for idx, (word, freq) in enumerate(word_freq.items()) if freq >= min_freq}\n",
    "    vocab[\"<PAD>\"] = 0  # Padding token\n",
    "    return vocab\n",
    "\n",
    "# Combine training and test data to build the vocabulary\n",
    "combined_data = train_x_tokenized + test_x_tokenized\n",
    "vocab = build_vocab(combined_data)\n",
    "\n",
    "# Step 3: Convert the tokenized sentences to numerical format using the vocabulary\n",
    "def tokenize(sentences, vocab):\n",
    "    return [[vocab.get(word, vocab[\"<PAD>\"]) for word in sentence] for sentence in sentences]\n",
    "\n",
    "train_x_tokenized_indices = tokenize(train_x_tokenized, vocab)\n",
    "test_x_tokenized_indices = tokenize(test_x_tokenized, vocab)\n",
    "\n",
    "# Step 4: Pad the tokenized sequences to a fixed length (max_len)\n",
    "def pad_sequences(tokenized_sentences, max_len):\n",
    "    return [sentence[:max_len] + [0] * (max_len - len(sentence)) if len(sentence) < max_len else sentence[:max_len] for sentence in tokenized_sentences]\n",
    "\n",
    "max_len = 5  # Set max sequence length\n",
    "train_x_padded = pad_sequences(train_x_tokenized_indices, max_len)\n",
    "test_x_padded = pad_sequences(test_x_tokenized_indices, max_len)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x_tensor = torch.tensor(train_x_padded, dtype=torch.long)\n",
    "test_x_tensor = torch.tensor(test_x_padded, dtype=torch.long)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23882471ff866585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T20:40:55.358143Z",
     "start_time": "2024-09-21T20:38:23.111473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 0.6947\n",
      "Epoch [2/6], Loss: 0.6615\n",
      "Epoch [3/6], Loss: 0.6119\n",
      "Epoch [4/6], Loss: 0.5154\n",
      "Epoch [5/6], Loss: 0.3997\n",
      "Epoch [6/6], Loss: 0.3206\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, max_len):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1d = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * ((max_len - 4 + 1) // 2), 10)\n",
    "        self.fc2 = nn.Linear(10, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, embed_dim, max_len)\n",
    "        x = F.relu(self.conv1d(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)  # Use the actual vocabulary size\n",
    "embed_dim = 100\n",
    "num_classes = 1  # Binary classification (0 or 1)\n",
    "max_len = 5  # Example max sequence length (make sure it matches your padded sequence length)\n",
    "\n",
    "# Initialize the model\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes, max_len)\n",
    "\n",
    "# Training steps\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=4):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y.float())  # Squeeze output for binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
