{
 "cells": [
  {
   "cell_type": "code",
   "id": "67cc1acae8e48474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:02.709886Z",
     "start_time": "2024-09-26T16:28:59.921976Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:03.445324Z",
     "start_time": "2024-09-26T16:29:02.714648Z"
    }
   },
   "source": [
    "def load_docs(directory):\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)  # Remove non-alphabetic characters\n",
    "            filtered_text = re.sub(r\"\\b[a-zA-Z']\\b\", \"\", cleaned_text)  # Remove single characters\n",
    "            cleaned_text = re.sub(r\"\\n\", \" \", filtered_text)  # Replace new lines with space\n",
    "            cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # Replace multiple spaces with single space\n",
    "            docs.append(cleaned_text)\n",
    "    return docs\n",
    "\n",
    "neg_dir = '../NLP/TEXT/txt_sentoken/neg/'\n",
    "pos_dir = '../NLP/TEXT/txt_sentoken/pos/'\n",
    "\n",
    "# Load negative and positive documents\n",
    "neg_docs = load_docs(neg_dir)\n",
    "pos_docs = load_docs(pos_dir)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7c55022cedfbde46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:03.587493Z",
     "start_time": "2024-09-26T16:29:03.584128Z"
    }
   },
   "source": [
    "print(len(neg_docs), len(pos_docs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5648e7a34cbdd71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:03.645577Z",
     "start_time": "2024-09-26T16:29:03.642105Z"
    }
   },
   "source": [
    "all_words = pos_docs + neg_docs\n",
    "labels = [1] * len(pos_docs) + [0] * len(neg_docs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "795b9a024dff984c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:03.704304Z",
     "start_time": "2024-09-26T16:29:03.696488Z"
    }
   },
   "source": [
    "# Split the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(all_words, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "print(len(train_y),len(test_y))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:03.833169Z",
     "start_time": "2024-09-26T16:29:03.821486Z"
    }
   },
   "cell_type": "code",
   "source": "train_x[0]",
   "id": "461ee190844ac804",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"denzel washington is among the many actors this holiday season to give very impressive milestone performances this perhaps may be washington very best to date he joins matt damon and jim carrey in having the talent this season to become his character copying mannerisms and personal styles perfectly his very believable performance makes the long and uninteresting the hurricane much better film than it actually is when his rubin hurricane carter feels pain and is tormented by racism he creates for the audience great sense of sympathy disgust and maybe even some embarrassment from the fact that this is our very own country that is breaking laws to support silly prejudices the horrific true events carter suffered from all of his life are slightly more disturbing and terrifying because they only took place few decades ago the powerful expressions of denzel washington alone as his tortured carter sum up the troubles of racism in america history he is that amazing the problem with the hurricane is that when compared to washington all of the other actors are not nearly as memorable in their roles mainly because the script focuses so much on carter that it leaves little interest or room for the other actors this is without doubt washington movie but in order for any film to be completely satisfying it must utilize everyone in the cast to their highest potential not just the stars denzel washington is the only reason to see this film for his inspirational and very depressing tour de force performance as rubin carter we see black man whose existence has been punching bag for the white american court system from the age of eleven to twenty one he was sent to juvenile prison for crime he did not commit then after successful marriage and boxing career he was harshly imprisoned again for life after being framed for triple murder carter second term in prison seemed almost inevitable when faced off against jealous powerful white men who refused to let him succeed from these past experiences in jail carter realized that he must make his case heard for all of america after several court appeals an autobiography and many public marches for his freedom carter remained locked in prison it was not until young black boy from brooklyn being educated in toronto read carter book in the ' that serious battle for freedom took place the young boy and his three canadian guardians decide to dedicate all of their time to the case until carter is released the greatest scenes in the film are when carter begins to lose his mind after spending three months in isolation washington really shows the breakdown of carter when after swearing he would never go to jail again he is unfairly framed at the height of his innocent career the devastation in washington eyes show man who has struggled to do the right thing his whole life only to be punished for it in the hole carter attempts to maintain emotional balance but he cannot his anger is heightened to point of no control we see multiple carters violent angry one silent passive one and mediator we see an emotional boxing match inside carter head from this point in the hole to the end of the film this fight continues it is very apparent at times which side is winning the battle and in some scenes there is direct switch from angry to caring or the opposite when carter sees that white canadians want to help him get out of prison the angry carter at first erupts white people have been trying to hurt him his whole life this reminder leads to anger in his head however as the film continues and the canadians continue to help carter we see the calm side him he realizes that society is changing that racism is decreasing and now carter wants to leave jail immediately and become part of this world very rarely have seen such determination to perfect his role as washington has done here if only the other actors had this same determination dan hedaya ridiculously plays the man who sends carter to prison twice by forcing witnesses to lie and switching evidence around it felt like his character was taken straight out of scooby doo episode he would have gotten away with it if it wasn for those annoying little kids the kids in this film are the three canadians who are terribly underused making their relationship with carter underdeveloped and unbelievable the lawyers who help release carter also are underused as is clancy brown as kind prison guard the complete opposite of his character in shawshank redemption there is great amount of talent in this supporting cast but it just never appears hedaya was superb in clueless same for david paymer in get shorty harris yulin in ghostbusters and liev schreiber in the daytrippers it is mystery how this many actors signed on to such shallow roles on the other hand it is absolutely no mystery why denzel washington agreed to play rubin hurricane carter this role is about as far off from shallow as you can get \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:04.148589Z",
     "start_time": "2024-09-26T16:29:03.997643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Extract the text and labels\n",
    "all_words = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(all_words, labels, test_size=0.2, stratify=labels)"
   ],
   "id": "7342cb02454f2b71",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:05.010450Z",
     "start_time": "2024-09-26T16:29:04.162787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')"
   ],
   "id": "f63a6ceaf9fc7f27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.324833Z",
     "start_time": "2024-09-26T16:29:05.185922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Tokenize the sentences using nltk\n",
    "train_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in train_x]\n",
    "test_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in test_x]"
   ],
   "id": "cd700c301e24a8f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.459556Z",
     "start_time": "2024-09-26T16:29:08.346883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokenized sentences\n",
    "train_x_filtered = [[word for word in sentence if word not in stop_words] for sentence in train_x_tokenized]\n",
    "test_x_filtered = [[word for word in sentence if word not in stop_words] for sentence in test_x_tokenized]"
   ],
   "id": "d6c363756de030cf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.605297Z",
     "start_time": "2024-09-26T16:29:08.472092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Build a simple vocabulary from the tokenized data\n",
    "def build_vocab(tokenized_sentences, min_freq=1):\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    # Only keep words that appear more than `min_freq` times\n",
    "    vocab = {word: idx + 1 for idx, (word, freq) in enumerate(word_freq.items()) if freq >= min_freq}\n",
    "    vocab[\"<PAD>\"] = 0  # Padding token\n",
    "    return vocab\n",
    "\n",
    "# Combine training and test data to build the vocabulary\n",
    "combined_data = train_x_filtered + test_x_filtered\n",
    "vocab = build_vocab(combined_data)"
   ],
   "id": "f70ceb90e84de214",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.647534Z",
     "start_time": "2024-09-26T16:29:08.644105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"hello\" , vocab[\"hello\"])\n",
    "print(\"world\", vocab[\"world\"])"
   ],
   "id": "1ac10d46e923df5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 23095\n",
      "world 368\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.872338Z",
     "start_time": "2024-09-26T16:29:08.747554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Convert the tokenized sentences to numerical format using the vocabulary\n",
    "def tokenize(sentences, vocab):\n",
    "    return [[vocab.get(word, vocab[\"<PAD>\"]) for word in sentence] for sentence in sentences]\n",
    "\n",
    "train_x_filtered_indices = tokenize(train_x_filtered, vocab)\n",
    "test_x_filtered_indices = tokenize(test_x_filtered, vocab)"
   ],
   "id": "f2ba22eaf5643502",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.915936Z",
     "start_time": "2024-09-26T16:29:08.909017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Numeric form of first sentence:\n",
    "train_x_filtered_indices[0]"
   ],
   "id": "ddfa7506cfa2ba3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 3,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 4,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 4,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 2,\n",
       " 3,\n",
       " 105,\n",
       " 59,\n",
       " 106,\n",
       " 107,\n",
       " 98,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 10,\n",
       " 43,\n",
       " 96,\n",
       " 115,\n",
       " 116,\n",
       " 101,\n",
       " 117,\n",
       " 118,\n",
       " 62,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 62,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 66,\n",
       " 67,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 52,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 124,\n",
       " 98,\n",
       " 136,\n",
       " 146,\n",
       " 136,\n",
       " 2,\n",
       " 3,\n",
       " 147,\n",
       " 148,\n",
       " 124,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 4,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 127,\n",
       " 157,\n",
       " 158,\n",
       " 146,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 98,\n",
       " 78,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 66,\n",
       " 67,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 185,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 166,\n",
       " 192,\n",
       " 82,\n",
       " 83,\n",
       " 193,\n",
       " 194,\n",
       " 179,\n",
       " 195,\n",
       " 196,\n",
       " 185,\n",
       " 197,\n",
       " 179,\n",
       " 198,\n",
       " 177,\n",
       " 199,\n",
       " 200,\n",
       " 185,\n",
       " 201,\n",
       " 192,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 192,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 177,\n",
       " 185,\n",
       " 213,\n",
       " 177,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 185,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 177,\n",
       " 224,\n",
       " 200,\n",
       " 185,\n",
       " 225,\n",
       " 192,\n",
       " 206,\n",
       " 207,\n",
       " 226,\n",
       " 98,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 227,\n",
       " 34,\n",
       " 34,\n",
       " 228,\n",
       " 230,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 59,\n",
       " 231,\n",
       " 232,\n",
       " 59,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 85,\n",
       " 199,\n",
       " 200,\n",
       " 239,\n",
       " 184,\n",
       " 240,\n",
       " 202,\n",
       " 192,\n",
       " 235,\n",
       " 241,\n",
       " 235,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 196,\n",
       " 146,\n",
       " 245,\n",
       " 246,\n",
       " 177,\n",
       " 247,\n",
       " 248,\n",
       " 34,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 127,\n",
       " 255,\n",
       " 259,\n",
       " 4,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 203,\n",
       " 98,\n",
       " 269,\n",
       " 270,\n",
       " 139,\n",
       " 271,\n",
       " 256,\n",
       " 257,\n",
       " 272,\n",
       " 215,\n",
       " 177,\n",
       " 262,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 256,\n",
       " 277,\n",
       " 224,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 235,\n",
       " 281,\n",
       " 215,\n",
       " 282,\n",
       " 283,\n",
       " 255,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 150,\n",
       " 261,\n",
       " 235,\n",
       " 239,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 98,\n",
       " 291,\n",
       " 292,\n",
       " 92,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 98,\n",
       " 294,\n",
       " 301,\n",
       " 302,\n",
       " 15,\n",
       " 155,\n",
       " 98,\n",
       " 302]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Many machine learning models, especially neural networks, require inputs of a fixed size. This uniformity simplifies the architecture and ensures that the model can process each input consistently. Having fixed-length sentences is important for several reasons when vectorizing text data for machine learning:\n",
    "\n",
    "1. **Batch Processing**: Fixed-length sentences allow for efficient batch processing. When training models, it's common to process multiple samples simultaneously (in batches). Having inputs of the same length makes it easier to stack them into a single tensor, which can be processed in parallel.\n",
    "\n",
    "2. **Memory Management**: Fixed-length inputs help in managing memory more effectively. Variable-length inputs would require dynamic memory allocation, which can be inefficient and slow.\n",
    "\n",
    "3. **Padding and Truncation**: To achieve fixed-length sentences, we use padding (adding special tokens like `\"<PAD>\"`) or truncation (cutting off longer sentences). Padding ensures that shorter sentences reach the required length, while truncation ensures that longer sentences do not exceed it.\n",
    "\n",
    "4. **Consistency in Representation**: Fixed-length vectors ensure that each sentence is represented consistently, making it easier for the model to learn patterns and relationships within the data.\n"
   ],
   "id": "8cc1956f0e760ba1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:08.998437Z",
     "start_time": "2024-09-26T16:29:08.959251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Pad the tokenized sequences to a fixed length (max_len)\n",
    "def pad_sequences(tokenized_sentences, max_len):\n",
    "    return [sentence[:max_len] + [0] * (max_len - len(sentence)) if len(sentence) < max_len else sentence[:max_len] for sentence in tokenized_sentences]\n",
    "\n",
    "pad_len = 50  # Set max sequence length\n",
    "train_x_padded = pad_sequences(train_x_filtered_indices, pad_len)\n",
    "test_x_padded = pad_sequences(test_x_filtered_indices, pad_len)"
   ],
   "id": "24fb7cc7921a9367",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:09.048212Z",
     "start_time": "2024-09-26T16:29:09.042731Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_padded[2]",
   "id": "77c016457e866063",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 505,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 98,\n",
       " 509,\n",
       " 510,\n",
       " 98,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 518,\n",
       " 182,\n",
       " 221,\n",
       " 125,\n",
       " 523,\n",
       " 505,\n",
       " 511,\n",
       " 98,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 176,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 176,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "23882471ff866585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:29:09.109825Z",
     "start_time": "2024-09-26T16:29:09.102336Z"
    }
   },
   "source": [
    "# Define the model\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, max_len):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1d = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * ((max_len - 4 + 1) // 2), 10)\n",
    "        self.fc2 = nn.Linear(10, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, embed_dim, max_len)\n",
    "        x = F.relu(self.conv1d(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:42:38.584551Z",
     "start_time": "2024-09-26T16:42:12.787023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(vocab)  # Use the actual vocabulary size\n",
    "embed_dim = 100\n",
    "num_classes = 1  # Binary classification (0 or 1)\n",
    "max_len = pad_len\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes, max_len)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x_tensor = torch.tensor(train_x_padded, dtype=torch.long)\n",
    "test_x_tensor = torch.tensor(test_x_padded, dtype=torch.long)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Training steps\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to the same device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y.float())  # Squeeze output for binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions and labels for accuracy calculation\n",
    "            preds = (outputs.squeeze() > 0.5).float()  # Assuming binary classification\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=12)\n"
   ],
   "id": "9015eb426df75b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 0.6956, Accuracy: 0.5006\n",
      "Epoch [2/12], Loss: 0.6940, Accuracy: 0.5038\n",
      "Epoch [3/12], Loss: 0.6915, Accuracy: 0.5337\n",
      "Epoch [4/12], Loss: 0.6364, Accuracy: 0.6631\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 64\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrunning_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 64\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[20], line 51\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[1;32m     49\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     50\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 51\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Collect predictions and labels for accuracy calculation\u001B[39;00m\n\u001B[1;32m     54\u001B[0m preds \u001B[38;5;241m=\u001B[39m (outputs\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()  \u001B[38;5;66;03m# Assuming binary classification\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:30:16.573627Z",
     "start_time": "2024-09-26T16:30:16.358654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation steps\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to the same device\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y.float())\n",
    "            running_loss += loss.item()\n",
    "            preds = (outputs.squeeze() > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f'Test Loss: {running_loss/len(test_loader):.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    \n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'pos_neg.pt')"
   ],
   "id": "512e3600c0089d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1620\n",
      "Accuracy: 0.5225\n",
      "Precision: 0.5315\n",
      "Recall: 0.3800\n",
      "F1 Score: 0.4431\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:30:16.599809Z",
     "start_time": "2024-09-26T16:30:16.597512Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d68e8938dfc90510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:30:16.647919Z",
     "start_time": "2024-09-26T16:30:16.644869Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5ae264b14e73de7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
