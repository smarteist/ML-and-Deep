{
 "cells": [
  {
   "cell_type": "code",
   "id": "67cc1acae8e48474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:11.337488Z",
     "start_time": "2024-09-22T18:10:08.960467Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:12.158010Z",
     "start_time": "2024-09-22T18:10:11.461581Z"
    }
   },
   "source": [
    "def load_docs(directory):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)  # Remove non-alphabetic characters\n",
    "            filtered_text = re.sub(r\"\\b[a-zA-Z']\\b\", \"\", cleaned_text)  # Remove single characters\n",
    "            cleaned_text = re.sub(r\"\\n\", \" \", filtered_text)  # Replace new lines with space\n",
    "            cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # Replace multiple spaces with single space\n",
    "            docs.append(cleaned_text)\n",
    "    return docs\n",
    "\n",
    "neg_dir = '../NLP/TEXT/txt_sentoken/neg/'\n",
    "pos_dir = '../NLP/TEXT/txt_sentoken/pos/'\n",
    "\n",
    "# Load negative and positive documents\n",
    "neg_docs = load_docs(neg_dir)\n",
    "pos_docs = load_docs(pos_dir)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7c55022cedfbde46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:12.171289Z",
     "start_time": "2024-09-22T18:10:12.166899Z"
    }
   },
   "source": [
    "print(len(neg_docs), len(pos_docs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5648e7a34cbdd71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:12.224783Z",
     "start_time": "2024-09-22T18:10:12.220598Z"
    }
   },
   "source": [
    "all_words = pos_docs + neg_docs\n",
    "labels = [1] * len(pos_docs) + [0] * len(neg_docs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "795b9a024dff984c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:12.284289Z",
     "start_time": "2024-09-22T18:10:12.276528Z"
    }
   },
   "source": [
    "# Split the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(all_words, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "print(len(train_y),len(test_y))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:12.342749Z",
     "start_time": "2024-09-22T18:10:12.334526Z"
    }
   },
   "cell_type": "code",
   "source": "train_x[0]",
   "id": "461ee190844ac804",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ingredients starving artist lusting after beautiful woman from his childhood synopsis great expectations is modernized version of charles dickens novel in the original charles dickens classic an orphan boy named pip learns about life through his friendship with an escaped convict his relationship with bitter old lady named miss havisham and his hopeless lifetime infatuation with havisham snobbish adopted daughter estella the gist of the story is that an anonymous benefactor sends pip to london for pip to become an upper class gentleman pip leaves for london with great expectations to be groomed into gentleman so that he can one day be classy enough to marry estella but life has way of complicating things pip becomes an arrogant until he learns just who his secret benefactor is in this new version of great expectations poor florida lad named finn ethan hawke with talent for drawing has an early memory of helping an escaped convict robert de niro but soon the focus of finn life becomes romance as he develops lifelong infatuation with estella gwyneth paltrow an upper crust girl with some psychological and emotional issues due to being raised in dysfunctional environment estella was raised in wealth by ms dinsmoor anne bancroft an eccentric aged and embittered manhating single aunt whose groom left her abandoned at the altar on the day of her marriage this causes estella to 'fear the daylight' of relationships but finn thinks it just case of snobbery when finn becomes young man an anonymous benefactor sets finn up in new york with the connections and publicity to be groomed into famous artist finn leaves florida with great expectations to be successful artist so that some day he might become classy enough to marry the snobbish estella what will happen to all of finn great expectations opinion if youre looking for romance with happy ending look no further youve found one here ethan hawke is romantic lead he romanced winona ryder in reality bites he romanced julie delpy in before sunrise he romanced uma thurman in gattaca now in great expectations he gets to chase gwyneth paltrow railthin blonde hawke plays lovestruck earnest and unchanged finn throughout the film meanwhile paltrow portrays estella as alternating between infuriating tease and ice princess estella whole purpose is to get men to fall in love and then hurt them rather than become intimate anne bancroft goes little overboard as the deranged and pessimistic ms dinsmoor while robert de niro carries off the convict part suitably the difference between the classic dickens novel and this adaptation is that the dickens novel is ripe with deep and timeless themes about class struggle and love whereas great expectations is more artsy less complicated story about artist dealing with lifetime tease \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:13.677095Z",
     "start_time": "2024-09-22T18:10:12.413861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')"
   ],
   "id": "f63a6ceaf9fc7f27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.363890Z",
     "start_time": "2024-09-22T18:10:13.701727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Tokenize the sentences using nltk\n",
    "train_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in train_x]\n",
    "test_x_tokenized = [nltk.word_tokenize(sentence.lower()) for sentence in test_x]"
   ],
   "id": "cd700c301e24a8f3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.386021Z",
     "start_time": "2024-09-22T18:10:16.375450Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_tokenized[0]",
   "id": "d6c363756de030cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ingredients',\n",
       " 'starving',\n",
       " 'artist',\n",
       " 'lusting',\n",
       " 'after',\n",
       " 'beautiful',\n",
       " 'woman',\n",
       " 'from',\n",
       " 'his',\n",
       " 'childhood',\n",
       " 'synopsis',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'is',\n",
       " 'modernized',\n",
       " 'version',\n",
       " 'of',\n",
       " 'charles',\n",
       " 'dickens',\n",
       " 'novel',\n",
       " 'in',\n",
       " 'the',\n",
       " 'original',\n",
       " 'charles',\n",
       " 'dickens',\n",
       " 'classic',\n",
       " 'an',\n",
       " 'orphan',\n",
       " 'boy',\n",
       " 'named',\n",
       " 'pip',\n",
       " 'learns',\n",
       " 'about',\n",
       " 'life',\n",
       " 'through',\n",
       " 'his',\n",
       " 'friendship',\n",
       " 'with',\n",
       " 'an',\n",
       " 'escaped',\n",
       " 'convict',\n",
       " 'his',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'bitter',\n",
       " 'old',\n",
       " 'lady',\n",
       " 'named',\n",
       " 'miss',\n",
       " 'havisham',\n",
       " 'and',\n",
       " 'his',\n",
       " 'hopeless',\n",
       " 'lifetime',\n",
       " 'infatuation',\n",
       " 'with',\n",
       " 'havisham',\n",
       " 'snobbish',\n",
       " 'adopted',\n",
       " 'daughter',\n",
       " 'estella',\n",
       " 'the',\n",
       " 'gist',\n",
       " 'of',\n",
       " 'the',\n",
       " 'story',\n",
       " 'is',\n",
       " 'that',\n",
       " 'an',\n",
       " 'anonymous',\n",
       " 'benefactor',\n",
       " 'sends',\n",
       " 'pip',\n",
       " 'to',\n",
       " 'london',\n",
       " 'for',\n",
       " 'pip',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'class',\n",
       " 'gentleman',\n",
       " 'pip',\n",
       " 'leaves',\n",
       " 'for',\n",
       " 'london',\n",
       " 'with',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'to',\n",
       " 'be',\n",
       " 'groomed',\n",
       " 'into',\n",
       " 'gentleman',\n",
       " 'so',\n",
       " 'that',\n",
       " 'he',\n",
       " 'can',\n",
       " 'one',\n",
       " 'day',\n",
       " 'be',\n",
       " 'classy',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'marry',\n",
       " 'estella',\n",
       " 'but',\n",
       " 'life',\n",
       " 'has',\n",
       " 'way',\n",
       " 'of',\n",
       " 'complicating',\n",
       " 'things',\n",
       " 'pip',\n",
       " 'becomes',\n",
       " 'an',\n",
       " 'arrogant',\n",
       " 'until',\n",
       " 'he',\n",
       " 'learns',\n",
       " 'just',\n",
       " 'who',\n",
       " 'his',\n",
       " 'secret',\n",
       " 'benefactor',\n",
       " 'is',\n",
       " 'in',\n",
       " 'this',\n",
       " 'new',\n",
       " 'version',\n",
       " 'of',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'poor',\n",
       " 'florida',\n",
       " 'lad',\n",
       " 'named',\n",
       " 'finn',\n",
       " 'ethan',\n",
       " 'hawke',\n",
       " 'with',\n",
       " 'talent',\n",
       " 'for',\n",
       " 'drawing',\n",
       " 'has',\n",
       " 'an',\n",
       " 'early',\n",
       " 'memory',\n",
       " 'of',\n",
       " 'helping',\n",
       " 'an',\n",
       " 'escaped',\n",
       " 'convict',\n",
       " 'robert',\n",
       " 'de',\n",
       " 'niro',\n",
       " 'but',\n",
       " 'soon',\n",
       " 'the',\n",
       " 'focus',\n",
       " 'of',\n",
       " 'finn',\n",
       " 'life',\n",
       " 'becomes',\n",
       " 'romance',\n",
       " 'as',\n",
       " 'he',\n",
       " 'develops',\n",
       " 'lifelong',\n",
       " 'infatuation',\n",
       " 'with',\n",
       " 'estella',\n",
       " 'gwyneth',\n",
       " 'paltrow',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'crust',\n",
       " 'girl',\n",
       " 'with',\n",
       " 'some',\n",
       " 'psychological',\n",
       " 'and',\n",
       " 'emotional',\n",
       " 'issues',\n",
       " 'due',\n",
       " 'to',\n",
       " 'being',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'dysfunctional',\n",
       " 'environment',\n",
       " 'estella',\n",
       " 'was',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'wealth',\n",
       " 'by',\n",
       " 'ms',\n",
       " 'dinsmoor',\n",
       " 'anne',\n",
       " 'bancroft',\n",
       " 'an',\n",
       " 'eccentric',\n",
       " 'aged',\n",
       " 'and',\n",
       " 'embittered',\n",
       " 'manhating',\n",
       " 'single',\n",
       " 'aunt',\n",
       " 'whose',\n",
       " 'groom',\n",
       " 'left',\n",
       " 'her',\n",
       " 'abandoned',\n",
       " 'at',\n",
       " 'the',\n",
       " 'altar',\n",
       " 'on',\n",
       " 'the',\n",
       " 'day',\n",
       " 'of',\n",
       " 'her',\n",
       " 'marriage',\n",
       " 'this',\n",
       " 'causes',\n",
       " 'estella',\n",
       " 'to',\n",
       " \"'fear\",\n",
       " 'the',\n",
       " 'daylight',\n",
       " \"'\",\n",
       " 'of',\n",
       " 'relationships',\n",
       " 'but',\n",
       " 'finn',\n",
       " 'thinks',\n",
       " 'it',\n",
       " 'just',\n",
       " 'case',\n",
       " 'of',\n",
       " 'snobbery',\n",
       " 'when',\n",
       " 'finn',\n",
       " 'becomes',\n",
       " 'young',\n",
       " 'man',\n",
       " 'an',\n",
       " 'anonymous',\n",
       " 'benefactor',\n",
       " 'sets',\n",
       " 'finn',\n",
       " 'up',\n",
       " 'in',\n",
       " 'new',\n",
       " 'york',\n",
       " 'with',\n",
       " 'the',\n",
       " 'connections',\n",
       " 'and',\n",
       " 'publicity',\n",
       " 'to',\n",
       " 'be',\n",
       " 'groomed',\n",
       " 'into',\n",
       " 'famous',\n",
       " 'artist',\n",
       " 'finn',\n",
       " 'leaves',\n",
       " 'florida',\n",
       " 'with',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'to',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'artist',\n",
       " 'so',\n",
       " 'that',\n",
       " 'some',\n",
       " 'day',\n",
       " 'he',\n",
       " 'might',\n",
       " 'become',\n",
       " 'classy',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'marry',\n",
       " 'the',\n",
       " 'snobbish',\n",
       " 'estella',\n",
       " 'what',\n",
       " 'will',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'finn',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'opinion',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'romance',\n",
       " 'with',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'look',\n",
       " 'no',\n",
       " 'further',\n",
       " 'youve',\n",
       " 'found',\n",
       " 'one',\n",
       " 'here',\n",
       " 'ethan',\n",
       " 'hawke',\n",
       " 'is',\n",
       " 'romantic',\n",
       " 'lead',\n",
       " 'he',\n",
       " 'romanced',\n",
       " 'winona',\n",
       " 'ryder',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'bites',\n",
       " 'he',\n",
       " 'romanced',\n",
       " 'julie',\n",
       " 'delpy',\n",
       " 'in',\n",
       " 'before',\n",
       " 'sunrise',\n",
       " 'he',\n",
       " 'romanced',\n",
       " 'uma',\n",
       " 'thurman',\n",
       " 'in',\n",
       " 'gattaca',\n",
       " 'now',\n",
       " 'in',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'he',\n",
       " 'gets',\n",
       " 'to',\n",
       " 'chase',\n",
       " 'gwyneth',\n",
       " 'paltrow',\n",
       " 'railthin',\n",
       " 'blonde',\n",
       " 'hawke',\n",
       " 'plays',\n",
       " 'lovestruck',\n",
       " 'earnest',\n",
       " 'and',\n",
       " 'unchanged',\n",
       " 'finn',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'meanwhile',\n",
       " 'paltrow',\n",
       " 'portrays',\n",
       " 'estella',\n",
       " 'as',\n",
       " 'alternating',\n",
       " 'between',\n",
       " 'infuriating',\n",
       " 'tease',\n",
       " 'and',\n",
       " 'ice',\n",
       " 'princess',\n",
       " 'estella',\n",
       " 'whole',\n",
       " 'purpose',\n",
       " 'is',\n",
       " 'to',\n",
       " 'get',\n",
       " 'men',\n",
       " 'to',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'love',\n",
       " 'and',\n",
       " 'then',\n",
       " 'hurt',\n",
       " 'them',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'become',\n",
       " 'intimate',\n",
       " 'anne',\n",
       " 'bancroft',\n",
       " 'goes',\n",
       " 'little',\n",
       " 'overboard',\n",
       " 'as',\n",
       " 'the',\n",
       " 'deranged',\n",
       " 'and',\n",
       " 'pessimistic',\n",
       " 'ms',\n",
       " 'dinsmoor',\n",
       " 'while',\n",
       " 'robert',\n",
       " 'de',\n",
       " 'niro',\n",
       " 'carries',\n",
       " 'off',\n",
       " 'the',\n",
       " 'convict',\n",
       " 'part',\n",
       " 'suitably',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classic',\n",
       " 'dickens',\n",
       " 'novel',\n",
       " 'and',\n",
       " 'this',\n",
       " 'adaptation',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'dickens',\n",
       " 'novel',\n",
       " 'is',\n",
       " 'ripe',\n",
       " 'with',\n",
       " 'deep',\n",
       " 'and',\n",
       " 'timeless',\n",
       " 'themes',\n",
       " 'about',\n",
       " 'class',\n",
       " 'struggle',\n",
       " 'and',\n",
       " 'love',\n",
       " 'whereas',\n",
       " 'great',\n",
       " 'expectations',\n",
       " 'is',\n",
       " 'more',\n",
       " 'artsy',\n",
       " 'less',\n",
       " 'complicated',\n",
       " 'story',\n",
       " 'about',\n",
       " 'artist',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'lifetime',\n",
       " 'tease']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.666247Z",
     "start_time": "2024-09-22T18:10:16.427233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Build a simple vocabulary from the tokenized data\n",
    "def build_vocab(tokenized_sentences, min_freq=1):\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    # Only keep words that appear more than `min_freq` times\n",
    "    vocab = {word: idx + 1 for idx, (word, freq) in enumerate(word_freq.items()) if freq >= min_freq}\n",
    "    vocab[\"<PAD>\"] = 0  # Padding token\n",
    "    return vocab\n",
    "\n",
    "# Combine training and test data to build the vocabulary\n",
    "combined_data = train_x_tokenized + test_x_tokenized\n",
    "vocab = build_vocab(combined_data)"
   ],
   "id": "f70ceb90e84de214",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.703782Z",
     "start_time": "2024-09-22T18:10:16.701007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"hello\" , vocab[\"hello\"])\n",
    "print(\"world\", vocab[\"world\"])"
   ],
   "id": "1ac10d46e923df5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 10841\n",
      "world 1529\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.919621Z",
     "start_time": "2024-09-22T18:10:16.747864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Convert the tokenized sentences to numerical format using the vocabulary\n",
    "def tokenize(sentences, vocab):\n",
    "    return [[vocab.get(word, vocab[\"<PAD>\"]) for word in sentence] for sentence in sentences]\n",
    "\n",
    "train_x_tokenized_indices = tokenize(train_x_tokenized, vocab)\n",
    "test_x_tokenized_indices = tokenize(test_x_tokenized, vocab)"
   ],
   "id": "f2ba22eaf5643502",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:16.959944Z",
     "start_time": "2024-09-22T18:10:16.954079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Numeric form of first sentence:\n",
    "train_x_tokenized_indices[0]"
   ],
   "id": "ddfa7506cfa2ba3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 18,\n",
       " 19,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 9,\n",
       " 34,\n",
       " 35,\n",
       " 25,\n",
       " 36,\n",
       " 37,\n",
       " 9,\n",
       " 38,\n",
       " 35,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 28,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 9,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 35,\n",
       " 43,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 22,\n",
       " 52,\n",
       " 17,\n",
       " 22,\n",
       " 53,\n",
       " 14,\n",
       " 54,\n",
       " 25,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 29,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 29,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 29,\n",
       " 65,\n",
       " 60,\n",
       " 59,\n",
       " 35,\n",
       " 12,\n",
       " 13,\n",
       " 58,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 64,\n",
       " 69,\n",
       " 54,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 66,\n",
       " 74,\n",
       " 75,\n",
       " 58,\n",
       " 76,\n",
       " 51,\n",
       " 77,\n",
       " 32,\n",
       " 78,\n",
       " 79,\n",
       " 17,\n",
       " 80,\n",
       " 81,\n",
       " 29,\n",
       " 82,\n",
       " 25,\n",
       " 83,\n",
       " 84,\n",
       " 70,\n",
       " 30,\n",
       " 85,\n",
       " 86,\n",
       " 9,\n",
       " 87,\n",
       " 56,\n",
       " 14,\n",
       " 21,\n",
       " 88,\n",
       " 89,\n",
       " 16,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 28,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 35,\n",
       " 96,\n",
       " 60,\n",
       " 97,\n",
       " 78,\n",
       " 25,\n",
       " 98,\n",
       " 99,\n",
       " 17,\n",
       " 100,\n",
       " 25,\n",
       " 36,\n",
       " 37,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 77,\n",
       " 104,\n",
       " 22,\n",
       " 105,\n",
       " 17,\n",
       " 93,\n",
       " 32,\n",
       " 82,\n",
       " 106,\n",
       " 107,\n",
       " 70,\n",
       " 108,\n",
       " 109,\n",
       " 47,\n",
       " 35,\n",
       " 51,\n",
       " 110,\n",
       " 111,\n",
       " 25,\n",
       " 62,\n",
       " 112,\n",
       " 113,\n",
       " 35,\n",
       " 114,\n",
       " 115,\n",
       " 44,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 58,\n",
       " 119,\n",
       " 120,\n",
       " 21,\n",
       " 121,\n",
       " 122,\n",
       " 51,\n",
       " 123,\n",
       " 120,\n",
       " 21,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 25,\n",
       " 130,\n",
       " 131,\n",
       " 44,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 22,\n",
       " 142,\n",
       " 143,\n",
       " 22,\n",
       " 73,\n",
       " 17,\n",
       " 139,\n",
       " 144,\n",
       " 88,\n",
       " 145,\n",
       " 51,\n",
       " 58,\n",
       " 146,\n",
       " 22,\n",
       " 147,\n",
       " 148,\n",
       " 17,\n",
       " 149,\n",
       " 77,\n",
       " 93,\n",
       " 150,\n",
       " 151,\n",
       " 85,\n",
       " 152,\n",
       " 17,\n",
       " 153,\n",
       " 154,\n",
       " 93,\n",
       " 82,\n",
       " 155,\n",
       " 156,\n",
       " 25,\n",
       " 55,\n",
       " 56,\n",
       " 157,\n",
       " 93,\n",
       " 158,\n",
       " 21,\n",
       " 89,\n",
       " 159,\n",
       " 35,\n",
       " 22,\n",
       " 160,\n",
       " 44,\n",
       " 161,\n",
       " 58,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 162,\n",
       " 3,\n",
       " 93,\n",
       " 65,\n",
       " 91,\n",
       " 35,\n",
       " 12,\n",
       " 13,\n",
       " 58,\n",
       " 66,\n",
       " 163,\n",
       " 3,\n",
       " 69,\n",
       " 54,\n",
       " 114,\n",
       " 73,\n",
       " 70,\n",
       " 164,\n",
       " 61,\n",
       " 74,\n",
       " 75,\n",
       " 58,\n",
       " 76,\n",
       " 22,\n",
       " 48,\n",
       " 51,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 58,\n",
       " 168,\n",
       " 17,\n",
       " 93,\n",
       " 12,\n",
       " 13,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 60,\n",
       " 106,\n",
       " 35,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 72,\n",
       " 180,\n",
       " 94,\n",
       " 95,\n",
       " 14,\n",
       " 181,\n",
       " 182,\n",
       " 70,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 21,\n",
       " 186,\n",
       " 187,\n",
       " 70,\n",
       " 183,\n",
       " 188,\n",
       " 189,\n",
       " 21,\n",
       " 190,\n",
       " 191,\n",
       " 70,\n",
       " 183,\n",
       " 192,\n",
       " 193,\n",
       " 21,\n",
       " 194,\n",
       " 195,\n",
       " 21,\n",
       " 12,\n",
       " 13,\n",
       " 70,\n",
       " 196,\n",
       " 58,\n",
       " 197,\n",
       " 110,\n",
       " 111,\n",
       " 198,\n",
       " 199,\n",
       " 95,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 44,\n",
       " 203,\n",
       " 93,\n",
       " 204,\n",
       " 22,\n",
       " 205,\n",
       " 206,\n",
       " 111,\n",
       " 207,\n",
       " 51,\n",
       " 107,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 44,\n",
       " 212,\n",
       " 213,\n",
       " 51,\n",
       " 214,\n",
       " 215,\n",
       " 14,\n",
       " 58,\n",
       " 216,\n",
       " 217,\n",
       " 58,\n",
       " 218,\n",
       " 21,\n",
       " 219,\n",
       " 44,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 61,\n",
       " 225,\n",
       " 128,\n",
       " 129,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 107,\n",
       " 22,\n",
       " 229,\n",
       " 44,\n",
       " 230,\n",
       " 126,\n",
       " 127,\n",
       " 231,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 232,\n",
       " 233,\n",
       " 22,\n",
       " 37,\n",
       " 234,\n",
       " 235,\n",
       " 22,\n",
       " 236,\n",
       " 209,\n",
       " 22,\n",
       " 24,\n",
       " 19,\n",
       " 20,\n",
       " 44,\n",
       " 88,\n",
       " 237,\n",
       " 14,\n",
       " 54,\n",
       " 22,\n",
       " 19,\n",
       " 20,\n",
       " 14,\n",
       " 238,\n",
       " 35,\n",
       " 239,\n",
       " 44,\n",
       " 240,\n",
       " 241,\n",
       " 31,\n",
       " 63,\n",
       " 242,\n",
       " 44,\n",
       " 219,\n",
       " 243,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 53,\n",
       " 31,\n",
       " 3,\n",
       " 248,\n",
       " 35,\n",
       " 46,\n",
       " 211]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:17.035221Z",
     "start_time": "2024-09-22T18:10:17.002122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Pad the tokenized sequences to a fixed length (max_len)\n",
    "def pad_sequences(tokenized_sentences, max_len):\n",
    "    return [sentence[:max_len] + [0] * (max_len - len(sentence)) if len(sentence) < max_len else sentence[:max_len] for sentence in tokenized_sentences]\n",
    "\n",
    "max_len = 50  # Set max sequence length\n",
    "train_x_padded = pad_sequences(train_x_tokenized_indices, max_len)\n",
    "test_x_padded = pad_sequences(test_x_tokenized_indices, max_len)"
   ],
   "id": "24fb7cc7921a9367",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:17.074768Z",
     "start_time": "2024-09-22T18:10:17.069606Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_padded[2]",
   "id": "77c016457e866063",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 17,\n",
       " 22,\n",
       " 401,\n",
       " 410,\n",
       " 21,\n",
       " 22,\n",
       " 411,\n",
       " 412,\n",
       " 143,\n",
       " 413,\n",
       " 412,\n",
       " 143,\n",
       " 414,\n",
       " 14,\n",
       " 360,\n",
       " 415,\n",
       " 58,\n",
       " 412,\n",
       " 143,\n",
       " 416,\n",
       " 21,\n",
       " 54,\n",
       " 151,\n",
       " 417,\n",
       " 418,\n",
       " 17,\n",
       " 419,\n",
       " 420,\n",
       " 143,\n",
       " 421,\n",
       " 148,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 44,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 394,\n",
       " 44,\n",
       " 418,\n",
       " 17,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 21]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:10:17.139136Z",
     "start_time": "2024-09-22T18:10:17.119266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_x_tensor = torch.tensor(train_x_padded, dtype=torch.long)\n",
    "test_x_tensor = torch.tensor(test_x_padded, dtype=torch.long)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ],
   "id": "c04ca16feffb5d92",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "23882471ff866585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:14:31.136644Z",
     "start_time": "2024-09-22T18:10:17.183242Z"
    }
   },
   "source": [
    "# Define the model\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, max_len):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1d = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * ((max_len - 4 + 1) // 2), 10)\n",
    "        self.fc2 = nn.Linear(10, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, embed_dim, max_len)\n",
    "        x = F.relu(self.conv1d(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)  # Use the actual vocabulary size\n",
    "embed_dim = 100\n",
    "num_classes = 1  # Binary classification (0 or 1)\n",
    "\n",
    "# Initialize the model\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes, max_len)\n",
    "\n",
    "# Training steps\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y.float())  # Squeeze output for binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=12)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 0.6980\n",
      "Epoch [2/12], Loss: 0.6695\n",
      "Epoch [3/12], Loss: 0.5330\n",
      "Epoch [4/12], Loss: 0.2668\n",
      "Epoch [5/12], Loss: 0.1560\n",
      "Epoch [6/12], Loss: 0.1229\n",
      "Epoch [7/12], Loss: 0.0950\n",
      "Epoch [8/12], Loss: 0.0648\n",
      "Epoch [9/12], Loss: 0.0567\n",
      "Epoch [10/12], Loss: 0.0624\n",
      "Epoch [11/12], Loss: 0.0483\n",
      "Epoch [12/12], Loss: 0.0491\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:15:43.136327Z",
     "start_time": "2024-09-22T18:15:43.113510Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'pos_neg.pt')",
   "id": "512e3600c0089d27",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
