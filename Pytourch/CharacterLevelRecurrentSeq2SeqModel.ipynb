{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:08:38.043172Z",
     "start_time": "2024-10-16T10:08:37.974410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import SVG, display\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n"
   ],
   "id": "4eea097acec7f9c0",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SVG, display\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, TensorDataset\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchviz\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_dot\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torchviz'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005685,
     "end_time": "2024-03-14T06:08:15.311391",
     "exception": false,
     "start_time": "2024-03-14T06:08:15.305706",
     "status": "completed"
    },
    "tags": []
   },
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we implement a basic character-level recurrent sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Normally, word-level models are more common in machine translation domain.\n",
    "\n",
    "We will start with input sentences from a English sentences and corresponding target sequences from French sentences. And we will use LSTM as an encoder turns input sequences to 2 state vectors(the last LSTM state and discard the outputs). A LSTM decoder is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. \n",
    "\n",
    "In inference mode, when we want to decode unknown input sequences, we encode the input sequence into state vectors. -Start with a target sequence of size 1(just the start-of-sequence character). - Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character - Sample the next character using these predictions(here we use argmax). And append the sampled character to the target sequence- Repeat until we generate the end-of-sequence character or we hit the character limit.\n",
    "\n",
    "\n",
    "# Loading the Dataset"
   ],
   "id": "538fa58a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:08:38.055822630Z",
     "start_time": "2024-10-16T09:52:48.364970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ],
   "id": "fe6c5b7dff6a7f40",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f6b4132d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T06:08:18.240583Z",
     "iopub.status.busy": "2024-03-14T06:08:18.240219Z",
     "iopub.status.idle": "2024-03-14T06:08:19.459426Z",
     "shell.execute_reply": "2024-03-14T06:08:19.458450Z"
    },
    "papermill": {
     "duration": 1.228079,
     "end_time": "2024-03-14T06:08:19.461897",
     "exception": false,
     "start_time": "2024-03-14T06:08:18.233818",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-16T10:08:38.056352845Z",
     "start_time": "2024-10-16T09:52:59.487859Z"
    }
   },
   "source": "!unzip -o fra-eng.zip",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\r\n",
      "  inflating: _about.txt              \r\n",
      "  inflating: fra.txt                 \r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c4f0da7a",
   "metadata": {
    "papermill": {
     "duration": 0.004544,
     "end_time": "2024-03-14T06:08:19.471715",
     "exception": false,
     "start_time": "2024-03-14T06:08:19.467171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "a418da47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T06:08:19.482783Z",
     "iopub.status.busy": "2024-03-14T06:08:19.482468Z",
     "iopub.status.idle": "2024-03-14T06:08:19.716086Z",
     "shell.execute_reply": "2024-03-14T06:08:19.714878Z"
    },
    "papermill": {
     "duration": 0.241837,
     "end_time": "2024-03-14T06:08:19.718321",
     "exception": false,
     "start_time": "2024-03-14T06:08:19.476484",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:35.524617Z",
     "start_time": "2024-10-16T10:09:35.089916Z"
    }
   },
   "source": [
    "num_samples=10000 # number of samples to train on\n",
    "\n",
    "# vectorize the data\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n",
    "\n",
    "data_path=os.path.join('', \"fra.txt\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines=f.read().split(\"\\n\")\n",
    "    \n",
    "\n",
    "# Initialize character sets\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Data reading and processing\n",
    "for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        target_characters.add(char)\n",
    "\n",
    "# Sort characters and create token indices\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "input_token_index = dict((char, i) for i, char in enumerate(input_characters))\n",
    "target_token_index = dict((char, i) for i, char in enumerate(target_characters))\n",
    "\n",
    "# Reverse mapping for decoding\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "input_token_index=dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index=dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "# Re-create the data from scratch\n",
    "num_encoder_tokens = len(input_token_index)  # Number of unique input tokens\n",
    "num_decoder_tokens = len(target_token_index)  # Number of unique output tokens\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])  # Find the max sequence length for encoder\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])  # Find the max sequence length for decoder\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Input characters:\", input_characters)\n",
    "print(\"Target characters:\", target_characters)\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input characters: [' ', '!', '\"', '$', '%', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '5', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Target characters: ['\\t', '\\n', ' ', '!', '%', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '5', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'À', 'Ç', 'É', 'Ê', 'à', 'â', 'ç', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'ù', 'û', 'œ', '\\u2009', '’', '\\u202f']\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 91\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:39.686480Z",
     "start_time": "2024-10-16T10:09:39.680614Z"
    }
   },
   "cell_type": "code",
   "source": "target_characters[20]",
   "id": "84ea6f7d9071f0af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a2bb0f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T06:08:19.730166Z",
     "iopub.status.busy": "2024-03-14T06:08:19.729826Z",
     "iopub.status.idle": "2024-03-14T06:08:20.345317Z",
     "shell.execute_reply": "2024-03-14T06:08:20.343600Z"
    },
    "papermill": {
     "duration": 0.625075,
     "end_time": "2024-03-14T06:08:20.348715",
     "exception": false,
     "start_time": "2024-03-14T06:08:19.723640",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:40.577633Z",
     "start_time": "2024-10-16T10:09:40.248446Z"
    }
   },
   "source": [
    "# Initialize encoder and decoder input data arrays\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length), dtype=\"int32\"\n",
    ")  # This is now of integer type for class indices\n",
    "\n",
    "# Populate the arrays with the token indices\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data starts from index t-1\n",
    "            decoder_target_data[i, t - 1] = target_token_index[char]\n",
    "\n",
    "\n",
    "print(len(encoder_input_data))\n",
    "print(len(decoder_input_data))\n",
    "print(len(decoder_target_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:40.672328Z",
     "start_time": "2024-10-16T10:09:40.668294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(decoder_target_data[1]))\n",
    "print(len(decoder_target_data[1]))\n",
    "print(len(decoder_target_data[1]))"
   ],
   "id": "c73bfba43d07df32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "59\n",
      "59\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e66a67d7",
   "metadata": {
    "papermill": {
     "duration": 0.00539,
     "end_time": "2024-03-14T06:08:20.359466",
     "exception": false,
     "start_time": "2024-03-14T06:08:20.354076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2cc94fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T06:08:20.370928Z",
     "iopub.status.busy": "2024-03-14T06:08:20.370599Z",
     "iopub.status.idle": "2024-03-14T06:08:34.356496Z",
     "shell.execute_reply": "2024-03-14T06:08:34.355453Z"
    },
    "papermill": {
     "duration": 13.994101,
     "end_time": "2024-03-14T06:08:34.358677",
     "exception": false,
     "start_time": "2024-03-14T06:08:20.364576",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:41.978554Z",
     "start_time": "2024-10-16T10:09:41.948759Z"
    }
   },
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, num_encoder_tokens, num_decoder_tokens, latent_dim):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm = nn.LSTM(num_encoder_tokens, latent_dim, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(num_decoder_tokens, latent_dim, batch_first=True)\n",
    "        self.decoder_dense = nn.Linear(latent_dim, num_decoder_tokens)\n",
    "    \n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        # Encoder forward pass\n",
    "        _, (state_h, state_c) = self.encoder_lstm(encoder_inputs)\n",
    "        \n",
    "        # Decoder forward pass using the encoder's final state as the initial state\n",
    "        decoder_outputs, _ = self.decoder_lstm(decoder_inputs, (state_h, state_c))\n",
    "        \n",
    "        # Dense layer\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "        \n",
    "        return F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model parameters\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space\n",
    "num_encoder_tokens = encoder_input_data.shape[2]  # Based on input shape\n",
    "num_decoder_tokens = decoder_input_data.shape[2]  # Based on input shape\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = Seq2SeqModel(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder_lstm): LSTM(70, 256, batch_first=True)\n",
       "  (decoder_lstm): LSTM(91, 256, batch_first=True)\n",
       "  (decoder_dense): Linear(in_features=256, out_features=91, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:42.738949Z",
     "start_time": "2024-10-16T10:09:42.686242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_model(model, encoder_input_size, decoder_input_size):\n",
    "    # Create dummy input tensors with the specified input sizes\n",
    "    encoder_dummy_input = torch.randn(encoder_input_size).to(next(model.parameters()).device)\n",
    "    decoder_dummy_input = torch.randn(decoder_input_size).to(next(model.parameters()).device)\n",
    "\n",
    "    # Perform a forward pass through the model to capture the computational graph\n",
    "    output = model(encoder_dummy_input, decoder_dummy_input)\n",
    "\n",
    "    # Create a dot graph visualization using torchviz\n",
    "    dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "    # Render the dot graph to SVG format\n",
    "    svg = dot.render(format=\"svg\", outfile=\"./seq2seq.svg\")\n",
    "\n",
    "    # Display the SVG\n",
    "    display(SVG(svg))\n",
    "    \n",
    "# Define input sizes for encoder and decoder (batch_size=32, sequence_length=10)\n",
    "encoder_input_size = (32, 10, num_encoder_tokens)  # Example size\n",
    "decoder_input_size = (32, 10, num_decoder_tokens)  # Example size\n",
    "\n",
    "plot_model(model, encoder_input_size, decoder_input_size)"
   ],
   "id": "b1c7e1db11220e02",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_dot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m encoder_input_size \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m10\u001B[39m, num_encoder_tokens)  \u001B[38;5;66;03m# Example size\u001B[39;00m\n\u001B[1;32m     20\u001B[0m decoder_input_size \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m10\u001B[39m, num_decoder_tokens)  \u001B[38;5;66;03m# Example size\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[43mplot_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_input_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_input_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[21], line 10\u001B[0m, in \u001B[0;36mplot_model\u001B[0;34m(model, encoder_input_size, decoder_input_size)\u001B[0m\n\u001B[1;32m      7\u001B[0m output \u001B[38;5;241m=\u001B[39m model(encoder_dummy_input, decoder_dummy_input)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Create a dot graph visualization using torchviz\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m dot \u001B[38;5;241m=\u001B[39m \u001B[43mmake_dot\u001B[49m(output, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(model\u001B[38;5;241m.\u001B[39mnamed_parameters()))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Render the dot graph to SVG format\u001B[39;00m\n\u001B[1;32m     13\u001B[0m svg \u001B[38;5;241m=\u001B[39m dot\u001B[38;5;241m.\u001B[39mrender(\u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msvg\u001B[39m\u001B[38;5;124m\"\u001B[39m, outfile\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./seq2seq.svg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'make_dot' is not defined"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "e38f60ee",
   "metadata": {
    "papermill": {
     "duration": 0.005296,
     "end_time": "2024-03-14T06:08:34.369169",
     "exception": false,
     "start_time": "2024-03-14T06:08:34.363873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T06:08:34.420071Z",
     "iopub.status.busy": "2024-03-14T06:08:34.419503Z",
     "iopub.status.idle": "2024-03-14T06:09:52.248220Z",
     "shell.execute_reply": "2024-03-14T06:09:52.247411Z"
    },
    "papermill": {
     "duration": 77.8372,
     "end_time": "2024-03-14T06:09:52.250522",
     "exception": false,
     "start_time": "2024-03-14T06:08:34.413322",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-16T10:11:32.398443Z",
     "start_time": "2024-10-16T10:11:24.225405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set parameters\n",
    "\n",
    "epochs = 100  # number of epochs to train for\n",
    "batch_size = 64  # batch size of training\n",
    "validation_split = 0.2  # 20% validation data\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "encoder_input_data = torch.tensor(encoder_input_data, dtype=torch.float32)\n",
    "decoder_input_data = torch.tensor(decoder_input_data, dtype=torch.float32)\n",
    "decoder_target_data = torch.tensor(decoder_target_data, dtype=torch.long)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Make sure \"target_token_index\" is defined, it contains indices of target tokens (ignore padding token index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_token_index[\" \"])  # Optionally ignore padding index\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Calculate training and validation sizes\n",
    "total_size = encoder_input_data.shape[0]\n",
    "val_size = int(total_size * validation_split)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "# Create data indices for training and validation splits\n",
    "indices = torch.randperm(total_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Use TensorDataset to create datasets for training and validation\n",
    "train_dataset = TensorDataset(encoder_input_data[train_indices], decoder_input_data[train_indices], decoder_target_data[train_indices])\n",
    "val_dataset = TensorDataset(encoder_input_data[val_indices], decoder_input_data[val_indices], decoder_target_data[val_indices])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "\n",
    "    for encoder_inputs, decoder_inputs, decoder_targets in train_loader:\n",
    "        encoder_inputs = encoder_inputs.to(device)\n",
    "        decoder_inputs = decoder_inputs.to(device)\n",
    "        decoder_targets = decoder_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(encoder_inputs, decoder_inputs)\n",
    "        \n",
    "        # Reshape outputs and targets for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        decoder_targets = decoder_targets.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, decoder_targets)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in val_loader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "            \n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            decoder_targets = decoder_targets.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, decoder_targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Average losses over batches\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the model components separately\n",
    "torch.save({\n",
    "    'encoder_lstm': model.encoder_lstm.state_dict(),\n",
    "    'decoder_lstm': model.decoder_lstm.state_dict(),\n",
    "    'dense': model.decoder_dense.state_dict()\n",
    "}, 's2s_model.pth')\n",
    "\n",
    "print(\"Model saved as s2s_model.pth\")"
   ],
   "id": "f79b3666",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9672/4120227806.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoder_input_data = torch.tensor(encoder_input_data, dtype=torch.float32)\n",
      "/tmp/ipykernel_9672/4120227806.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input_data = torch.tensor(decoder_input_data, dtype=torch.float32)\n",
      "/tmp/ipykernel_9672/4120227806.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_target_data = torch.tensor(decoder_target_data, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.1551, Validation Loss: 0.1353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Set model to training mode\u001B[39;00m\n\u001B[1;32m     42\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 44\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoder_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_targets\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_inputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mencoder_inputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_inputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdecoder_inputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataset.py:211\u001B[0m, in \u001B[0;36mTensorDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataset.py:211\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(tensor[index] \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "107802bc",
   "metadata": {
    "papermill": {
     "duration": 0.101704,
     "end_time": "2024-03-14T06:09:52.453987",
     "exception": false,
     "start_time": "2024-03-14T06:09:52.352283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "\n",
    "* Encoding input and retrieve intial decoder state\n",
    "* Runing one step of decoder with this initial state and a \"start of sequence\" token as target. Output wil be the bext target token\n",
    "* Repeat with the current target token and curent states"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:46.372562Z",
     "start_time": "2024-10-16T10:09:46.345297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the saved state_dict from the file 's2s_model.pth'\n",
    "checkpoint = torch.load('s2s_model.pth', map_location=device)\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.encoder_lstm.load_state_dict(checkpoint['encoder_lstm'])\n",
    "model.decoder_lstm.load_state_dict(checkpoint['decoder_lstm'])\n",
    "model.decoder_dense.load_state_dict(checkpoint['dense'])\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ],
   "id": "aa18bcbf186342c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9672/1576220595.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('s2s_model.pth', map_location=device)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:47.223028Z",
     "start_time": "2024-10-16T10:09:47.138966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to decode a sequence using the loaded model\n",
    "def decode_sequence(input_seq, max_decoder_seq_length):\n",
    "    # Encode the input as state vectors\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.float32).to(device)\n",
    "    _, (state_h, state_c) = model.encoder_lstm(input_seq)\n",
    "    \n",
    "    # Initialize the target sequence with the start token\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))  # (batch_size, time_steps, num_decoder_tokens)\n",
    "    start_token_index = target_token_index.get('\\t')  # Start token for target\n",
    "    target_seq[0, 0, start_token_index] = 1.  # One-hot encode the start token\n",
    "\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    # Initialize states for the decoder\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        # Convert target_seq to a tensor\n",
    "        target_seq_tensor = torch.tensor(target_seq, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Run the decoder on the current target sequence and states\n",
    "        decoder_outputs, (state_h, state_c) = model.decoder_lstm(target_seq_tensor, (state_h, state_c))\n",
    "        decoder_output = model.decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # Choose the character with the highest probability\n",
    "        token_index = torch.argmax(decoder_output[0, -1, :]).item()\n",
    "        sampled_char = reverse_target_char_index[token_index]\n",
    "        \n",
    "        # Append the decoded character to the decoded_sentence list\n",
    "        decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length or find the end token\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (with the sampled character as the next input)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, token_index] = 1.  # One-hot encode the sampled character\n",
    "\n",
    "    return ''.join(decoded_sentence[:-1])  # Exclude the end token\n",
    "\n",
    "\n",
    "# Example of using the decode_sequence function for inference\n",
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set) for trying out decoding\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]  # Take a single sequence\n",
    "    decoded_sentence = decode_sequence(input_seq, max_decoder_seq_length)  # Pass max_decoder_seq_length\n",
    "    \n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])  # Original input sentence\n",
    "    print(\"Decoded sentence:\", decoded_sentence)  # Model's output sentence\n"
   ],
   "id": "46adde1416241fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Parssez-vous.\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Parssez-vous.\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Parssez-vous.\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Parssez-vous.\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T10:09:48.481870Z",
     "start_time": "2024-10-16T10:09:48.471362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_seq = encoder_input_data[seq_index:seq_index+1]  # Take a single sequence\n",
    "decoded_sentence = decode_sequence(input_seq, max_decoder_seq_length)  # Pass max_decoder_seq_length\n",
    "\n",
    "print(\"-\")\n",
    "print(\"Input sentence:\", input_texts[seq_index])  # Original input sentence\n",
    "print(\"Decoded sentence:\", decoded_sentence)  # Model's output sentence\n"
   ],
   "id": "1e9cb861d1861cc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "094c9327",
   "metadata": {
    "papermill": {
     "duration": 0.101904,
     "end_time": "2024-03-14T06:10:12.636293",
     "exception": false,
     "start_time": "2024-03-14T06:10:12.534389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledge\n",
    "* https://github.com/keras-team/keras-io/blob/master/examples/nlp/lstm_seq2seq.py\n",
    "* https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "* https://huggingface.co/blog/ray-rag"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 122.112882,
   "end_time": "2024-03-14T06:10:14.562006",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-14T06:08:12.449124",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
