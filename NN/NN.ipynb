{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:29:36.443881Z",
     "start_time": "2024-09-21T14:29:36.436963Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display, SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:29:36.530403Z",
     "start_time": "2024-09-21T14:29:36.491529Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"329pt\" height=\"360pt\" viewBox=\"0.00 0.00 328.97 360.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.869141 0.869141) rotate(0) translate(4 410.2)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-410.2 374.5,-410.2 374.5,4 -4,4\"/>\n",
       "<!-- Input_1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Input_1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"36\" cy=\"-231\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"36\" y=\"-227.12\" font-family=\"Times,serif\" font-size=\"10.00\">Input 1</text>\n",
       "</g>\n",
       "<!-- Hidden_1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Hidden_1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.5\" cy=\"-216\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-212.12\" font-family=\"Times,serif\" font-size=\"10.00\">Hidden 1</text>\n",
       "</g>\n",
       "<!-- Input_1&#45;&gt;Hidden_1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Input_1-&gt;Hidden_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.51,-214.48C75.41,-211.59 82.81,-209.02 90,-207.5 107.36,-203.82 126.75,-204.45 143.77,-206.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.22,-209.98 153.62,-207.93 144.22,-203.05 143.22,-209.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-210.7\" font-family=\"Times,serif\" font-size=\"14.00\">W1[0,0]</text>\n",
       "</g>\n",
       "<!-- Hidden_2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Hidden_2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.5\" cy=\"-316\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-312.12\" font-family=\"Times,serif\" font-size=\"10.00\">Hidden 2</text>\n",
       "</g>\n",
       "<!-- Input_1&#45;&gt;Hidden_2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Input_1-&gt;Hidden_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.49,-263.23C61.35,-278.12 73.93,-294.56 90,-304 105.67,-313.21 125.13,-316.91 142.67,-318.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.52,-321.54 152.64,-318.41 142.77,-314.55 142.52,-321.54\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-320.73\" font-family=\"Times,serif\" font-size=\"14.00\">W1[0,1]</text>\n",
       "</g>\n",
       "<!-- Hidden_3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Hidden_3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.5\" cy=\"-126\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-122.12\" font-family=\"Times,serif\" font-size=\"10.00\">Hidden 3</text>\n",
       "</g>\n",
       "<!-- Input_1&#45;&gt;Hidden_3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Input_1-&gt;Hidden_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.07,-203.74C64.32,-198.07 68.52,-191.99 72,-186 82.52,-167.91 73.62,-155.52 90,-142.5 104.8,-130.74 124.82,-125.96 143.02,-124.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.04,-127.88 152.83,-123.84 142.66,-120.89 143.04,-127.88\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">W1[0,2]</text>\n",
       "</g>\n",
       "<!-- Input_2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Input_2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"36\" cy=\"-366\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"36\" y=\"-362.12\" font-family=\"Times,serif\" font-size=\"10.00\">Input 2</text>\n",
       "</g>\n",
       "<!-- Input_2&#45;&gt;Hidden_1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Input_2-&gt;Hidden_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.22,-364.58C93.52,-361.87 119.72,-354.98 136.5,-338 158.17,-316.07 142.08,-299.22 154.5,-271 156.92,-265.5 159.85,-259.91 162.96,-254.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.72,-256.71 167.92,-246.34 159.73,-253.08 165.72,-256.71\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-364.58\" font-family=\"Times,serif\" font-size=\"14.00\">W1[1,0]</text>\n",
       "</g>\n",
       "<!-- Input_2&#45;&gt;Hidden_2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Input_2-&gt;Hidden_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.85,-381.4C89.09,-388.91 115.36,-394.32 136.5,-384 148.57,-378.11 158.68,-367.99 166.74,-357.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.58,-359.38 172.42,-349.17 163.83,-355.38 169.58,-359.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-392.9\" font-family=\"Times,serif\" font-size=\"14.00\">W1[1,1]</text>\n",
       "</g>\n",
       "<!-- Input_2&#45;&gt;Hidden_3 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Input_2-&gt;Hidden_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-332.5C63.39,-298.58 83.24,-250.18 90,-243.5 105.94,-227.74 121.22,-240.39 136.5,-224 153.47,-205.81 142.7,-192.9 154.5,-171 156.04,-168.14 157.76,-165.28 159.59,-162.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.41,-164.53 165.26,-154.33 156.67,-160.53 162.41,-164.53\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-246.7\" font-family=\"Times,serif\" font-size=\"14.00\">W1[1,2]</text>\n",
       "</g>\n",
       "<!-- Bias_H -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Bias_H</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"72,-177 0,-177 0,-105 72,-105 72,-177\"/>\n",
       "<text text-anchor=\"middle\" x=\"36\" y=\"-137.12\" font-family=\"Times,serif\" font-size=\"10.00\">Bias</text>\n",
       "</g>\n",
       "<!-- Bias_H&#45;&gt;Hidden_1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>Bias_H-&gt;Hidden_1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M72.49,-155.25C91.71,-163.29 115.7,-173.83 136.5,-184.5 140.73,-186.67 145.09,-189.04 149.41,-191.47\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"147.59,-194.46 158,-196.43 151.09,-188.4 147.59,-194.46\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-187.7\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"red\">b1[0]</text>\n",
       "</g>\n",
       "<!-- Bias_H&#45;&gt;Hidden_2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>Bias_H-&gt;Hidden_2</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M67.37,-177.5C69.07,-180.29 70.64,-183.14 72,-186 87.28,-218.13 67.07,-236.79 90,-264 104.44,-281.14 116.56,-273.28 136.5,-283.5 140.94,-285.78 145.51,-288.29 150.01,-290.88\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"148,-293.76 158.39,-295.85 151.57,-287.74 148,-293.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-286.7\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"red\">b1[1]</text>\n",
       "</g>\n",
       "<!-- Bias_H&#45;&gt;Hidden_3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>Bias_H-&gt;Hidden_3</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M65.16,-104.52C72.43,-97.59 80.83,-91.32 90,-87.5 109.07,-79.55 116.84,-81.11 136.5,-87.5 142.45,-89.43 148.29,-92.31 153.79,-95.64\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"151.77,-98.5 162.02,-101.17 155.67,-92.69 151.77,-98.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.25\" y=\"-90.7\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"red\">b1[2]</text>\n",
       "</g>\n",
       "<!-- Output -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Output</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"334.5\" cy=\"-158\" rx=\"36\" ry=\"36\"/>\n",
       "<text text-anchor=\"middle\" x=\"334.5\" y=\"-154.12\" font-family=\"Times,serif\" font-size=\"10.00\">Output</text>\n",
       "</g>\n",
       "<!-- Hidden_1&#45;&gt;Output -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>Hidden_1-&gt;Output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.08,-202.69C243.69,-194.68 268.9,-184.39 290.18,-175.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.25,-179.04 299.18,-172.02 288.6,-172.56 291.25,-179.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-197.1\" font-family=\"Times,serif\" font-size=\"14.00\">W2[0]</text>\n",
       "</g>\n",
       "<!-- Hidden_2&#45;&gt;Output -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>Hidden_2-&gt;Output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.27,-289.55C239.12,-263.01 276,-221.97 302.22,-192.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.56,-195.43 308.64,-185.66 299.36,-190.76 304.56,-195.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-259\" font-family=\"Times,serif\" font-size=\"14.00\">W2[1]</text>\n",
       "</g>\n",
       "<!-- Hidden_3&#45;&gt;Output -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>Hidden_3-&gt;Output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M225.87,-133.75C244.49,-137.94 267.72,-143.18 287.84,-147.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"286.95,-151.1 297.47,-149.88 288.49,-144.27 286.95,-151.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-149.01\" font-family=\"Times,serif\" font-size=\"14.00\">W2[2]</text>\n",
       "</g>\n",
       "<!-- Bias_O -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Bias_O</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226.5,-72 154.5,-72 154.5,0 226.5,0 226.5,-72\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-32.12\" font-family=\"Times,serif\" font-size=\"10.00\">Bias</text>\n",
       "</g>\n",
       "<!-- Bias_O&#45;&gt;Output -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>Bias_O-&gt;Output</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M226.97,-66.47C248.63,-85.08 276.23,-108.8 297.91,-127.43\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"295.49,-129.96 305.36,-133.82 300.05,-124.65 295.49,-129.96\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.5\" y=\"-114.72\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"red\">b2</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the XOR dataset\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Weight Initialization\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)  # He initialization for ReLU\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)  # Xavier for sigmoid\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Normalize input\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "# Initialize weights and velocities\n",
    "W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "v_W1, v_b1, v_W2, v_b2 = np.zeros_like(W1), np.zeros_like(b1), np.zeros_like(W2), np.zeros_like(b2)\n",
    "\n",
    "# Create a directed graph\n",
    "dot = Digraph()\n",
    "dot.attr(rankdir='LR', size='8,5')\n",
    "dot.attr('node', shape='circle', fixedsize='true', width='1.0', fontsize='10')\n",
    "bias_style = {'shape': 'square', 'style': 'filled', 'fillcolor': 'lightgrey', 'fontsize': '10'}\n",
    "for i in range(input_size):\n",
    "    dot.node(f'Input_{i + 1}', f'Input {i + 1}')\n",
    "\n",
    "# Add bias node for hidden layer\n",
    "dot.node('Bias_H', 'Bias', **bias_style)\n",
    "for i in range(hidden_size):\n",
    "    dot.node(f'Hidden_{i + 1}', f'Hidden {i + 1}')\n",
    "\n",
    "# Add bias node for output layer\n",
    "dot.node('Bias_O', 'Bias', **bias_style)\n",
    "\n",
    "# Add output layer node\n",
    "dot.node('Output', 'Output')\n",
    "for i in range(input_size):\n",
    "    for j in range(hidden_size):\n",
    "        dot.edge(f'Input_{i + 1}', f'Hidden_{j + 1}', label=f'W1[{i},{j}]')\n",
    "\n",
    "# Connect bias for hidden layer to hidden layer nodes\n",
    "for j in range(hidden_size):\n",
    "    dot.edge('Bias_H', f'Hidden_{j + 1}', label=f'b1[{j}]', style='dashed', color='red', fontcolor='red')\n",
    "\n",
    "# Connect hidden layer to output layer with weight indexes\n",
    "for i in range(hidden_size):\n",
    "    dot.edge(f'Hidden_{i + 1}', 'Output', label=f'W2[{i}]')\n",
    "\n",
    "# Connect bias for output layer to output node\n",
    "dot.edge('Bias_O', 'Output', label='b2', style='dashed', color='red', fontcolor='red')\n",
    "svg = dot.pipe(format='svg')\n",
    "display(SVG(svg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Error Function (Loss Function)**\n",
    "The error function, or loss function, measures the discrepancy between the predicted outputs of the neural network and the actual target values. Selecting an appropriate loss function is crucial as it directs the optimization process to adjust the network's weights effectively.\n",
    "\n",
    "#### **a. Binary Cross-Entropy Loss (Log Loss)**\n",
    "For binary classification tasks, the **Binary Cross-Entropy Loss** is preferred, especially when using the sigmoid activation function in the output layer. This loss function encourages the model to output probabilities close to 0 or 1, aligning well with the probabilistic interpretation of classification.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "L_{\\text{BCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $ y_i $ is the true label for the $i$-th training example (0 or 1),\n",
    "- $ \\hat{y}_i $ is the predicted probability for the $i$-th example (output of the sigmoid function),\n",
    "- $ N $ is the number of training examples.\n",
    "\n",
    "**Advantages:**\n",
    "- **Probabilistic Outputs:** Produces outputs interpretable as probabilities.\n",
    "- **Sensitive to Misclassifications:** Heavily penalizes confident but incorrect predictions, promoting better decision boundaries.\n",
    "\n",
    "#### **b. Mean Squared Error (MSE) Loss**\n",
    "While **Binary Cross-Entropy** is typically favored for classification tasks, **Mean Squared Error (MSE)** is another common loss function, especially in regression tasks. It can also be applied to classification, though it has different characteristics.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "L_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $ y_i $ is the true label for the $i$-th training example,\n",
    "- $ \\hat{y}_i $ is the predicted value for the $i$-th example,\n",
    "- $ N $ is the number of training examples.\n",
    "\n",
    "**Advantages:**\n",
    "- **Simplicity:** Easy to understand and implement.\n",
    "- **Smooth Gradient:** Provides a smooth gradient, beneficial for certain optimization scenarios.\n",
    "\n",
    "**Considerations:**\n",
    "- **Less Effective for Classification:** Does not align as naturally with probabilistic outputs, potentially leading to slower convergence compared to BCE.\n",
    "- **Gradient Behavior:** Smaller gradients when predictions are close to targets can slow down learning.\n",
    "\n",
    "**When to Use:**\n",
    "- **Regression Tasks:** Predicting continuous values.\n",
    "- **Alternative Classification Approach:** When experimenting with different loss functions to observe their impact on learning dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Activation Function**\n",
    "Activation functions introduce non-linearity into the neural network, enabling it to learn complex patterns and representations from the data.\n",
    "\n",
    "#### **a. Sigmoid Activation Function**\n",
    "The sigmoid function maps input values to an output range between 0 and 1, making it suitable for binary classification tasks where outputs represent probabilities.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Characteristics:**\n",
    "- **Output Range:** (0, 1)\n",
    "- **Smooth Gradient:** Facilitates gradient-based optimization methods.\n",
    "- **Historical Usage:** Widely used in early neural network models, especially for output layers in binary classification.\n",
    "\n",
    "**Limitations:**\n",
    "- **Vanishing Gradient Problem:** Gradients can become very small, hindering the learning process in deep networks.\n",
    "- **Output Not Zero-Centered:** Can lead to inefficient gradient updates.\n",
    "\n",
    "#### **b. Rectified Linear Unit (ReLU) Activation Function**\n",
    "The **ReLU** activation function is the default choice for hidden layers in many neural network architectures due to its simplicity and effectiveness.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Characteristics:**\n",
    "- **Output Range:** [0, ∞)\n",
    "- **Computational Efficiency:** Simple computation makes it faster during training.\n",
    "- **Sparsity:** Encourages sparsity in activations, leading to more efficient representations.\n",
    "- **Mitigates Vanishing Gradients:** Unlike sigmoid, ReLU does not saturate in the positive region, allowing gradients to pass through more effectively.\n",
    "\n",
    "**Advantages:**\n",
    "- **Improved Convergence:** Often leads to faster convergence during training compared to sigmoid or tanh.\n",
    "- **Alleviates Vanishing Gradient Problem:** Maintains larger gradients for positive inputs, facilitating deeper network training.\n",
    "\n",
    "**Considerations:**\n",
    "- **Dying ReLU Problem:** Neurons can sometimes become inactive and only output zeros, especially if weights are updated in a way that negative inputs dominate.\n",
    "\n",
    "**Variants:**\n",
    "- **Leaky ReLU:** Allows a small, non-zero gradient when the unit is not active.\n",
    "  $$\n",
    "  \\text{Leaky ReLU}(x) = \\begin{cases}\n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha x & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  where $ \\alpha $ is a small constant (e.g., 0.01).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Data Normalization**\n",
    "Data normalization is a preprocessing step that scales input features to a standard range, typically ensuring that each feature has a mean of 0 and a standard deviation of 1. This process is essential for facilitating efficient and stable training of the neural network.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $ X $ is the original input data,\n",
    "- $ \\mu $ is the mean of the input features,\n",
    "- $ \\sigma $ is the standard deviation of the input features,\n",
    "- $ X_{\\text{norm}} $ is the normalized input data.\n",
    "\n",
    "**Advantages:**\n",
    "- **Faster Convergence:** Normalized data often leads to faster and more stable convergence during training.\n",
    "- **Prevents Dominance of Features:** Ensures that features with larger scales do not dominate the learning process.\n",
    "- **Improves Numerical Stability:** Helps in avoiding numerical issues during computations, especially in deep networks with many layers.\n",
    "\n",
    "**Considerations:**\n",
    "- **Applicability:** Particularly important for activation functions sensitive to the scale of input data, such as sigmoid and tanh.\n",
    "- **Consistency:** Ensure that the mean and standard deviation calculated from the training data are also applied to the test data to maintain consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Bias in Neural Networks**\n",
    "**Biases** are additional parameters in neural networks that allow the activation function to be shifted, providing the model with greater flexibility to fit the data. Without biases, the activation functions would always pass through the origin (0,0), limiting the network's ability to model certain patterns.\n",
    "\n",
    "**Role of Biases:**\n",
    "- **Shifting Activation Functions:** Biases enable the activation functions to move away from the origin, allowing neurons to activate even when all input features are zero.\n",
    "- **Increasing Model Flexibility:** By providing an additional degree of freedom, biases help the network model more complex relationships in the data.\n",
    "\n",
    "**Mathematical Representation:**\n",
    "For a neuron in a hidden or output layer:\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} W_i x_i + b\n",
    "$$\n",
    "where:\n",
    "- $ W_i $ are the weights,\n",
    "- $ x_i $ are the input features,\n",
    "- $ b $ is the bias term,\n",
    "- $ z $ is the linear combination before activation.\n",
    "\n",
    "**Advantages:**\n",
    "- **Non-Origin Activation:** Allows neurons to output non-zero values even when inputs are zero, essential for learning accurate representations.\n",
    "- **Enhanced Learning Capability:** Provides the network with the ability to represent functions that are not symmetric around the origin, increasing the expressiveness of the model.\n",
    "\n",
    "**Considerations:**\n",
    "- **Separate Weight Updates:** Biases are treated as separate parameters and have their own gradients and updates during the training process.\n",
    "- **Regularization:** Depending on the regularization strategy, biases can also be regularized to prevent overfitting, although this is less common than regularizing weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Optimization Function (Gradient Descent with Momentum)**\n",
    "Optimizing the neural network involves adjusting its weights to minimize the loss function. **Gradient Descent with Momentum** is an advanced optimization technique that accelerates convergence and reduces oscillations during training.\n",
    "\n",
    "**Gradient Descent with Momentum:**\n",
    "Momentum adds a fraction of the previous weight update to the current update, allowing the optimizer to build up speed in directions with consistent gradients and dampen oscillations in directions with varying gradients.\n",
    "\n",
    "**Equations:**\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla L\n",
    "$$\n",
    "$$\n",
    "W = W - \\alpha v_t\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $ v_t $ is the velocity term at time step $ t $,\n",
    "- $ \\beta $ is the momentum factor (typically around 0.9),\n",
    "- $ \\alpha $ is the learning rate,\n",
    "- $ \\nabla L $ is the gradient of the loss function $ L $.\n",
    "\n",
    "**Advantages:**\n",
    "- **Faster Convergence:** Accelerates updates in consistent gradient directions.\n",
    "- **Reduces Oscillations:** Smoothens the path towards the minimum, especially in ravines or areas with high curvature.\n",
    "\n",
    "**Considerations:**\n",
    "- **Parameter Tuning:** Requires careful selection of the momentum factor $ \\beta $ and learning rate $ \\alpha $ to balance speed and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Weight Regularization (L2 Regularization)**\n",
    "Regularization techniques are employed to prevent overfitting by discouraging overly complex models. **L2 Regularization**, also known as **Weight Decay**, adds a penalty proportional to the square of the magnitude of the weights to the loss function.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\frac{\\lambda}{2} \\sum_{i} W_i^2\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $ L_{\\text{original}} $ is the original loss function (e.g., BCE or MSE),\n",
    "- $ \\lambda $ is the regularization parameter controlling the strength of the penalty,\n",
    "- $ W_i $ are the weights of the network.\n",
    "\n",
    "**Effect on Weight Updates:**\n",
    "$$\n",
    "W = W - \\alpha \\left( \\nabla L + \\lambda W \\right)\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "- **Prevents Overfitting:** Penalizes large weights, promoting simpler models that generalize better to unseen data.\n",
    "- **Encourages Weight Decay:** Gradually reduces the magnitude of weights, leading to smoother decision boundaries.\n",
    "\n",
    "**Considerations:**\n",
    "- **Regularization Parameter $ \\lambda $:** Must be carefully chosen; too high can lead to underfitting, while too low may not effectively prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Weight Initialization**\n",
    "Proper initialization of network weights is critical for effective training, as it can significantly influence the speed of convergence and the quality of the final model.\n",
    "\n",
    "#### **a. Xavier Initialization (Glorot Initialization)**\n",
    "Designed to maintain the variance of activations and backpropagated gradients through layers, **Xavier Initialization** is suitable for activation functions like sigmoid and tanh.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "where $ n_{\\text{in}} $ is the number of input units to the layer.\n",
    "\n",
    "**Advantages:**\n",
    "- **Balanced Variance:** Prevents activations from becoming too large or too small, facilitating stable gradients.\n",
    "- **Improves Convergence:** Leads to faster and more reliable training, especially in deep networks.\n",
    "\n",
    "#### **b. He Initialization**\n",
    "**He Initialization** is tailored for activation functions like ReLU, which do not have outputs centered around zero.\n",
    "\n",
    "**Definition:**\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "- **Suited for ReLU:** Accounts for the properties of ReLU activations, maintaining variance through layers.\n",
    "- **Enhances Training Stability:** Reduces the likelihood of dying ReLU neurons and promotes healthier gradient flows.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Neural Network Architecture and Data Preparation**\n",
    "\n",
    "#### **a. Defining the XOR Dataset**\n",
    "The XOR problem is a classic example used to demonstrate the capability of neural networks to solve non-linearly separable problems.\n",
    "\n",
    "**Explanation:**\n",
    "- **Input Features ($ X $)**: Four examples with two binary features each.\n",
    "- **Target Labels ($ y $)**: Corresponding binary labels representing the XOR output.\n",
    "\n",
    "#### **b. Neural Network Architecture**\n",
    "The architecture defines the structure of the neural network, including the number of layers, the number of neurons in each layer, and how they are connected.\n",
    "\n",
    "**Explanation:**\n",
    "- **Input Layer ($ \\text{input\\_size} $)**: Number of input features, which is 2 for the XOR problem.\n",
    "- **Hidden Layer ($ \\text{hidden\\_size} $)**: Number of neurons in the hidden layer, set to 3 to provide sufficient capacity to learn the XOR function.\n",
    "- **Output Layer ($ \\text{output\\_size} $)**: Single neuron outputting the predicted probability of the positive class.\n",
    "\n",
    "#### **c. Weight and Velocity Initialization**\n",
    "Weights and velocities are initialized to prepare the network for training.\n",
    "\n",
    "**Explanation:**\n",
    "- **Weight Initialization:** \n",
    "  - **Hidden Layer Weights ($ W1 $)**: Initialized using **He Initialization**, suitable for ReLU activation functions.\n",
    "  - **Hidden Layer Biases ($ b1 $)**: Initialized to zeros, allowing the network to learn the appropriate bias during training.\n",
    "  - **Output Layer Weights ($ W2 $)**: Initialized using **Xavier Initialization**, appropriate for sigmoid activation functions.\n",
    "  - **Output Layer Biases ($ b2 $)**: Initialized to zeros for the same reasons as $ b1 $.\n",
    "  \n",
    "- **Data Normalization:**\n",
    "  - **Mean ($ \\mu $) and Standard Deviation ($ \\sigma $)**: Calculated for each feature to standardize the input data.\n",
    "  - **Normalized Input ($ X_{\\text{norm}} $)**: Each feature in the input data is scaled to have a mean of 0 and a standard deviation of 1, facilitating more effective training.\n",
    "  \n",
    "- **Velocity Initialization:**\n",
    "  - **Velocities ($ v_{W1}, v_{b1}, v_{W2}, v_{b2} $)**: Initialized to zeros. These will store the momentum terms during the gradient descent with momentum optimization, allowing the network to accelerate updates in consistent gradient directions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points and Recommendations**\n",
    "\n",
    "1. **Selecting the Right Loss Function:**\n",
    "   - **Binary Classification:** Prefer **Binary Cross-Entropy Loss** for probabilistic outputs.\n",
    "   - **Regression Tasks:** Use **Mean Squared Error (MSE)** to measure continuous prediction errors.\n",
    "\n",
    "2. **Choosing Activation Functions:**\n",
    "   - **Hidden Layers:** **ReLU** is generally more effective and efficient.\n",
    "   - **Output Layers:** **Sigmoid** for binary classification to produce probabilities.\n",
    "\n",
    "3. **Data Normalization:**\n",
    "   - **Importance:** Ensures all input features contribute equally, preventing features with larger scales from dominating the learning process.\n",
    "   - **Implementation:** Normalize inputs to have a mean of 0 and a standard deviation of 1 before training.\n",
    "\n",
    "4. **Bias in Neural Networks:**\n",
    "   - **Role:** Provides flexibility by allowing activation functions to shift, enabling the network to model more complex patterns.\n",
    "   - **Initialization:** Typically initialized to zero, allowing the network to learn the appropriate bias during training.\n",
    "\n",
    "5. **Optimizing with Momentum:**\n",
    "   - **Momentum Factor ($ \\beta $)** around 0.9 is a good starting point.\n",
    "   - **Learning Rate ($ \\alpha $)** should be tuned based on the specific problem and data.\n",
    "\n",
    "6. **Regularization to Prevent Overfitting:**\n",
    "   - Implement **L2 Regularization** by adding a penalty term to the loss function.\n",
    "   - Carefully tune the regularization parameter $ \\lambda $ to balance fit and generalization.\n",
    "\n",
    "7. **Weight Initialization Strategies:**\n",
    "   - Use **Xavier Initialization** for networks with sigmoid or tanh activations.\n",
    "   - Opt for **He Initialization** when employing ReLU activations to maintain healthy gradients.\n",
    "\n",
    "8. **Monitoring and Early Stopping:**\n",
    "   - Track loss over epochs to monitor convergence.\n",
    "   - Implement **Early Stopping** to halt training when improvements plateau, preventing overfitting.\n",
    "\n",
    "9. **Visualization and Debugging:**\n",
    "   - Use visualization tools to understand the network architecture, including biases.\n",
    "   - Ensure nodes are appropriately sized and labeled to avoid visualization warnings and enhance clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.368061\n",
      "Epoch 6, Loss: 0.256823\n",
      "Epoch 11, Loss: 0.145229\n",
      "Epoch 16, Loss: 0.072241\n",
      "Epoch 21, Loss: 0.007039\n",
      "Epoch 26, Loss: 0.002064\n",
      "Epoch 31, Loss: 0.001184\n",
      "Epoch 36, Loss: 0.000801\n",
      "Epoch 41, Loss: 0.000458\n",
      "Epoch 46, Loss: 0.000299\n",
      "\n",
      "Final Weights and Biases:\n",
      "W1: [[-2.3785184   1.35277908  2.31148624]\n",
      " [ 2.32902894 -1.81769619 -1.96889099]]\n",
      "b1: [[-1.21347921 -0.13021565 -0.27040081]]\n",
      "W2: [[2.63784793]\n",
      " [1.58065546]\n",
      " [2.51188077]]\n",
      "b2: [[-4.24148394]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Loss functions\n",
    "def binary_cross_entropy(y, output):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    return -np.mean(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
    "\n",
    "def mean_squared_error(y, output):\n",
    "    return np.mean((y - output) ** 2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.3\n",
    "epochs = 50\n",
    "momentum = 0.9\n",
    "patience = 100\n",
    "threshold = 1e-6\n",
    "loss_function = 'mse'  # Choose 'bce' for Binary Cross-Entropy or 'mse' for Mean Squared Error\n",
    "\n",
    "loss_history, best_loss, no_improve = [], float('inf'), 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X_norm, W1) + b1\n",
    "    hidden = relu(hidden_input)\n",
    "    output_input = np.dot(hidden, W2) + b2\n",
    "    output = sigmoid(output_input)\n",
    "    \n",
    "    # Compute loss\n",
    "    if loss_function == 'bce':\n",
    "        loss = binary_cross_entropy(y, output)\n",
    "    elif loss_function == 'mse':\n",
    "        loss = mean_squared_error(y, output)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function. Choose 'bce' or 'mse'.\")\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backpropagation\n",
    "    if loss_function == 'bce':\n",
    "        error = output - y\n",
    "    elif loss_function == 'mse':\n",
    "        error = -(y - output)\n",
    "    \n",
    "    if loss_function == 'bce':\n",
    "        delta2 = error * sigmoid_derivative(output)\n",
    "    elif loss_function == 'mse':\n",
    "        delta2 = error * sigmoid_derivative(output)  # Derivative remains the same for consistency\n",
    "    \n",
    "    delta1 = np.dot(delta2, W2.T) * relu_derivative(hidden)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_W2 = np.dot(hidden.T, delta2)\n",
    "    grad_b2 = delta2.sum(axis=0, keepdims=True)\n",
    "    grad_W1 = np.dot(X_norm.T, delta1)\n",
    "    grad_b1 = delta1.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    # Momentum update\n",
    "    v_W2 = momentum * v_W2 - learning_rate * grad_W2\n",
    "    v_b2 = momentum * v_b2 - learning_rate * grad_b2\n",
    "    v_W1 = momentum * v_W1 - learning_rate * grad_W1\n",
    "    v_b1 = momentum * v_b1 - learning_rate * grad_b1\n",
    "    \n",
    "    # Update weights\n",
    "    W2 += v_W2\n",
    "    b2 += v_b2\n",
    "    W1 += v_W1\n",
    "    b1 += v_b1\n",
    "    \n",
    "    # Early stopping\n",
    "    if loss < best_loss - threshold:\n",
    "        best_loss, no_improve = loss, 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Logging\n",
    "    if epoch % max(1, epochs // 10) == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss:.6f}')\n",
    "\n",
    "# Final weights and biases\n",
    "print(\"\\nFinal Weights and Biases:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"b2:\", b2)\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, label=\"Loss\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:30:07.233250Z",
     "start_time": "2024-09-21T14:30:07.227561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0 0] -> Predicted: 2.38% one (Rounded: 0)\n",
      "Test: [0 1] -> Predicted: 99.31% one (Rounded: 1)\n",
      "Test: [1 0] -> Predicted: 100.00% one (Rounded: 1)\n",
      "Test: [1 1] -> Predicted: 1.70% one (Rounded: 0)\n"
     ]
    }
   ],
   "source": [
    "# Test inputs\n",
    "test_input = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Normalize test inputs using training data statistics\n",
    "test_input_normalized = (test_input - X_mean) / X_std\n",
    "# Testing the trained model\n",
    "predicted_output = output  # From the last forward pass during training\n",
    "\n",
    "# For clarity, perform forward propagation again on test data\n",
    "layer1_input_test = np.dot(test_input_normalized, W1) + b1\n",
    "layer1_output_test = relu(layer1_input_test)\n",
    "\n",
    "layer2_input_test = np.dot(layer1_output_test, W2) + b2\n",
    "predicted_output_test = sigmoid(layer2_input_test)\n",
    "\n",
    "# Convert predictions to percentages\n",
    "percented_output = predicted_output_test * 100\n",
    "\n",
    "for i, prediction in enumerate(percented_output):\n",
    "    rounded_output = round(predicted_output_test[i][0])\n",
    "    print(f\"Test: {test_input[i]} -> Predicted: {prediction[0]:.2f}% one (Rounded: {rounded_output})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
